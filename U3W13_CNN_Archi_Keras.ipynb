{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakhanrajpatlolla/aiml-learning/blob/master/U3W13_CNN_Archi_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7W6IuZjFVoD"
      },
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQL9oiyvFYck"
      },
      "source": [
        "### Not for Grading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssvaS0BoGWLp"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC3yBYSkGWLp"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* understand Convolutional neural networks\n",
        "* understand terms like filtering, convolution, pooling, and the intuition behind their working\n",
        "* build CNN model to tackle fashion most dataset using Keras\n",
        "* build CNN model to tackle digit most dataset using Keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSvlKHJ05pQd"
      },
      "source": [
        "## Setup Steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t-K9H-B5pQe"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_CGILrZ5pQe"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "EFYcjfU65pQe"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"U3W13_CNN_Archi_Keras\" #name of the notebook\n",
        "Answer = \"Ungraded\"\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    from IPython.display import HTML, display\n",
        "    ipython.magic(\"sx apt-get install graphviz\")\n",
        "    ipython.magic(\"sx pip install graphviz\")\n",
        "    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/fruits_weight_sphercity.csv\")\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getComplexity() and getAdditional() and getConcepts() and getComments():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"feedback_experiments_input\" : Comments, \"notebook\" : notebook}\n",
        "\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://learn-iiith.talentsprint.com/notebook_submissions\")\n",
        "        # print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "      return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wDGpB4AGWLq"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWVZa0E6GWLq"
      },
      "source": [
        "The idea of the CNN was derived from the biological process of how the visual cortex is structured and works.\n",
        "\n",
        "As per the study of David H. and Torsten Wheel,\n",
        "\n",
        "* neurons in the visual cortex have a local **receptive field** that means neurons will respond to stimuli only in the restricted region, and\n",
        "\n",
        "* receptive fields of all neurons combine to make the whole visual image.\n",
        "\n",
        "The above study inspired the paper Neocognitron in 1980 and which later evolved into Convolutional Neural networks (CNN).\n",
        "\n",
        "The most important building block of a CNN is the convolutional layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7Iax1yyGWLq"
      },
      "source": [
        "### Convolutional Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z3LXJ26GWLr"
      },
      "source": [
        "Neurons in the first convolutional layer are not connected to every single pixel in the input image (like they were in the layers of DNN), but only to pixels in their receptive fields as shown in the figure below. In turn, each neuron in the second convolutional layer is connected only to neurons located within a receptive field in the first layer.\n",
        "\n",
        "This architecture allows the network to concentrate on small low-level features in the first hidden layer, then assemble them into larger higher-level features in the next hidden layer, and so on.\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/conv_layers_multiple_fmap.png\" width=600px/>\n",
        "</center>\n",
        "$\\hspace{6cm} \\text {Convolutional layers with multiple feature maps, and images with three color channels}$\n",
        "<br><br>\n",
        "\n",
        "A neuron’s weights can be represented as a small image with the size of the receptive field. This is termed as **filter**. And, a layer full of neurons using the same filter outputs a **feature map**, which highlights the areas in an image that activate the filter the most.\n",
        "\n",
        "**A convolutional layer has multiple filters and outputs one feature map per filter**, also:\n",
        "\n",
        "* it has one neuron per pixel in each feature map, and all neurons within a given feature map share the same parameters (i.e., the same weights and bias term).\n",
        "\n",
        "* neurons in different feature maps use different parameters.\n",
        "\n",
        "* a neuron’s receptive field extends across all the previous layers’ feature maps i.e, its shape will be $f_h$ x $f_w$ x depth of previous layer, where $f_h$ and $f_w$ are the height and width of the receptive field.\n",
        "\n",
        "In short, a convolutional layer simultaneously applies multiple trainable filters to its inputs, making it capable of detecting multiple features anywhere in its inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YKqcqihOMihW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFyAL26MGWLr"
      },
      "source": [
        "For instance, if the input image and the filter look like the following:\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/CNN_Initial.PNG\" width=400px/>\n",
        "</center>\n",
        "\n",
        "$\\hspace{10.5cm} \\text {Input}   \\hspace{4cm} \\text {Kernel/Filter}$                               \n",
        "\n",
        "The filter (green) slides over the input image (blue) one pixel at a time starting from the top left. The filter multiplies its own values with the overlapping values of the image while sliding over it and adds all of them up to output a single value for each overlap until the entire image is traversed:\n",
        "<br><br>\n",
        "<center>\n",
        "\n",
        "![](https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/CNN_Ani.gif)\n",
        "\n",
        "\n",
        "\n",
        "</center>\n",
        "\n",
        "In the case of images with **multiple channels** (e.g. RGB), the Kernel has the same depth as that of the input image. Convolution is performed between $K_n$ and $I_n$ stack ([$K_1,I_1$], [$K_2,I_2$], [$K_3,I_3$]) and all the results are summed with the bias to give us a squashed one-depth channel Convoluted Feature Output:\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/3Channel_CNN_Ani.gif\" />\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Single filter/kernel having two channel**\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/conv-multi-in.png\" />\n",
        "</center>\n",
        "\n",
        "### **Two filter/kernel having three channel each**\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/conv-1x1.png\" />\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "hdv-4oBe29PV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj4ArdG0GWLs"
      },
      "source": [
        "**Note that:**\n",
        "\n",
        "* all neurons located in the same row $i$ and column $j$ but in different feature maps are connected to the outputs of the exact same neurons in the previous layer.\n",
        "\n",
        "* we do not have to define the filters manually: instead, during training, the convolutional layer will automatically learn the most useful filters for its task, and the layers above will learn to combine them into more complex patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV9JjuNrGWLs"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL4hjwyJGWLt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization, InputLayer, Conv1D, ReLU, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feZFBVDmGWLu"
      },
      "source": [
        "#### Using a TensorFlow implementation, we will now understand convolutional operations on images\n",
        "\n",
        "* Applying a convolutional filter\n",
        "* Feature Map generation\n",
        "* Stride and Padding\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysxxzh6RGWLv"
      },
      "source": [
        "In TensorFlow,\n",
        "\n",
        "* each input image is typically represented as a 3D tensor of shape\n",
        "[height, width, channels].\n",
        "\n",
        "* A mini-batch is represented as a 4D tensor of shape [minibatch size, height, width, channels].\n",
        "\n",
        "* The weights of a convolutional layer are represented as a 4D tensor of shape [$f_h$, $f_w$, $f_{n′}$, $f_n$].\n",
        "\n",
        "* The bias terms of a convolutional layer are simply represented as a 1D tensor of shape [$f_n$].\n",
        "\n",
        "where\n",
        "\n",
        "$f_h$ is filter (or receptive field) height,\n",
        "\n",
        "$f_w$ is filter width,\n",
        "\n",
        "$f_{n′}$ is number of feature maps in the previous layer (layer $l – 1$), and\n",
        "\n",
        "$f_n$ is number of feature maps in the current layer (layer $l$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGi23P1BGWLv"
      },
      "source": [
        "Let's creates two filters and apply them to any two MNIST digit  images.\n",
        "\n",
        "In the following code we use Scikit-Learn’s `fetch_openml()` (which loads digit MNIST dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading Data Digit MNIST Dataset\n",
        "Using SciKit-Learns ```fetch_openml``` to load MNIST data."
      ],
      "metadata": {
        "id": "PXCYE8bXGy8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = fetch_openml('mnist_784', cache=False)"
      ],
      "metadata": {
        "id": "Q8hvfwKNGy9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the data:"
      ],
      "metadata": {
        "id": "w9Yk2RcaHMsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Data Shape :',mnist.data.shape,'\\n')\n",
        "print('Keys : ',mnist.keys(),'\\n')\n",
        "print('Description :', mnist.DESCR)"
      ],
      "metadata": {
        "id": "jZkaS7-2HMsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separating features and targets"
      ],
      "metadata": {
        "id": "8fR9ePvOHMsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X, y) = mnist[\"data\"].values, mnist[\"target\"].values\n",
        "# Shape and datatype of X_train_full\n",
        "X.shape, X.dtype\n"
      ],
      "metadata": {
        "id": "abULopVUHMsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting label from string to int"
      ],
      "metadata": {
        "id": "TQUmV9rmHMsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(y[90]))\n",
        "y = y.astype(np.uint8)\n",
        "print(type(y[90]))"
      ],
      "metadata": {
        "id": "YRsjBCtRHMsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_digit = X[4] # Check the result by putting different values in square bracket\n",
        "sample_digit_image = sample_digit.reshape(28, 28)\n",
        "plt.imshow(sample_digit_image, cmap=mpl.cm.binary)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JmLk3cuhCzKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_digit1 = X[2]\n",
        "sample_digit2 = X[12]"
      ],
      "metadata": {
        "id": "6fGbNxbCC_bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_digit1.shape"
      ],
      "metadata": {
        "id": "kpGoOeI2DyeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uexJKiHoGWLw"
      },
      "outputs": [],
      "source": [
        "# Scale image features\n",
        "sample_digit1 =sample_digit1.reshape( 28, 28) / 255\n",
        "sample_digit2 = sample_digit2.reshape(28, 28) / 255\n",
        "\n",
        "# Visualize images\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "ax[0].imshow(sample_digit1, cmap=mpl.cm.binary)\n",
        "ax[1].imshow(sample_digit2, cmap=mpl.cm.binary)\n",
        "plt.show()\n",
        "sample_digit1.shape, sample_digit2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTNPRQfQGWLw"
      },
      "outputs": [],
      "source": [
        "# Combine images as single 4D array\n",
        "images = np.array([sample_digit1, sample_digit2])[:,:,:,np.newaxis]\n",
        "batch_size, height, width, channels = images.shape\n",
        "batch_size, height, width, channels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Creating filters for horizontal and vertical edges**"
      ],
      "metadata": {
        "id": "LY15X3qFxZDt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmuVHfc4GWLx"
      },
      "source": [
        "Create two 7 × 7 filters (one with a vertical white line in the middle, and the other with a horizontal white line in the middle).\n",
        "\n",
        "Note that shape should be [$f_h$, $f_w$, $f_{n′}$, $f_n$]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQNcmmIyGWLx"
      },
      "outputs": [],
      "source": [
        "# Create 2 filters\n",
        "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
        "# vertical line\n",
        "filters[:, 3, :, 0] = 1\n",
        "# horizontal line\n",
        "filters[3, :, :, 1] = 1\n",
        "filters.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filters[:,:,:,0]"
      ],
      "metadata": {
        "id": "bndqX8B_yRYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filters[:,:,:,1]"
      ],
      "metadata": {
        "id": "oapZRHEszS0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGl6HLzBGWLx"
      },
      "source": [
        "Apply the filters to both images using the `tf.nn.conv2d()` function, which is part of TensorFlow’s low-level Deep Learning API.\n",
        "\n",
        "Here, we use zero padding (`padding=\"SAME\"`) and a stride of 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDtI9fdjGWLx"
      },
      "outputs": [],
      "source": [
        "# Convolutional layer\n",
        "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.shape"
      ],
      "metadata": {
        "id": "PBnurx7czhdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5MgZldBGWLx"
      },
      "outputs": [],
      "source": [
        "# Images' 1st feature map using vertical filter\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "ax[0].imshow(outputs[0, :, :, 0], cmap=\"gray\")\n",
        "ax[1].imshow(outputs[1, :, :, 0], cmap=\"gray\")\n",
        "print(\"1st feature map through verticle filter\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBTXOBz7GWLx"
      },
      "outputs": [],
      "source": [
        "# Images' 2nd feature map using horizontal filter\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "ax[0].imshow(outputs[0, :, :, 1], cmap=\"gray\")\n",
        "ax[1].imshow(outputs[1, :, :, 1], cmap=\"gray\")\n",
        "print(\"2nd feature map through horizontal filter\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsXxF2G2GWLy"
      },
      "source": [
        "In `tf.nn.conv2d()`:\n",
        "\n",
        "* **images** is the input mini-batch (a 4D tensor).\n",
        "\n",
        "* **filters** is the set of filters to apply (also a 4D tensor).\n",
        "\n",
        "* **strides** is equal to 1, but it could also be a 1D array with four elements, where the two central elements are the vertical and horizontal strides ($s_h$ and $s_w$). The first and last elements must currently be equal to 1. They may one day be used to specify a batch stride (to skip some instances) and a channel stride (to skip some of the previous layer’s feature maps or channels).\n",
        "\n",
        "* **padding** must be either \"SAME\" or \"VALID\":\n",
        "\n",
        "  * If set to \"SAME\", the convolutional layer uses zero padding if necessary. The output size is set to the number of input neurons divided by the stride, rounded up. For example, if the input size is 13 and the stride is 5 as shown in the figure below, then the output size is 3 (i.e., 13 / 5 = 2.6, rounded up to 3). Then zeros are added as evenly as possible around the inputs, as needed.\n",
        "\n",
        "  * If set to \"VALID\", the convolutional layer does not use zero padding and may\n",
        "ignore some rows and columns at the bottom and right of the input image,\n",
        "depending on the stride. This means that every neuron’s receptive field lies strictly within valid positions inside the input, hence the name valid.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://wizardforcel.gitbooks.io/scikit-and-tensorflow-workbooks-bjpcjp/content/pics/padding-options.png\" width=450px/>\n",
        "</center>\n",
        "$\\hspace{9.5cm} \\text {Different padding options}$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### **Zero Padding**\n",
        " <br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/zero_padding.png\" />\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "XSqkxdVAFI2N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZh1eecQGWLy"
      },
      "source": [
        "**In the above example, we manually defined the filters, but in a real CNN we would normally define filters as trainable variables so the neural net can learn which filters work best. Instead of manually creating the variables,** use the `keras.layers.Conv2D` layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk5qBb4uGWLy"
      },
      "outputs": [],
      "source": [
        "conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deeper look into One Layer of a CNN\n",
        "\n",
        "\n",
        " <br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/One_Layer_CNN.png\" width=750\n",
        "px/>\n",
        "</center>\n",
        "\n",
        "<center>\n",
        "\n",
        "**Equations**\n",
        "\n",
        "~--------------------------------------~\n",
        "\n",
        "$Z^{[1]}=W^{[1]} \\circledast A^{[0]} + b^{[1]}$\n",
        "\n",
        "$A^{[1]}=ReLu(Z^{[1]})$\n",
        "\n",
        "~--------------------------------------~\n",
        "\n",
        " **Shape of output :**\n",
        "\n",
        "$$\\frac{n+2p-f}{s} +1$$\n",
        "\n",
        "~-------------------------------------~\n",
        "\n",
        "**Example :** In this case: n=6, f=3, p=0, s=1\n",
        "\n",
        "$$\\frac{6+2*0-3}{1} +1=4$$\n"
      ],
      "metadata": {
        "id": "Amu40MjkFvW5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXZD3hEhGWLy"
      },
      "source": [
        "As we can see, convolutional layers have quite a few hyperparameters and we can use cross-validation to find the right values, but this is very time-consuming.\n",
        "\n",
        "Also, CNNs' convolutional layers require a huge amount of RAM. This problem is taken care of by the second common building block of CNNs: the pooling layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcAQ-t5pGWLz"
      },
      "source": [
        "### **Pooling Layers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIp2wjP6GWLz"
      },
      "source": [
        "The goal of the pooling layer is to subsample (i.e., shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters. Each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a receptive field.\n",
        "\n",
        "However, **a pooling neuron has no weights**; all it does is aggregate the inputs using an aggregation function such as the **max** or **mean/average**.\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Max_Pool.png\" width=500px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Average_Pool.png\" width=500px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "The below figure shows a max pooling layer, with a 2 × 2 pooling kernel, stride of 2 and no padding. Only the max input value in each receptive field makes it to the next layer. Because of the stride of 2, the output image has half the size of the input image.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Pooling_layer.png\" width=600px/>\n",
        "</center>\n",
        "$\\hspace{7cm} \\text {Max pooling layer (2x2 pooling kernel, stride 2, no padding)}$\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5NgBVoNGWLz"
      },
      "source": [
        "#### Using a TensorFlow implementation, we will now understand pooling operation on images\n",
        "\n",
        "* Applying a pooling layer\n",
        "* Image compression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hstKAFXtGWLz"
      },
      "source": [
        "The following code creates a max pooling layer using a 2 × 2 kernel and applies it to `images`. The strides default to the kernel size, so this layer will use a stride of 2 (both horizontally and vertically):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape"
      ],
      "metadata": {
        "id": "nblU0nxUu2b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNx9eeWMGWLz"
      },
      "outputs": [],
      "source": [
        "# Pooling layer\n",
        "max_pool = keras.layers.MaxPool2D(pool_size=2, padding=\"valid\", dtype='float64')(images)\n",
        "max_pool.shape, max_pool[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rO_s79qGWL0"
      },
      "source": [
        "To create an average pooling layer, just use AvgPool2D instead of MaxPool2D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiO1GfIbGWL0"
      },
      "outputs": [],
      "source": [
        "# Images before and after pooling operation, showing only single channel\n",
        "fig, ax = plt.subplots(2,2, figsize=(10,6))\n",
        "\n",
        "ax[0][0].imshow(images[0, :, :, :].reshape(28, 28), cmap=\"gray\")\n",
        "ax[0][1].imshow(images[1, :, :, :].reshape(28, 28), cmap=\"gray\")\n",
        "\n",
        "ax[1][0].imshow(max_pool[0].numpy().reshape(14,14), cmap=\"gray\")\n",
        "ax[1][1].imshow(max_pool[1].numpy().reshape(14,14), cmap=\"gray\")\n",
        "\n",
        "print(\"Images before and after pooling operation:\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVFraiWNGWL1"
      },
      "outputs": [],
      "source": [
        "# Compression ratio\n",
        "original_bytes = images.nbytes\n",
        "pooling_bytes = np.array(max_pool).nbytes\n",
        "ratio = pooling_bytes / original_bytes\n",
        "print(\"The compression ratio between the original images size and the total size after pooling is:\", ratio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAbHLiAzGWL1"
      },
      "source": [
        "From the above results, we can see that the images are shrunk.\n",
        "\n",
        "Now we know all the building blocks to create convolutional neural networks.\n",
        "Let's  understand a typical CNN architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wACRVHSnGWL1"
      },
      "source": [
        "### **CNN Architecture**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOk9Z4MdGWL1"
      },
      "source": [
        "Typical CNN architectures stack a few convolutional layers (generally followed by a ReLU layer), then a pooling layer, then another few convolutional layers (+ReLU), then another pooling layer, and so on. The image gets smaller and smaller as it progresses through the network, but it also gets deeper and deeper (i.e., with more feature maps), as shown in the figure below. At the top of the stack, a regular feedforward neural network is added, composed of a few fully connected layers (+ReLUs), and the final layer outputs the prediction.\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://i1.wp.com/thecleverprogrammer.com/wp-content/uploads/2020/11/1-cnnlayer.png?resize=1024%2C259&ssl=1\" width=600px/>\n",
        "</center>\n",
        "\n",
        "$\\hspace{11cm} \\text {CNN architecture}$\n",
        "<br><br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CNN Example**\n",
        "\n",
        "Have a look into the detailing of each step involved along with the parameter calculations:"
      ],
      "metadata": {
        "id": "ZtaM_hgA6-n3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/CNN_Ex1.png\" width=600 px/>\n",
        "\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/CNN_Ex2.png\" width=600 px/>\n",
        "\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/CNN_Ex3.png\"\n",
        "width=600 px/>\n",
        "\n",
        "**Parameters Calculation of each layer**\n",
        "\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/CNN_Parameters.png\" width=800 px/>\n",
        "\n",
        "\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "JMEIf-m1W8HP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **CNN on Fashion MNIST dataset**\n",
        "Here is how we can implement a simple CNN to tackle the Fashion MNIST dataset:"
      ],
      "metadata": {
        "id": "umwjk70LW8a9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-s7j23vGWL1"
      },
      "outputs": [],
      "source": [
        "# Using Keras to load the dataset\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eoG8ubBGWL2"
      },
      "outputs": [],
      "source": [
        "# Visualize an image from data\n",
        "print(\"Label: \", y_train_full[0])\n",
        "plt.imshow(X_train_full[0], cmap='Greys')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ATde942GWL2"
      },
      "outputs": [],
      "source": [
        "# Shape and datatype of X_train\n",
        "X_train_full.shape, X_train_full.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxHft9mdGWL2"
      },
      "outputs": [],
      "source": [
        "# Split into training and validation data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=123)\n",
        "X_train.shape, X_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0v9OOixGWL2"
      },
      "outputs": [],
      "source": [
        "# Reshape train, test, and validation data\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "X_val = X_val.reshape(X_val.shape[0], 28, 28, 1)\n",
        "X_train.shape, X_val.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5tHXN4rGWL3"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/CNN_architecture_fmnist.png\" width=900px/>\n",
        "</center>\n",
        "$\\hspace{7.5cm} \\text {CNN architecture used for Fashion MNIST dataset}$\n",
        "<br><br>\n",
        "\n",
        "The CNN architecture used for FMNIST dataset is shown in the figure above and its corresponding sequential model is created using the code below:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Building**"
      ],
      "metadata": {
        "id": "AufjBh6_vNVD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDS4NtKeGWL3"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = Sequential([\n",
        "                    Conv2D(64, 7, activation=\"relu\", padding=\"same\", input_shape=[28, 28, 1]),\n",
        "                    MaxPooling2D(2),\n",
        "\n",
        "                    Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
        "\n",
        "                    Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
        "                    MaxPooling2D(2),\n",
        "\n",
        "                    Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
        "\n",
        "                    Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
        "                    MaxPooling2D(2),\n",
        "\n",
        "                    Flatten(),\n",
        "                    Dense(128, activation=\"relu\"),\n",
        "                    Dropout(0.5),\n",
        "                    Dense(64, activation=\"relu\"),\n",
        "                    Dropout(0.5),\n",
        "                    Dense(10, activation=\"softmax\")\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36rNsPuLGWL3"
      },
      "source": [
        "Let’s go through this model:\n",
        "\n",
        "* The first layer uses 64 fairly large filters (7 × 7) but no stride because the input images are not very large. It also sets input_shape=[28, 28, 1], because the images are 28 × 28 pixels, with a single color channel (i.e., grayscale).\n",
        "\n",
        "* Next, we have a max pooling layer which uses a pool size of 2, so it divides each spatial dimension by a factor of 2.\n",
        "\n",
        "* Then we repeat the same structure twice: two convolutional layers followed by a max pooling layer. For larger images, we could repeat this structure several more times (the number of repetitions is a hyperparameter we can tune).\n",
        "\n",
        "* Note that the number of filters grows as we climb up the CNN toward the output layer (it is initially 64, then 128, then 256): since the number of low-level features is often fairly low (e.g., small circles, horizontal lines), but there are many different ways to combine them into higher-level features.\n",
        "\n",
        "* Next is the fully connected network, composed of two hidden dense layers and a dense output layer. Note that we must flatten its inputs since a dense network expects a 1D array of features for each instance. We also add two dropout layers, to reduce overfitting."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "EB9KQ2mU34To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLiFiuyeGWL3"
      },
      "outputs": [],
      "source": [
        "# Compile model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Augmentation**"
      ],
      "metadata": {
        "id": "nyL3Lab4veQO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyYKZOdiGWL3"
      },
      "source": [
        "Here we use `ImageDataGenerator` that generates batches of tensor image data with real-time data augmentation. The images output by the generator will have the same dimensions as the input images. It lets us augment the images in real-time while the model is still training.\n",
        "\n",
        "We can apply any random transformations on the training image as it is passed to the model. Few parameters are:\n",
        "\n",
        "* **rescale**: scale the image\n",
        "* **horizontal_flip**: randomly flip inputs horizontally\n",
        "* **width_shift_range**: shift the image to the left or right (horizontal shifts)\n",
        "* **height_shift_range**: shift the image vertically (up or down)\n",
        "* **rotation_range**: degree range for random rotations of the image\n",
        "\n",
        "To know more about ImageDataGenerator, click [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di7PG3XGGWL3"
      },
      "outputs": [],
      "source": [
        "# Model Parameters\n",
        "batch_size = 256\n",
        "\n",
        "# Instantiate ImageDataGenerator\n",
        "gen = ImageDataGenerator(rescale = 1.0/255,\n",
        "                         width_shift_range = 0.005,\n",
        "                         height_shift_range = 0.005,\n",
        "                         rotation_range = 0,\n",
        "                         horizontal_flip = True)\n",
        "\n",
        "# Generate batches of tensor image data\n",
        "train_batches = gen.flow(X_train, y_train, batch_size = batch_size)\n",
        "val_batches = gen.flow(X_val, y_val, batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9y4B3LQmF8M"
      },
      "source": [
        "**Note:** Use [`flow_from_directory()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory) to load images from the directory. It takes the path to a directory & generates batches of augmented data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the Model**"
      ],
      "metadata": {
        "id": "1AzgbfIKvics"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaIvMTWrmEXL"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(train_batches,\n",
        "                    steps_per_epoch = X_train.shape[0]//batch_size,\n",
        "                    epochs = 1,\n",
        "                    validation_data = val_batches,\n",
        "                    validation_steps = X_val.shape[0]//batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### We can increase the number of epoch for improved accuracy"
      ],
      "metadata": {
        "id": "0nBxA1Kb6QoH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3R93T5IGWL4"
      },
      "outputs": [],
      "source": [
        "# Instantiate ImageDataGenerator\n",
        "test_gen = ImageDataGenerator(rescale=1.0/255)\n",
        "# Generate batches of tensor image data\n",
        "test_batches = test_gen.flow(X_test, y_test, batch_size= 50)\n",
        "\n",
        "# Evaluate Model against test data and get the score\n",
        "score = model.evaluate(test_batches)\n",
        "\n",
        "# Print Metrics\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p48GcdeKGWL4"
      },
      "source": [
        "By increasing the number of epochs, the accuracy can be improved."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predictions**"
      ],
      "metadata": {
        "id": "FPirMziwvppo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmWA8_3AGWL4"
      },
      "outputs": [],
      "source": [
        "# Predict class probabilities for first three instances of X_test\n",
        "X_new = X_test[-3:]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXaz6rgeGWL4"
      },
      "outputs": [],
      "source": [
        "# List of labels\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2Z_sUv8GWL4"
      },
      "outputs": [],
      "source": [
        "# Predict class labels for first three instances of X_test\n",
        "y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
        "print(\"Predicted labels: \\n\", y_pred)\n",
        "\n",
        "print(np.array(class_names)[y_pred])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsrtUE5DGWL4"
      },
      "outputs": [],
      "source": [
        "# Actual labels\n",
        "fig, ax = plt.subplots(1,3)\n",
        "for axi, i in zip(ax.ravel(), np.arange(len(X_new))):\n",
        "    axi.imshow(X_new[i].reshape(28, 28), cmap='Greys')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WjysELFx1D9"
      },
      "source": [
        "From the above results, we can see the performance of network on three images."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **CNN on Digit MNIST Dataset**\n",
        "#### Loading Data:\n",
        "Using SciKit-Learns ```fetch_openml``` to load MNIST data"
      ],
      "metadata": {
        "id": "UN4dAL3cvzVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = fetch_openml('mnist_784', cache=False)"
      ],
      "metadata": {
        "id": "s7mOfLhVoc-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding the data**"
      ],
      "metadata": {
        "id": "G7A6-dk8oapg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Data Shape :',mnist.data.shape,'\\n')\n",
        "print('Keys : ',mnist.keys(),'\\n')\n",
        "print('Description :', mnist.DESCR)"
      ],
      "metadata": {
        "id": "k0hpchr3oZYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Separating features and targets**"
      ],
      "metadata": {
        "id": "y7Etqe75oA0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X, y) = mnist[\"data\"].values, mnist[\"target\"].values\n",
        "# Shape and datatype of X_train_full\n",
        "X.shape, X.dtype\n"
      ],
      "metadata": {
        "id": "L1Xmd1RioWNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting label from string to int**"
      ],
      "metadata": {
        "id": "bHm3uPwA09uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(y[90]))\n",
        "y = y.astype(np.uint8)\n",
        "print(type(y[90]))"
      ],
      "metadata": {
        "id": "s5eux-mg1D0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_full, X_test, y_train_full, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
        "# Shape and datatype of X_train\n",
        "X_train_full.shape, X_train_full.dtype"
      ],
      "metadata": {
        "id": "U-79_dzWtsjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and validation data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=123)\n",
        "X_train.shape, X_val.shape"
      ],
      "metadata": {
        "id": "dHuXREpQr7Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing the data set**"
      ],
      "metadata": {
        "id": "hiuoj-60orly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_digit = X_train[60] # Check the result by putting different values in square bracket\n",
        "sample_digit_image = sample_digit.reshape(28, 28)\n",
        "plt.imshow(sample_digit_image, cmap=mpl.cm.binary)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C-HzjA5HowR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[60]"
      ],
      "metadata": {
        "id": "RxJM7yJv1SoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape train, test, and validation data\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "X_val = X_val.reshape(X_val.shape[0], 28, 28, 1)\n",
        "X_train.shape, X_val.shape, X_test.shape"
      ],
      "metadata": {
        "id": "n2CH7LrEs4Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Building**"
      ],
      "metadata": {
        "id": "1e5cqAhJvVgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "                    Conv2D(64,3, activation=\"relu\", padding=\"same\", input_shape=[28, 28, 1]),\n",
        "                    BatchNormalization(),\n",
        "                    MaxPooling2D(2),\n",
        "\n",
        "                    Conv2D(64,3, activation=\"relu\", padding=\"same\"),\n",
        "                    BatchNormalization(),\n",
        "                    MaxPooling2D(2),\n",
        "\n",
        "                    Conv2D(32,3, activation=\"relu\", padding=\"same\", input_shape=[28, 28, 1]),\n",
        "                    BatchNormalization(),\n",
        "                    MaxPooling2D(2),\n",
        "\n",
        "                    Flatten(),\n",
        "                    Dense(64, activation=\"relu\"),\n",
        "                    Dropout(0.25),\n",
        "                    Dense(32, activation=\"relu\"),\n",
        "                    Dropout(0.25),\n",
        "                    Dense(10, activation=\"softmax\")\n",
        "                    ])"
      ],
      "metadata": {
        "id": "CRf-9l6uvU0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "O4w4r2su0dDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "daDY6dVhx1Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augmentation**"
      ],
      "metadata": {
        "id": "wSlQM8kGyXPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Parameters\n",
        "batch_size = 256\n",
        "\n",
        "# Instantiate ImageDataGenerator\n",
        "gen = ImageDataGenerator(rescale = 1.0/255,\n",
        "                         width_shift_range = 0.2,\n",
        "                         height_shift_range = 0.2,\n",
        "                         rotation_range = 0,\n",
        "                         horizontal_flip = True)\n",
        "\n",
        "# Generate batches of tensor image data\n",
        "train_batches = gen.flow(X_train, y_train, batch_size = batch_size)\n",
        "val_batches = gen.flow(X_val, y_val, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "Wk2fuzgVx9_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the model**"
      ],
      "metadata": {
        "id": "2-mv4FAPykiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(train_batches,\n",
        "                    steps_per_epoch = X_train.shape[0]//batch_size,\n",
        "                    epochs = 5,\n",
        "                    validation_data = val_batches,\n",
        "                    validation_steps = X_val.shape[0]//batch_size)"
      ],
      "metadata": {
        "id": "8ZHVHsddyk0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate ImageDataGenerator\n",
        "test_gen = ImageDataGenerator(rescale=1.0/255)\n",
        "# Generate batches of tensor image data\n",
        "test_batches = test_gen.flow(X_test, y_test, batch_size= 50)\n",
        "\n",
        "# Evaluate Model against test data and get the score\n",
        "score = model.evaluate(test_batches)\n",
        "\n",
        "# Print Metrics\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "akCQJi2Ryxcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predictions**"
      ],
      "metadata": {
        "id": "VgN_tXvKAi5L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XpwLY8tAkbE"
      },
      "outputs": [],
      "source": [
        "# Predict class probabilities for first three instances of X_test\n",
        "X_new = X_test[-3:]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LazQ6pDsAkbK"
      },
      "outputs": [],
      "source": [
        "# Predict class labels for first three instances of X_test\n",
        "y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
        "print(\"Predicted labels: \\n\", y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hnseg9GzAkbM"
      },
      "outputs": [],
      "source": [
        "# Actual labels\n",
        "fig, ax = plt.subplots(1,3)\n",
        "for axi, i in zip(ax.ravel(), np.arange(len(X_new))):\n",
        "    axi.imshow(X_new[i].reshape(28, 28), cmap='Greys')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCqiG78qAkbN"
      },
      "source": [
        "From the above results, we can see the performance of network on three images."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Classic Networks:**\n",
        "\n",
        "* AlexNet\n",
        "* LeNet\n",
        "* VGG\n",
        "* ResNet\n",
        "* Inception"
      ],
      "metadata": {
        "id": "BqsoBKABR1wy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJx93dFgYUzS"
      },
      "source": [
        "### AlexNet\n",
        "Now, let's look at the AlexNet architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97_jAo5AYby7"
      },
      "source": [
        "The AlexNet CNN architecture won the 2012 ImageNet ILSVRC challenge by a\n",
        "large margin. It was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. It was the first to stack convolutional layers directly on top of one another, instead of stacking a pooling layer on top of each convolutional layer. The table given below presents this architecture.\n",
        "<br><br>\n",
        "\n",
        "|  Layer  |  Type  | Maps  |  Size  |  Kernel Size | Stride  |  Padding  |  Activation  \n",
        "|:--------------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|\n",
        "|  Out  | Fully connected | - |  1,000 | - | - | - | Softmax |\n",
        "| F10 | Fully connected | - | 4,096 | - | - | - | ReLU |\n",
        "| F9 | Fully connected | - | 4,096 | - | - | - | ReLU |\n",
        "| S8 | Max pooling | 256 | 6 x 6 | 3 x 3 | 2 | valid | - |\n",
        "| C7 | Convolution | 256 | 13 x 13 | 3 x 3 | 1 | same | ReLU |\n",
        "| C6 | Convolution | 384 | 13 x 13 | 3 x 3 | 1 | same | ReLU |\n",
        "| C5 | Convolution | 384 | 13 x 13 | 3 x 3 | 1 | same | ReLU |\n",
        "| S4 | Max pooling | 256 | 13 x 13 | 3 x 3 | 2 | valid | - |\n",
        "| C3 | Convolution | 256 | 27 x 27 | 5 x 5 | 1 | same | ReLU |\n",
        "| S2 | Max pooling | 96 | 27 x 27 | 3 x 3 | 2 | valid | - |\n",
        "| C1 | Convolution | 96 | 55 x 55 | 11 x 11 | 4 | valid | ReLU |\n",
        "| In | Input | 3 (RGB) | 227 x 227 | - | - | - | - |\n",
        "\n",
        "$\\hspace{11cm} \\text {AlexNet architecture}$\n",
        "<br><br>\n",
        "\n",
        "To reduce overfitting, the authors used two regularization techniques:\n",
        "\n",
        "* **dropout** with a 50% dropout rate during training to the outputs of layers F9 and F10\n",
        "\n",
        "* **data augmentation** by randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions.\n",
        "\n",
        "Let's build the AlexNet architecture and train it on the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. The dataset contains 60,000 colour images, each with dimensions 32x32px. The content of the images within the dataset is sampled from 10 classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG8oPbxgfD9M"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO8nJXENc4UZ"
      },
      "outputs": [],
      "source": [
        "# List to refer labels\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SicSsFDlg0q3"
      },
      "source": [
        "By default, the CIFAR dataset is partitioned into 50,000 training data and 10,000 test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hryFMs6yg5w1"
      },
      "outputs": [],
      "source": [
        "# Shape of train and test images\n",
        "train_images.shape, test_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8lSZGQX2k7i"
      },
      "source": [
        "Instead of taking all the 50000 instances for training, we will only use the first 5000 instances as validation set and next the 5000 instances as training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZbeEwoBdY1k"
      },
      "outputs": [],
      "source": [
        "# Validation data\n",
        "val_images, val_labels = train_images[:5000], train_labels[:5000]\n",
        "# Training data\n",
        "train_images, train_labels = train_images[5000:10000], train_labels[5000:10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp4JiR0U3E3l"
      },
      "source": [
        "To ease data manipulation and modification we transform the dataset into a TensorFlow dataset using `tf.data.Dataset.from_tensor_slices` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX3SelvVhlLr"
      },
      "outputs": [],
      "source": [
        "# Convert to TensorFlow dataset\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arkXw5fujoYC"
      },
      "outputs": [],
      "source": [
        "# Visualize few images\n",
        "plt.figure(figsize=(10,10))\n",
        "for i, (image, label) in enumerate(train_ds.take(5)):\n",
        "    ax = plt.subplot(5,5,i+1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(class_names[label.numpy()[0]])\n",
        "    plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeZ0jMuj4i7x"
      },
      "source": [
        "Now as per the network, we need to do few preprocessing steps:\n",
        "\n",
        "* Normalize images to have a mean of 0 and standard deviation of 1\n",
        "\n",
        "* Resizing of the images from 32 x 32 to 227 x 227. The AlexNet network input expects a 227x227 image.\n",
        "\n",
        "We create a function called `process_images` for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQNnlYXnjj7-"
      },
      "outputs": [],
      "source": [
        "def process_images(image, label):\n",
        "    ''' Normalize images to have a mean of 0 and standard deviation of 1,\n",
        "        resize images from 32x32 to 227x227 '''\n",
        "    image = tf.image.per_image_standardization(image)\n",
        "    image = tf.image.resize(image, (227,227))\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yrMgCW7jhoX"
      },
      "outputs": [],
      "source": [
        "# Size of training, testing, validation set\n",
        "train_ds_size = len(train_ds)\n",
        "test_ds_size = len(test_ds)\n",
        "val_ds_size = len(val_ds)\n",
        "print(\"Training data size:\", train_ds_size)\n",
        "print(\"Test data size:\", test_ds_size)\n",
        "print(\"Validation data size:\", val_ds_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0MLwnvCjdq-"
      },
      "outputs": [],
      "source": [
        "# Perform preprocessing with shuffle and batch data operations\n",
        "train_ds = (train_ds.map(process_images)\n",
        "                    .shuffle(buffer_size=train_ds_size)\n",
        "                    .batch(batch_size=32, drop_remainder=True))\n",
        "\n",
        "test_ds = (test_ds.map(process_images)\n",
        "                  .shuffle(buffer_size=test_ds_size)\n",
        "                  .batch(batch_size=32, drop_remainder=True))\n",
        "\n",
        "val_ds = (val_ds.map(process_images)\n",
        "                .shuffle(buffer_size=val_ds_size)\n",
        "                .batch(batch_size=32, drop_remainder=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvS4htQliMqx"
      },
      "source": [
        "Model implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYfX1HGIiOfd"
      },
      "outputs": [],
      "source": [
        "# Create AlexNet CNN architecture\n",
        "model = Sequential([\n",
        "                    Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),\n",
        "                    BatchNormalization(),\n",
        "                    MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
        "                    Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "                    BatchNormalization(),\n",
        "                    MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
        "                    Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "                    BatchNormalization(),\n",
        "                    Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "                    BatchNormalization(),\n",
        "                    Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "                    BatchNormalization(),\n",
        "                    MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
        "                    Flatten(),\n",
        "                    Dense(4096, activation='relu'),\n",
        "                    Dropout(0.5),\n",
        "                    Dense(4096, activation='relu'),\n",
        "                    Dropout(0.5),\n",
        "                    Dense(10, activation='softmax')\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCGPud2bjFfR"
      },
      "outputs": [],
      "source": [
        "# Compile model\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = tf.optimizers.SGD(learning_rate = 0.001), metrics = ['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbbswBHpjMF3"
      },
      "outputs": [],
      "source": [
        "# Train model on training set\n",
        "history = model.fit(train_ds, epochs = 1, validation_data = val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpwGIkPwjUiZ"
      },
      "outputs": [],
      "source": [
        "# Model performance on test set\n",
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT6B1L_QR9wz"
      },
      "source": [
        "By increasing the number of training instances and epochs, the model performance can be increased.\n",
        "\n",
        "Other than AlexNet we have few more CNN architectures such as ResNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqEr5Jx-GWL7"
      },
      "source": [
        "### Theory Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dfmVta-GWL7"
      },
      "source": [
        "1. What are the advantages of a CNN over a fully connected DNN for image classification?\n",
        "\n",
        " These are the main advantages of a CNN over a fully connected DNN for image\n",
        "classification:\n",
        "\n",
        "  * Because consecutive layers are only partially connected and because it heavily reuses its weights, a CNN has many fewer parameters than a fully connected DNN, which makes it much faster to train, reduces the risk of overfitting, and requires much less training data.\n",
        "  \n",
        "  * When a CNN has learned a kernel that can detect a particular feature, it can\n",
        "detect that feature anywhere in the image. In contrast, when a DNN learns a\n",
        "feature in one location, it can detect it only in that particular location. Since images typically have very repetitive features, CNNs are able to generalize much better than DNNs for image processing tasks such as classification, using fewer training examples.\n",
        "\n",
        "  * Finally, a DNN has no prior knowledge of how pixels are organized; it does not know that nearby pixels are close. A CNN’s architecture embeds this prior\n",
        "knowledge. Lower layers typically identify features in small areas of the images, while higher layers combine the lower-level features into larger features. This works well with most natural images, giving CNNs a decisive head start compared to DNNs.\n",
        "\n",
        "2. Why would you want to add a max pooling layer rather than a convolutional\n",
        "layer with the same stride?\n",
        "\n",
        " A max pooling layer has no parameters at all, whereas a convolutional layer has\n",
        "quite a few."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task**\n",
        "Create a CNN model again for Digit MNIST DataSet using the given architecture below:\n",
        "<center>\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/CNN_Practice.png\"\n",
        "width=700 px/>\n",
        "\n",
        "</center>\n",
        "\n",
        "* Use batch normalization and dropout and play with the layer, neurons, other parameters/hyperparameters and check the accuracy.\n",
        "\n",
        "* Keep epoch less than 5/6 initially.\n",
        "\n",
        "* Complete all the steps till the last predictin and visualization.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PzXv9SSqTG1D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r35isHfTVGKc"
      },
      "source": [
        "#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Walkthrough = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for Ungrading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}