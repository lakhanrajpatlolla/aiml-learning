{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakhanrajpatlolla/aiml-learning/blob/master/Self_Attention_for_Tfr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Self Attention**"
      ],
      "metadata": {
        "id": "qDyKduDLxk5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Learning Objectives**\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand the implementation details of self attention\n",
        "* understand & implement the concept of Queries, Keys and Values\n",
        "* understand & implement the concept of masking\n",
        "\n"
      ],
      "metadata": {
        "id": "5GuKvsDyxqBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Introduction**\n",
        "\n",
        "Most of the popular language models are Transformer-based architectures which  uses an important techique called 'self-attention'. The primary function of self-attention is to generate the context aware vectors from the sequence. See the example below.\n",
        "\n",
        "Note : We are writing **'as per the paper'** to  mention the  paper -->[\"**Attention Is All You Need**\"](https://arxiv.org/pdf/1706.03762v6.pdf) ."
      ],
      "metadata": {
        "id": "u8ZkdGMJxReR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= https://www.dropbox.com/scl/fi/0fi9619uk3eizxe0saxub/Self_Attention_Scores.png?rlkey=q7kji50ctfbv4igfnuyas54vj&raw=1 width=900px/>\n",
        "</center>\n",
        "\n",
        "**outputs = sum(inputs * pairwise_scores(inputs, inputs))**"
      ],
      "metadata": {
        "id": "DaMRO3dwyk-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "According to the self attention scores which are depicted in the picture, the word 'train pays' more attention to station rather than other words in consideration such as 'on' or 'the'.The self-attention model allows inputs to interact with each other (i.e calculate attention of all other inputs wrt one input)."
      ],
      "metadata": {
        "id": "1KYYVeyJyxRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Implementation Details**"
      ],
      "metadata": {
        "id": "loVmoaNgzd4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Inside each attention head is a **Scaled Dot Product Self-Attention** operation, the operation returns a Attention vector as given by equation below:\n",
        "\n",
        "$$ Self Attention = softmax(\\frac{x^{T}_i x_j}{\\sqrt{d_k}})x_j $$\n",
        "\n",
        "The term  **$x^{T}_i x_j$** is dot product of input vector with itself. The  'pivot_vector' and the 'vector' forms the 'xi' and 'xj' of the above Self Attention function."
      ],
      "metadata": {
        "id": "i1oUKCgWn4vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Attention Eqn. with Queries, Keys and Values**\n",
        "\n",
        "We computed the Self Attention based on the inputs of vectors themselves. This means that for fixed inputs, these attention weights would always be fixed. In other words, there are no learnable parameters. Need to introduce some learnable parmeters which will make the self attention mechanism more flexible and tunable for various tasks. To fullfil this purpose, three weight matices are introduced and multiplied with input $x_i$ seperately and three new terms **Queries(Q), Keys(K) and Values(V)** comes into picture as given by equations below. Vectorized implemenation  & Shape tracking are also shown along with equations."
      ],
      "metadata": {
        "id": "mjWbWLPWoT7D"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FfmE2IrQy6RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorized implemenation  & Shape tracking**\n",
        "\n",
        "$ d_{model} $ = Embedding vector for each word ( 512 as per the paper).\n",
        "\n",
        "$ X   \\Rightarrow (T \\times d_{model}) $\n",
        "\n",
        "\n",
        "$ Q = X W^{Q}   \\Rightarrow (T \\times d_{model}) \\times (d_{model} \\times d_k  )  \\Rightarrow   (T \\times d_k ) $\n",
        "\n",
        "\n",
        "$ K = X W^{K}   \\Rightarrow (T \\times d_{model}) \\times (d_{model} \\times d_k  )  \\Rightarrow   (T \\times d_k ) $\n",
        "\n",
        "\n",
        "$ V = X W^{V}   \\Rightarrow (T \\times d_{model}) \\times (d_{model} \\times d_v  )  \\Rightarrow   (T \\times d_v) $\n",
        "\n",
        "Dot product of Queries and Keys:\n",
        "\n",
        "$ Q K^{T}   \\Rightarrow (T \\times d_{k}) \\times (d_{k} \\times T  )  \\Rightarrow   (T \\times T) $\n",
        "\n",
        "T query vectors and T key vectors (Input Sequence), so need TxT attention weights. Make Sense! Taking SoftMax doesn't change the shape.\n",
        "\n",
        " **Shapes as per the paper**\n",
        "\n",
        "$\n",
        "\\begin{array}{|c|c|} \\hline\n",
        "Object   &  Shape & values  \\\\ \\hline\n",
        "q_i, k_i  &  d_k  &  (64,) \\\\\n",
        "v_i   &   d_v   &   (64,)  \\\\\n",
        "x_i   &   d_{model}   & (512,)  \\\\\n",
        "W^{Q}, W^{K}  &   d_{model} \\times d_k   &   (512, 64)  \\\\\n",
        "W^{V}   &   d_{model} \\times d_v   &  (512,64)  \\\\ \\hline\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "**Batch consideration**\n",
        "\n",
        "In code, a batch of N samples are processed at a time. Everyting would be  **N times**, like: $ N \\times T \\times d_k $ instead of just $ T \\times d_k$."
      ],
      "metadata": {
        "id": "YydtWlogofdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fianl Scaled Dot Product Attention** equation inside each attention head with **Queries(Q)**, **Keys(Q)**, and **Values(V)**, which returns a Attention vector.\n",
        "\n",
        "<center>\n",
        "<img src= https://www.dropbox.com/scl/fi/pfr6b522rccvp7bkuqilg/Scaled_dot_product_Attention.png?rlkey=nyba4vhest5995hdain8igayq&raw=1 width=250px/>\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "$$Attention(Q, K, V) = softmax(\\frac{QK^T)}{\\sqrt{d_k}})V$$\n",
        "\n",
        "\n",
        "\n",
        "$ Shape \\ of \\ attention \\ output = (T \\times T) \\times (T \\times d_v)   \\Rightarrow  (T \\times d_v)  $"
      ],
      "metadata": {
        "id": "HAgRDAQinLxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementation with dummy data**"
      ],
      "metadata": {
        "id": "_-Np1Dra543b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf_e2mFOmTz4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T, d_k, d_v = 4, 6, 6   # T= Number of terms\n",
        "q = np.random.randn(T, d_k) # 4X6\n",
        "k = np.random.randn(T, d_k) # 4X6\n",
        "v = np.random.randn(T, d_v) # 4X6"
      ],
      "metadata": {
        "id": "vpn1dlRwmcW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owmD4WWZmzb0",
        "outputId": "4b3b6fe8-4613-41ad-e24f-955a2004292b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " [[ 0.03076571 -0.66596084 -0.57981773  0.78204297 -2.20486326  0.26485014]\n",
            " [-0.67861224  0.49590148  0.13423506 -0.4223308   0.7713341  -1.18389824]\n",
            " [ 2.0906267   0.27694552 -1.1727269   1.66930336 -0.14534764  0.74502536]\n",
            " [-1.13411916  1.13097436 -1.65952086 -2.06938392  2.02277881  0.37287297]]\n",
            "K\n",
            " [[-1.22715745  1.49918116  0.89824522 -0.08052928 -2.21706475 -0.26191323]\n",
            " [ 0.71747107  0.009494   -0.69504954  0.02322563 -1.25806545 -0.11341351]\n",
            " [ 0.91258838  1.11400796  0.46615481 -0.170631   -0.17803358 -0.97992068]\n",
            " [ 1.05682361  0.70185397  0.62036256 -0.63105621 -0.80103572 -2.07085978]]\n",
            "V\n",
            " [[-0.48742851  1.76485745  0.36118831  0.84109156  1.64645719  2.49370612]\n",
            " [-0.41495437 -0.82550919 -1.8979914   1.13401928 -0.17321176 -0.39958629]\n",
            " [-0.86078293  0.08485973  0.13252766  0.6324223   0.25428529 -0.25960824]\n",
            " [-1.01291217  0.73806086  0.21143847 -0.17391314 -1.78605033 -0.59720633]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.matmul(q, k.T) # Dot product"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kxm5ZJ2pSQK",
        "outputId": "de35b3ca-0e9e-4396-e6fe-c6fb1e00f5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.19901068,  3.18074107, -0.98452726, -0.07039688],\n",
              "       [ 0.33077719, -1.4214042 ,  1.09058459,  1.81448694],\n",
              "       [-3.21104763,  2.45482831,  0.68070498, -0.80355552],\n",
              "       [-2.81902441, -2.28465071, -0.92107062, -2.52087782]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Why we need sqrt(d_k) in denominator\n",
        "q.var(), k.var(), np.matmul(q, k.T).var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfvhpKSYpmKk",
        "outputId": "9273f795-8ce1-4505-cf27-a180a7bb5203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.341162827829847, 0.936821161732119, 4.089120011404893)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_dot_product = np.matmul(q, k.T) / math.sqrt(d_k)\n",
        "print(scaled_dot_product)\n",
        "q.var(), k.var(), scaled_dot_product.var()\n",
        "# Notice the reduction in variance of the product"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2D_3_01pr7I",
        "outputId": "4dc08149-60fc-4161-dd03-1daec32bb047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.30599064  1.29853211 -0.40193157 -0.0287394 ]\n",
            " [ 0.13503922 -0.58028584  0.4452293   0.74076119]\n",
            " [-1.31090471  1.00217946  0.27789664 -0.32805017]\n",
            " [-1.1508619  -0.93270475 -0.3760255  -1.02914406]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.341162827829847, 0.936821161732119, 0.6815200019008156)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_dot_product.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOUVRvTZq2SW",
        "outputId": "df5bcfba-5a3d-4adc-9b35-b7094067236a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Masking**\n",
        "* This is to ensure words don't get context from words generated in the future.\n",
        "* Not required in the encoders, but required in the decoders"
      ],
      "metadata": {
        "id": "xRlDnc8erHvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.tril(np.ones( (T, T) ))\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C1UZWj7rVKs",
        "outputId": "8576cfb8-3dba-43a9-ac34-7f4a0bf0f72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask[mask == 0] = -np.infty\n",
        "mask[mask == 1] = 0\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69Lbsom_rbjD",
        "outputId": "50825ad6-d34e-4277-b2ec-b5556af76dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_dot_product + mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIZ9Y63Erv37",
        "outputId": "e9e8e3aa-98d0-4274-b236-2150dd3dfafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.30599064,        -inf,        -inf,        -inf],\n",
              "       [ 0.13503922, -0.58028584,        -inf,        -inf],\n",
              "       [-1.31090471,  1.00217946,  0.27789664,        -inf],\n",
              "       [-1.1508619 , -0.93270475, -0.3760255 , -1.02914406]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax\n",
        "\n",
        "$ softmax = \\frac{\\exp(x_{i})}{\\sum_{j} \\exp({x_j})} $\n",
        "\n"
      ],
      "metadata": {
        "id": "35u68t6Pr88l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
      ],
      "metadata": {
        "id": "LvlHrv-atBTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax(scaled_dot_product + mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NedHyvCCvHK_",
        "outputId": "0a05c102-56ae-4775-b3d8-b7d4fa911f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.67157673, 0.32842327, 0.        , 0.        ],\n",
              "       [0.06248665, 0.63146158, 0.30605177, 0.        ],\n",
              "       [0.18039292, 0.22436955, 0.39149539, 0.20374214]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention = np.matmul (softmax(scaled_dot_product + mask),v)\n",
        "print(attention)\n",
        "attention.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ol2JKSEstFHT",
        "outputId": "f9b04c76-e747-4fef-e641-697cfb633d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.48742851  1.76485745  0.36118831  0.84109156  1.64645719  2.49370612]\n",
            " [-0.46362631  0.91412078 -0.38077887  0.93729584  1.04883557  1.54348157]\n",
            " [-0.55592966 -0.38502584 -1.13537887  0.96220057  0.07132949 -0.17595361]\n",
            " [-0.72439722  0.31674494 -0.26573278  0.61832334 -0.00619643  0.1368804 ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}