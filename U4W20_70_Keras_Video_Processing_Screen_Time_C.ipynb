{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakhanrajpatlolla/aiml-learning/blob/master/U4W20_70_Keras_Video_Processing_Screen_Time_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i35LuPCsxJaQ"
      },
      "source": [
        "\n",
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9wLqIUIyML-"
      },
      "source": [
        "## Learning Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD86AWh0yHG4"
      },
      "source": [
        "At the end of the experiment, you will be able to :\n",
        "\n",
        "* calculate the screen time of a character from a given video using deep learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvDJsz_mnH9j",
        "cellView": "form"
      },
      "source": [
        "#@title Experiment Walkthrough Video\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"<video width=\"800\" height=\"400\" controls>\n",
        "  <source src=\"https://cdn.talentsprint.com/talentsprint1/archives/sc/aiml/keras_video_processing_screen_time.mp4\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mE8sH-otYGH"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s7GpYrkCaSB"
      },
      "source": [
        "### History\n",
        "\n",
        "The screen time of an actor/character in a movie or an episode is very important. Many actors get paid according to their total screen time. Moreover, we also want to know how much time our favorite character acted on screen. So, have you ever wondered how can you calculate the total screen time of an actor? One of the best ways is by using deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y33c1pGWj8LW"
      },
      "source": [
        "### Description\n",
        "\n",
        "We will use a video clip of **`Tom and Jerry`** cartoon series and the model shall be trained on a video. The downloaded data is in the form of a video, which is nothing but a collection of a set of images. These images are called frames and can be combined to get the original video. So, a problem related to video data is not that different from an image classification or an object detection problem. There is just one extra step of extracting frames from the video.\n",
        "\n",
        "The Model will be evaluated (tested) on another video of **`Tom and Jerry`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2418775\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEzlYL4CDrmE"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"9959000490\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form",
        "outputId": "7c31f6ea-7889-4ab8-be42-78b609468246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import re\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"U4W20_70_Keras_Video_Processing_Screen_Time_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    from IPython.display import HTML, display\n",
        "    ipython.magic(\"sx wget -qq https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Video_Processing/Train_data.zip\")\n",
        "    ipython.magic(\"sx unzip --q Train_data.zip\")\n",
        "    ipython.magic(\"sx wget -qq https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Video_Processing/Test_data.zip\")\n",
        "    ipython.magic(\"sx unzip --q Test_data.zip\")\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getWalkthrough() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook, \"feedback_walkthrough\":Walkthrough ,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_inclass_mentor\": Mentor_support}\n",
        "\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://learn-iiith.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getWalkthrough():\n",
        "  try:\n",
        "    if not Walkthrough:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Walkthrough\n",
        "  except NameError:\n",
        "    print (\"Please answer Walkthrough Question\")\n",
        "    return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2418775&recordId=2450\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN9xA1xMwIq0"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c4Jb-bewLYL"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dense, InputLayer, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb2k9DC4xUnU"
      },
      "source": [
        "### Loading the video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNmxnBt6n4wo"
      },
      "source": [
        "train_videoFile = \"/content/Video_Processing/Tom_and_jerry_train.mp4\"\n",
        "\n",
        "# Create a directory to store all the frames\n",
        "all_images = \"/content/all_images\"\n",
        "os.mkdir(all_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfS-zv09x0XA"
      },
      "source": [
        "### Read the video, extract frames from it and save them as images\n",
        "\n",
        "For this task, use OpenCV as shown below and extract frames for every second"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxh2MX-alHmJ"
      },
      "source": [
        "def getVideoFrames(videopath, imagespath):\n",
        "\n",
        "  # Open the Video file using cv2.VideoCapture()\n",
        "  # Capturing the video from the given path\n",
        "  cap = cv2.VideoCapture(videopath)\n",
        "\n",
        "  # Frame rate of the video\n",
        "  # How many frames per video you want to capture\n",
        "  frameRate = cap.get(5)\n",
        "  i = 0\n",
        "\n",
        "  while True:\n",
        "      # Current frame number\n",
        "      # Capturing one frame per second\n",
        "      frameId = cap.get(1)\n",
        "\n",
        "      # Read frame by frame\n",
        "      ret, frame = cap.read()\n",
        "\n",
        "      if ret == False:\n",
        "          break\n",
        "\n",
        "      # Extract one frame for each second\n",
        "      if (frameId % math.floor(frameRate) == 0):\n",
        "\n",
        "          # Save each frame using cv2.imwrite()\n",
        "          cv2.imwrite(imagespath+'/frame'+str(i)+'.jpg',frame)\n",
        "          i+=1\n",
        "\n",
        "  # After loop release the VideoCapture and destroy all windows\n",
        "  cap.release()\n",
        "  cv2.destroyAllWindows()\n",
        "  return \"Successfully extracted the images!!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyYfWLZrZ1aL"
      },
      "source": [
        "getVideoFrames(\"/content/Train_data/Tom_and_jerry_train.mp4\", all_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsJX654c03xF"
      },
      "source": [
        "### Let us visualize an image (frame)\n",
        "\n",
        "We will first read the image using the imread() function of matplotlib, and then plot it using the imshow() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHsrxvubzcSq"
      },
      "source": [
        "img = plt.imread('/content/all_images/frame0.jpg')   # Reading the image by its name\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuRyZLKMr82T"
      },
      "source": [
        "Since the duration of the video is 4:58 minutes (298 seconds), we now have 298 images in total.\n",
        "\n",
        "In this problem, there are three classes as mentioned below and hence it is a multi-class classification problem\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "0 – The frame has neither JERRY nor TOM\n",
        "1 – JERRY is in the frame\n",
        "2 – TOM is in the frame\n",
        "```\n",
        "\n",
        "The `train_labels.csv` contains the respective labels for each extracted frame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "equWJJ4CciVR"
      },
      "source": [
        "### Label images for training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv3dD4wstGmZ"
      },
      "source": [
        "train_labels = '/content/Train_data/train_labels.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFtw2ge4tjip"
      },
      "source": [
        "df_train = pd.read_csv(train_labels)\n",
        "\n",
        "classes = df_train['Class'].unique().astype('str')\n",
        "print(\"Classes:\", classes)\n",
        "\n",
        "df_train['Image_ID'] = df_train['Image_ID'].apply(lambda x: all_images+'/'+x)\n",
        "\n",
        "labels = {0:'None', 1: 'Jerry', 2: 'Tom'}\n",
        "df_train['Labels'] = [labels[each] for each in df_train['Class']]\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itb2A6986FYk"
      },
      "source": [
        "### Visualize few Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCjuerKD1lez"
      },
      "source": [
        "eachClass = df_train.groupby('Class').first()\n",
        "eachClass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPLcnyH62oKm"
      },
      "source": [
        "for index, row in eachClass.iterrows():\n",
        "    img = plt.imread(row['Image_ID'])   # Reading the image by its name\n",
        "    plt.title(row['Labels'])\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toNojpJoy9be"
      },
      "source": [
        "### Input Data and Preprocessing\n",
        "\n",
        "To prepare this extracted images data as input to our neural network, the  below mentioned preprocessing steps are to be followed:\n",
        "\n",
        "* Read all images one by one\n",
        "* Resize each image to (224, 224, 3) for the input to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4sTUVr02VI_"
      },
      "source": [
        "def resizeFeatures(image_filenames):\n",
        "  res_img = []\n",
        "  for each_img in image_filenames:\n",
        "    img = plt.imread(each_img)\n",
        "    resized_img = cv2.resize(img, (224,224)).astype(int)\n",
        "    res_img.append(resized_img)\n",
        "\n",
        "  features = np.array(res_img)\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUmhcpDtbJM_"
      },
      "source": [
        "features = resizeFeatures(df_train['Image_ID'])\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OikOvI3s27TU"
      },
      "source": [
        "Since there are three classes, we will one hot encode them using the `to_categorical()` function of `keras.utils`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7xnZJmH25_N"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y = df_train[\"Class\"]\n",
        "\n",
        "# one hot encoding Classes\n",
        "one_hot_y = to_categorical(y)\n",
        "one_hot_y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dra2w6jQ3eAs"
      },
      "source": [
        "## Transfer Learning\n",
        "\n",
        "Since we have only 298 images, so it will be difficult to train a neural network with this little dataset. Here comes the concept of transfer learning.\n",
        "\n",
        "With the help of transfer learning, we can use features generated by a model trained on a large dataset into our model. Here we will use the VGG16 model trained on the “imagenet” dataset. For this, we are using TensorFlow high-level API Keras. With Keras, you can directly import the VGG16 model as shown in the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddk4vE7l30OP"
      },
      "source": [
        "# include_top=False to remove the top layer\n",
        "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxwA4XQc46a0"
      },
      "source": [
        "VGG16 model trained with imagenet dataset predicts on lots of classes, but in this problem, we are only having three classes, either `\"Tom\" or \"Jerry\" or \"None\"`.\n",
        "\n",
        "That’s why above we are using `include_top = False`, which signifies that we are not including fully connected layers from the VGG16 model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmfwtW7Z4CZj"
      },
      "source": [
        "Before passing any input to the model, it is important to preprocess it as per the model’s requirement. Use the `preprocess_input()` function of `keras.applications.vgg16` to perform this step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdUN8X494Pe_"
      },
      "source": [
        "# Preprocessing the input data\n",
        "X = preprocess_input(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lpCxBD84V4h"
      },
      "source": [
        "Generate a validation set using the  `train_test_split()` function of the sklearn to check the performance of the model on unseen images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdyH8_JJ4mDY"
      },
      "source": [
        "# Preparing the validation set\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, one_hot_y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"Training Features:\", X_train.shape)\n",
        "print(\"Training Labels:\", y_train.shape)\n",
        "print(\"Validation Features:\", X_valid.shape)\n",
        "print(\"Validation Labels:\", y_valid.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ_QVVk35zI8"
      },
      "source": [
        "Pass the above extracted `X_train and X_valid` features as **input to the pre-trained `vgg_model`** and get the predicted data and then use those features to retrain the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWtbZMQX50sE"
      },
      "source": [
        "X_train_predicted = vgg_model.predict(X_train)\n",
        "X_valid_predicted = vgg_model.predict(X_valid)\n",
        "\n",
        "print(\"Training Features:\", X_train_predicted.shape)\n",
        "print(\"Validation Features:\", X_valid_predicted.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dMJOPOexf5k"
      },
      "source": [
        "### VGG16\n",
        "![picture](https://miro.medium.com/max/788/1*_Lg1i7wv1pLpzp2F4MLrvw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTYzD-ad7B7r"
      },
      "source": [
        "Notice that the output features from VGG16 model will be having shape `7*7*512`\n",
        "\n",
        "Since we are not including fully connected layers from the VGG16 model, we need to create a model with some fully connected layers and an output layer with 3 classes, either `\"Tom\" or \"Jerry\" or \"None\"`.\n",
        "\n",
        "In order to pass the above extracted `X_train and X_valid` features to our neural network, we have to reshape it to 1-D, which will be an input shape for our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQYEw8zL7lCe"
      },
      "source": [
        "# Converting to 1-D\n",
        "X_train_reshaped = X_train_predicted.reshape(208, 7*7*512)\n",
        "X_valid_reshaped = X_valid_predicted.reshape(90, 7*7*512)\n",
        "\n",
        "print(\"Training Features:\", X_train_reshaped.shape)\n",
        "print(\"Validation Features:\", X_valid_reshaped.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWkdn1y17xUn"
      },
      "source": [
        "Now, preprocess the images and normalize by dividing the vector with it's maximum value, which helps the model to converge faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INM_9wEL7w_l"
      },
      "source": [
        "# Normalize the data\n",
        "X_train_centered = X_train_reshaped/X_train_reshaped.max()\n",
        "X_valid_centered = X_valid_reshaped/X_train_reshaped.max()\n",
        "\n",
        "print(\"Training Features:\", X_train_centered.shape)\n",
        "print(\"Validation Features:\", X_valid_centered.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtQYvaqo8bdB"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXcZQVbK8qxC"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(InputLayer(shape=(7*7*512,))) # Input layer\n",
        "\n",
        "model.add(Dense(1024, activation='sigmoid')) # Hidden layer\n",
        "\n",
        "model.add(Dropout(0.5)) # Dropout layer\n",
        "\n",
        "model.add(Dense(512, activation='sigmoid')) # Hidden layer\n",
        "\n",
        "model.add(Dropout(0.5)) # Dropout layer\n",
        "\n",
        "model.add(Dense(256, activation='sigmoid')) # Hidden layer\n",
        "\n",
        "model.add(Dropout(0.5)) # Dropout layer\n",
        "\n",
        "model.add(Dense(3, activation='softmax')) # Output layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXJcStpQm78F"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqgn2-DD9OJ3"
      },
      "source": [
        "### Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVjmCMPpF_wS"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbdDSz3A9TGu"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN7bGcjNGEoU"
      },
      "source": [
        "history = model.fit(X_train_centered, y_train, epochs=100, validation_data=(X_valid_centered, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvijPWJ7dlQZ"
      },
      "source": [
        "### Evaluate the model\n",
        "\n",
        "Calculating the screen time on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9agHYG0FBha"
      },
      "source": [
        "# Create a directory to store all the frames\n",
        "test_images = \"test_images\"\n",
        "os.mkdir(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfcgSMcpEme-"
      },
      "source": [
        "getVideoFrames(\"/content/Test_data/Tom_and_Jerry_test.mp4\", test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVPGvMDtEevk"
      },
      "source": [
        "### Load the Test Data\n",
        "\n",
        "Iterate over the **`Test Images`** directory to extract all the test Id's"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOKqGpJDaK2D"
      },
      "source": [
        "def getIDs(directory):\n",
        "  ids = []\n",
        "  for filename in os.listdir(directory):\n",
        "      if filename.endswith(\".jpg\"):\n",
        "          ids.append(os.path.join(directory, filename))\n",
        "      else:\n",
        "          continue\n",
        "  return ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJOFcybtbwOO"
      },
      "source": [
        "test_ids = getIDs(test_images)\n",
        "len(test_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49DWwnIwFAD6"
      },
      "source": [
        "Since the duration of the video is 3:1 minutes (186 seconds), we now have 186 images in total.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6YIwbpZep-7"
      },
      "source": [
        "### Input Data and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IvfHbaka6SE"
      },
      "source": [
        "test_features = resizeFeatures(test_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt14HYGsHSph"
      },
      "source": [
        "# preprocessing the images\n",
        "preprocessed_features = preprocess_input(test_features)\n",
        "\n",
        "# extracting features from the images using pretrained model\n",
        "output_features = vgg_model.predict(preprocessed_features)\n",
        "\n",
        "# converting the images to 1-D form\n",
        "reshaped_features = output_features.reshape(test_features.shape[0], 7*7*512)\n",
        "\n",
        "# Normalized images\n",
        "zero_centered_features = reshaped_features/reshaped_features.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l4vDUiee3AN"
      },
      "source": [
        "### Make predictions on the test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StVwj1ite83g"
      },
      "source": [
        "pred = model.predict(zero_centered_features)\n",
        "predictions =np.argmax(pred,axis=1)\n",
        "\n",
        "print(\"The screen time of JERRY is\", predictions[predictions==1].shape[0], \"seconds\")\n",
        "print(\"The screen time of TOM is\", predictions[predictions==2].shape[0], \"seconds\")\n",
        "print(\"The screen time of Neither JERRY nor TOM is\", predictions[predictions==0].shape[0], \"seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-b-CEcpkOCi"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12SwfIaukOCj"
      },
      "source": [
        "#@title State True or False: In VGG16 model, the parameter 'include_top = False' specifies to include the fully-connected layer at the top of the network { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"FALSE\" #@param [\"\",\"TRUE\", \"FALSE\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0DzTLAMkOCj"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlMms_IykOCj"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"good\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHnMPzf_kOCk"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRKla4emkOCk"
      },
      "source": [
        "#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Walkthrough = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtkER1AwkOCk"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-UhjrabkOCk"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MUHD7LC0kOCk",
        "outputId": "b1f0b4f4-f85a-4972-c928-04e0f68445b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 2450\n",
            "Date of submission:  05 Apr 2025\n",
            "Time of submission:  15:26:02\n",
            "View your submissions: https://learn-iiith.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ]
}