{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakhanrajpatlolla/aiml-learning/blob/master/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaHNxUntdN6a"
      },
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Installing and importing packages**"
      ],
      "metadata": {
        "id": "0ho6wusQn_ll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmxD_RmDn0ju"
      },
      "outputs": [],
      "source": [
        "!pip -q install openai\n",
        "!pip -q install langchain\n",
        "!pip -q install langchain-openai\n",
        "!pip -q install pypdf\n",
        "!pip -q install chromadb\n",
        "!pip -q install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "1KQwtAysn8KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Authentication for OpenAI API**"
      ],
      "metadata": {
        "id": "5XF7eEn4oaUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/openapi_key.txt')\n",
        "api_key = f.read()\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "openai.api_key= os.getenv('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "GdnkoLQvoEKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading the documents**\n",
        "\n",
        "[PDF Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf)"
      ],
      "metadata": {
        "id": "MifqadjtoW8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load PDF\n",
        "loaders = [\n",
        "    # Duplicate documents on purpose\n",
        "    PyPDFLoader(\"/content/pca_d1.pdf\"),\n",
        "    PyPDFLoader(\"/content/ens_d2.pdf\"),\n",
        "    PyPDFLoader(\"/content/ens_d2.pdf\"),\n",
        "]\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())"
      ],
      "metadata": {
        "id": "IrWOIYmAofck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "8N__DNS6r7kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Splitting of document**\n",
        "\n",
        "[Recursively split by character](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n",
        "\n",
        "[Split by character](https://python.langchain.com/docs/modules/data_connection/document_transformers/character_text_splitter)\n"
      ],
      "metadata": {
        "id": "jX09PKcPoZnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "EDOPJOxYtSvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "#from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 50\n",
        ")"
      ],
      "metadata": {
        "id": "Y1qix4F0saSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = text_splitter.split_documents(docs)\n",
        "print(len(splits))\n",
        "splits"
      ],
      "metadata": {
        "id": "wHNEkTYTsbKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Embeddings**\n",
        "\n",
        "Let's take our splits and embed them."
      ],
      "metadata": {
        "id": "HRYXCWXxsmH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "embedding = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "UAy8WJblsiXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding"
      ],
      "metadata": {
        "id": "paxP3GEUuSaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Understanding similarity search with a toy example**"
      ],
      "metadata": {
        "id": "OtqIaNDTsq3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"i like dogs\"\n",
        "sentence2 = \"i like cats\"\n",
        "sentence3 = \"the weather is ugly, too hot outside\""
      ],
      "metadata": {
        "id": "prWJAL50szYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding1 = embedding.embed_query(sentence1)\n",
        "embedding2 = embedding.embed_query(sentence2)\n",
        "embedding3 = embedding.embed_query(sentence3)"
      ],
      "metadata": {
        "id": "iAeeFUsis1wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embedding1), len(embedding2), len(embedding3)"
      ],
      "metadata": {
        "id": "nGc4yWSas4RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.dot(embedding1, embedding2), np.dot(embedding1, embedding3),np.dot(embedding2, embedding3)"
      ],
      "metadata": {
        "id": "lDbU70Zss6j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Vectorstores**"
      ],
      "metadata": {
        "id": "osU6iYMLswJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma # Light-weight and in memory"
      ],
      "metadata": {
        "id": "lIFZ53aAtLEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = 'docs/chroma/'\n",
        "!rm -rf ./docs/chroma  # remove old database files if any"
      ],
      "metadata": {
        "id": "oI9whQjptqav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits, # splits we created earlier\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory # save the directory\n",
        ")"
      ],
      "metadata": {
        "id": "BPeW91Qdtxmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb.persist() # Let's **save vectordb** so we can use it later!"
      ],
      "metadata": {
        "id": "HncL1qsEt3RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectordb._collection.count()) # same as number of splites"
      ],
      "metadata": {
        "id": "BxEeyHgjt0Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Similarity Search**"
      ],
      "metadata": {
        "id": "0-2WgevOt_TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"how does pca reduce the dimension?\""
      ],
      "metadata": {
        "id": "_OUYY3Mot2nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = vectordb.similarity_search(question,k=3) # k --> No. of doc as return\n",
        "print(len(docs))\n",
        "print(docs[0].page_content)\n",
        "print(docs[1].page_content)\n",
        "print(docs[2].page_content)"
      ],
      "metadata": {
        "id": "DGHDzVj4uyx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Edge case where failure may happen**\n",
        "\n",
        "1. Lack of Diversity : Semantic search fetches all similar documents, but does not enforce diversity.\n",
        "\n",
        "    - Notice that we're getting duplicate chunks (because of the duplicate `ens_d2.pdf` in the index). `docs[0]` and `docs[1]` are indentical.\n",
        "\n",
        "  **Addressing Diversity - MMR-Maximum Marginal Relevance**\n",
        "\n",
        "2. Lack of spefificity:  The question may be from a particular doc but answer may contain information from other doc.\n",
        "\n",
        "  **Addressing Specificity: Working with metadata - Manually**\n",
        "\n",
        "  **Working with metadata using self-query retriever -Automatically**"
      ],
      "metadata": {
        "id": "qBh5fXjrwHw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  **Example 1. Addressing Diversity - MMR-Maximum Marginal Relevance**"
      ],
      "metadata": {
        "id": "UJNlYo983oZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question= 'how ensemble method works?'\n",
        "docs = vectordb.similarity_search(question,k=2) # Without MMR"
      ],
      "metadata": {
        "id": "TBdrnHC3uy0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "id": "L9huk6xy3xWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[1]"
      ],
      "metadata": {
        "id": "vHBRG8p-4RF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[2]"
      ],
      "metadata": {
        "id": "4OxEZQYM4R1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[3]"
      ],
      "metadata": {
        "id": "giz14ycM4Tmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Addressing Diversity - MMR-Maximum Marginal Relevance"
      ],
      "metadata": {
        "id": "ns8VCrc25Y2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_with_mmr=vectordb.max_marginal_relevance_search(question, k=3, fetch_k=6) # With MMR"
      ],
      "metadata": {
        "id": "Qg70_vhh5e7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_with_mmr[0]"
      ],
      "metadata": {
        "id": "3r_KetcH5wDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_with_mmr[1]"
      ],
      "metadata": {
        "id": "vppdq9Bn5y0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_with_mmr[2]"
      ],
      "metadata": {
        "id": "uLqM_I87-thH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Example 2. Addressing Specificity: Working with metadata - Manually**"
      ],
      "metadata": {
        "id": "vZYR1s6x_bpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Without metadata information\n",
        "question = \"what is the role of variance in pca?\"\n",
        "docs = vectordb.similarity_search(question,k=5)\n",
        "for doc in docs:\n",
        "    print(doc.metadata) # metadata contains information about from which doc the answer has been fetched"
      ],
      "metadata": {
        "id": "9GzDPqSVAX6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice above, the last information is from 'ens_d2' doc."
      ],
      "metadata": {
        "id": "sMv4a6P_A0rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With metadata information\n",
        "question = \"what is the role of variance in pca?\"\n",
        "docs = vectordb.similarity_search(\n",
        "    question,\n",
        "    k=5,\n",
        "    filter={\"source\":'/content/pca_d1.pdf'} # manually passing metadata, using metadata filter.\n",
        ")\n",
        "\n",
        "for doc in docs:\n",
        "    print(doc.metadata)"
      ],
      "metadata": {
        "id": "P-9NluQR_VUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Addressing Specificity -Automatically: Working with metadata using self-query retriever**](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query)\n",
        "\n"
      ],
      "metadata": {
        "id": "tiFz42WiCJUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Additional tricks: Compression**\n",
        "\n",
        "Another approach for improving the quality of retrieved docs is compression. Information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
        "\n",
        "[Contextual compression](https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression) is meant to fix this."
      ],
      "metadata": {
        "id": "qQG1e-boGTTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Retrieval + Question Answering :  Connecting with LLMs**"
      ],
      "metadata": {
        "id": "2aIGynCLGw0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_name = \"gpt-3.5-turbo\"\n",
        "print(llm_name)"
      ],
      "metadata": {
        "id": "ehEr-pUMGMrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is principal component analysis?\"\n",
        "docs = vectordb.max_marginal_relevance_search(question, k=2, fetch_k=3)\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "Oc8EF156G3uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "id": "OpV03kILH6vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[1]"
      ],
      "metadata": {
        "id": "Mr4vt1ArIEWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#docs[2]"
      ],
      "metadata": {
        "id": "qpFPXVzbIFI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#docs[3]"
      ],
      "metadata": {
        "id": "RWbWEvssIV5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**[RetrievalQA chain](https://docs.smith.langchain.com/cookbook/hub-examples/retrieval-qa-chain)**\n",
        "\n",
        "####**[Vector store-backed retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore)**"
      ],
      "metadata": {
        "id": "eC-7gtzfHQYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model_name=llm_name, temperature=0)"
      ],
      "metadata": {
        "id": "vVMm3gMzG-4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "PnUVz2UmHWAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is principal component analysis?\"\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever(), return_source_documents=True)\n",
        "\n",
        "result = qa_chain.invoke({\"query\": question})"
      ],
      "metadata": {
        "id": "oFygf_Y7HWQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"result\"]"
      ],
      "metadata": {
        "id": "A-yxS2awHWT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"source_documents\"]"
      ],
      "metadata": {
        "id": "wBOgOkjPOvmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Under the hood? --> Understanding RAG Prompt**"
      ],
      "metadata": {
        "id": "Y-ea4zDtRnYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchainhub"
      ],
      "metadata": {
        "id": "9TLWjL0jRzeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "prompt"
      ],
      "metadata": {
        "id": "2USwNz7lRn0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use three sentences maximum.Keep the answer as concise as possible."
      ],
      "metadata": {
        "id": "BqGrJNIWLqks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build prompt\n",
        "from langchain.prompts import PromptTemplate\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)"
      ],
      "metadata": {
        "id": "tB6IsTdSLvgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QA_CHAIN_PROMPT"
      ],
      "metadata": {
        "id": "IQasHvgTSX92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run chain\n",
        "from langchain.chains import RetrievalQA\n",
        "qa_chain = RetrievalQA.from_chain_type(llm,\n",
        "                                       retriever=vectordb.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 2, \"fetch_k\":6} ), # \"k\":2, \"fetch_k\":3\n",
        "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
        "                                       return_source_documents=True\n",
        "                                       )"
      ],
      "metadata": {
        "id": "-LJ3BqKjL89C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain"
      ],
      "metadata": {
        "id": "ZTMm7LokcWdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 1**"
      ],
      "metadata": {
        "id": "SHn6k5ePZ2oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is principal component analysis?\"\n",
        "result = qa_chain.invoke({\"query\": question})\n",
        "result[\"source_documents\"]"
      ],
      "metadata": {
        "id": "UGORZtkrMvx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"result\"]"
      ],
      "metadata": {
        "id": "-FxSBsmdXtgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2**"
      ],
      "metadata": {
        "id": "MV-VTth3Zq6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What does it say about variance in context of both PCA and Ensemble?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"source_documents\"]"
      ],
      "metadata": {
        "id": "J1LmenL3ZNrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"result\"]"
      ],
      "metadata": {
        "id": "i1w5C9OOZjEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RetrievalQA chain types : [Map reduce, Refine, Map rerank(Legacy)](https://python.langchain.com/docs/modules/chains/)**\n",
        "\n",
        "- Whatever techniques we havae used is stuff method (default - chain_type=\"stuff\") and there is only one call to LLM"
      ],
      "metadata": {
        "id": "ksvPzTKHdjYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain_mr = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 4, \"fetch_k\":8}),\n",
        "    chain_type=\"map_reduce\"\n",
        ")"
      ],
      "metadata": {
        "id": "_gJGOi7vczPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question =\"What principal component analysis?\"\n",
        "result = qa_chain_mr({\"query\": question})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "id": "ELbLfWWAdxOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Make it like Chatbot : Adding Memory**"
      ],
      "metadata": {
        "id": "7NbLm1boaRT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")"
      ],
      "metadata": {
        "id": "Ngcffei9fHNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run chain\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "qa= ConversationalRetrievalChain.from_llm(llm,\n",
        "                                       retriever=vectordb.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 4, \"fetch_k\":8} ), # \"k\":2, \"fetch_k\":3\n",
        "                                       memory=memory\n",
        "                                       )"
      ],
      "metadata": {
        "id": "L994Ysxkff9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"tell me something about PCA\"\n",
        "result = qa.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "3jkbHJvXf9Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result['answer']"
      ],
      "metadata": {
        "id": "BHxC5yLRhLHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"please list point-wise,  how does pca works?\"\n",
        "result = qa({\"question\": question})"
      ],
      "metadata": {
        "id": "dN3atqo6hfc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['answer'])"
      ],
      "metadata": {
        "id": "2Z7pSOdThii4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what do we get from covariance matrix for doing PCA?\"\n",
        "result = qa({\"question\": question})\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "id": "rH_Z97sJiWLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Download the vector DB**"
      ],
      "metadata": {
        "id": "GHJ1ujNcuFNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the entire folder\n",
        "!zip -r /content/docs.zip /content/docs"
      ],
      "metadata": {
        "id": "Defg2TBIuIeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/docs.zip\")"
      ],
      "metadata": {
        "id": "wm3nYLKJuKYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Upload the vector db from previous step and unzip**"
      ],
      "metadata": {
        "id": "BMa6GdcUuXcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/docs.zip  -d /"
      ],
      "metadata": {
        "id": "VXn0d0k6ubPm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}