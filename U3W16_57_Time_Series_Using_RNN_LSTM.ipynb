{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakhanrajpatlolla/aiml-learning/blob/master/U3W16_57_Time_Series_Using_RNN_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvpnKs2ZpHmQ"
      },
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMRMCLwCpMsS"
      },
      "source": [
        "## Not for Grading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fea3yZGC7O-F"
      },
      "source": [
        "### Learning Objectives\n",
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* know sequence modelling\n",
        "* understand cell structure of basic RNN, LSTM, and GRU\n",
        "* implement RNNs -**simpleRNN** layers in sequence modelling problem\n",
        "\n",
        "Additional Concepts :\n",
        "* use **Lambda layers** to reshape the input and scale the output\n",
        "* use the **Huber loss** during training\n",
        "* Adjusting the learning rate dynamically\n",
        "\n",
        "We are going to use a synthetic timeseries dataset  that has been created for learning purpose. The case study is implmented in **Tenserflow & Keras**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZkfJyRz8fP3"
      },
      "source": [
        "### Introduction : Sequence Modelling:\n",
        "\n",
        "Say, we are going to develop a language model having a vocabulary of 1000 words. The ith training example being-\" A quick brown fox jumps over the lazy dog\".\n",
        "\n",
        "![Img](https://cdn.iisc.talentsprint.com/CDS/Images/Temporal_Sequence.PNG)\n",
        "\n",
        "\n",
        "Here, each word is a sequence one after another and we, call it temporal sequences (**time steps**).\n",
        "\n",
        "**Notation:**\n",
        "\n",
        "![Img](https://cdn.iisc.talentsprint.com/CDS/Images/Notation1.PNG)\n",
        "\n",
        "\n",
        "* Since the vocabulary is 1000 words, each sequence of each training example can be one-hot encoded into a 1D vector of size 1000 i.e. number of units in a single training example also known as **feature**.\n",
        "* For one training example with 9 temporal sequences the shape of the input:(1000,9).\n",
        "* For say, 10 such examples (**batch size** =10), we have to create a 3D tensor having the shape of (10,1000,9).\n",
        "\n",
        "**Notation:**\n",
        "\n",
        "![Img](https://cdn.iisc.talentsprint.com/CDS/Images/Tensor_shape_3D.PNG)\n",
        "\n",
        "\n",
        "We can take a 2D slice of the above 3D tensor for each time step and pass them as **input** having shape of (m,nx) for the corresponding time sequence/step in RNN model.\n",
        "\n",
        "* Note: The 3D tensor can also be arranged like:**[batch, timesteps, feature]**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSvlKHJ05pQd"
      },
      "source": [
        "## Setup Steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t-K9H-B5pQe"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_CGILrZ5pQe"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "EFYcjfU65pQe"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"U3W16_57_Time_Series_Using_RNN_LSTM\" #name of the notebook\n",
        "Answer = \"Ungraded\"\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    from IPython.display import HTML, display\n",
        "    ipython.magic(\"sx wget -qq https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Time_Series.csv\")\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getComplexity() and getAdditional() and getConcepts() and getComments():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"feedback_experiments_input\" : Comments, \"notebook\" : notebook}\n",
        "\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://learn-iiith.talentsprint.com/notebook_submissions\")\n",
        "        # print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "      return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5kDhl0kp4ML"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOjujz601HcS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3ZFrzpCpyTo"
      },
      "source": [
        "## Uploading Synthetic Data for time series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eNZaRZ4DUyh"
      },
      "outputs": [],
      "source": [
        "!wget -qq https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Time_Series.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_iJgeQDyyj0"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/Time_Series.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7NRCe4Ty7A3"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73sfXD75t4td"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df.Time_stamp, df.Magnitude)\n",
        "plt.xlabel(\"Time\")   # Label the x-axis\n",
        "plt.ylabel(\"Magnitude\")  # Label the y-axis\n",
        "plt.grid(True)     # Overlay a grid on the graph\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbW2GxrgpuZF"
      },
      "source": [
        "## Split the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhkuHzlol2VW"
      },
      "outputs": [],
      "source": [
        "time=df.Time_stamp\n",
        "series=df.Magnitude\n",
        "\n",
        "# Define the split time\n",
        "split_time = 1000\n",
        "\n",
        "# Get the train set\n",
        "time_train = time[:split_time]\n",
        "x_train = series[:split_time]\n",
        "\n",
        "# Get the validation set\n",
        "time_valid = time[split_time:]\n",
        "x_valid = series[split_time:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP6a2ckVqThP"
      },
      "source": [
        "## Prepare Features and Labels\n",
        "\n",
        "[**Preparing Time Series Features and Labels**](https://colab.research.google.com/drive/1kGhGgYdIFGxKwuC269J7seoRy3O1NPyB?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SawHhwx3qOMC"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "window_size = 20\n",
        "batch_size = 32\n",
        "shuffle_buffer_size = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sTTIOCbyShY"
      },
      "outputs": [],
      "source": [
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    \"\"\"Generates dataset windows\n",
        "\n",
        "    Args:\n",
        "      series (array of float) - contains the values of the time series\n",
        "      window_size (int) - the number of time steps to include in the feature\n",
        "      batch_size (int) - the batch size\n",
        "      shuffle_buffer(int) - buffer size to use for the shuffle method\n",
        "\n",
        "    Returns:\n",
        "      dataset (TF Dataset) - TF Dataset containing time windows\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a TF Dataset from the series values\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "\n",
        "    # Window the data but only take those with the specified size\n",
        "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "\n",
        "    # Flatten the windows by putting its elements in a single batch\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "\n",
        "    # Create tuples with features and labels\n",
        "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
        "\n",
        "    # Shuffle the windows\n",
        "    dataset = dataset.shuffle(shuffle_buffer)\n",
        "\n",
        "    # Create batches of windows\n",
        "    dataset = dataset.batch(batch_size).prefetch(1)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvmPMWqhqkUh"
      },
      "outputs": [],
      "source": [
        "# Generate the dataset windows\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXqdqRrn3mM0"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDUsp8n99Zhd"
      },
      "outputs": [],
      "source": [
        "# Print shapes of feature and label\n",
        "for window in dataset.take(1):\n",
        "  print(f'shape of feature: {window[0].shape}')\n",
        "  print(f'shape of label: {window[1].shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvzrTbBLHTx2"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "The model is composed mainly of [SimpleRNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN) layers. This type of RNN simply routs its output back to the input.  We are  stacking two of these layers in in this model so the first one should have `return_sequences` set to `True`.\n",
        "\n",
        "As mentioned in the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN#call_arguments), `SimpleRNN` layers expect a 3-dimensional tensor input with the shape `[batch, timesteps, feature`]. With that, we need to reshape the window from `(32, 20)` to `(32, 20, 1)`. This means the 20 datapoints in the window will be mapped to 20 timesteps of the RNN. We can do this reshaping in a separate cell but we can also do this within the model itself by using [Lambda](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda) layers. Notice the first layer below. It defines a lambda function that adds a dimension at the last axis of the input. That's exactly the transformation we need. For the `input_shape`, we can specify `None`  if we want to be the model to be more flexible with the number of timesteps. Alternatively, we can set it to `window_size` .\n",
        "\n",
        "Normally, we can just a have a `Dense` layer output . However, helping the training by scaling up the output to around the same figures as your labels. This will depend on the [activation functions](https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions) in the model. `SimpleRNN` uses *tanh* by default and that has an output range of `[-1,1]`. Thus there is an  another `Lambda()` layer to scale the output by 100 before it adjusts the layer weights. Try  removing this layer later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4nblWkqg1NL"
      },
      "outputs": [],
      "source": [
        "# Build the Model\n",
        "model_tune = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),input_shape=[window_size]),\n",
        "  tf.keras.layers.SimpleRNN(40, return_sequences=True),\n",
        "  tf.keras.layers.SimpleRNN(40),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "model_tune.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SI_wtyh4PV1"
      },
      "source": [
        "## Tune the Learning Rate\n",
        "\n",
        "You will then tune the learning rate as before. You will define a learning rate schedule that changes this hyperparameter dynamically. You will use the [Huber Loss](https://en.wikipedia.org/wiki/Huber_loss) as your loss function to minimize sensitivity to outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JuO12ZOq4Gy"
      },
      "outputs": [],
      "source": [
        "# Set the learning rate scheduler\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(momentum=0.9)\n",
        "\n",
        "# Set the training parameters\n",
        "model_tune.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer)\n",
        "\n",
        "# Train the model\n",
        "history = model_tune.fit(dataset, epochs=100, callbacks=[lr_schedule])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnwxnPNhdwUI"
      },
      "source": [
        "You can visualize the results and pick an optimal learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5He3pp-Hj758"
      },
      "outputs": [],
      "source": [
        "# Define the learning rate array\n",
        "lrs = 1e-8 * (10 ** (np.arange(100) / 20))\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Set the grid\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot the loss in log scale\n",
        "plt.semilogx(lrs, history.history[\"loss\"])\n",
        "\n",
        "# Increase the tickmarks size\n",
        "plt.tick_params('both', length=10, width=1, which='both')\n",
        "\n",
        "# Set the plot boundaries\n",
        "plt.axis([1e-8, 1e-3, 0, 50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKLJz8umd1wO"
      },
      "source": [
        "You can change the boundaries of the graph if you want to zoom in. The cell below chooses a narrower range so you can see more clearly where the graph becomes unstable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-XbgGzpuuVF"
      },
      "outputs": [],
      "source": [
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Set the grid\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot the loss in log scale\n",
        "plt.semilogx(lrs, history.history[\"loss\"])\n",
        "\n",
        "# Increase the tickmarks size\n",
        "plt.tick_params('both', length=10, width=1, which='both')\n",
        "\n",
        "# Set the plot boundaries\n",
        "plt.axis([1e-7, 1e-4, 0, 20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD8-TGbAJUNj"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "You can then declare the model again and train with the learning rate you picked. It is set to `1e-6`by default but feel free to change it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y1KMowRkHkC"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                      input_shape=[window_size]),\n",
        "  tf.keras.layers.SimpleRNN(40, return_sequences=True),\n",
        "  tf.keras.layers.SimpleRNN(40),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "# Set the learning rate\n",
        "learning_rate = 1e-6\n",
        "\n",
        "# Set the optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "\n",
        "# Set the training parameters\n",
        "model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(dataset,epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewNczbMaJz5Q"
      },
      "source": [
        "## Model Prediction\n",
        "\n",
        "Now it's time to generate the model predictions for the validation set time range. The model is a lot bigger and sequential in nature (i.e. inputs go through a series of time steps as opposed to parallel processing) can make predictions a bit slow."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
        "    plt.plot(time[start:end], series[start:end], format)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(False)"
      ],
      "metadata": {
        "id": "No8ucutEVprX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejBynEKekaKw"
      },
      "outputs": [],
      "source": [
        "# Initialize a list\n",
        "forecast = []\n",
        "\n",
        "# Reduce the original series\n",
        "forecast_series = series[split_time - window_size:]\n",
        "\n",
        "# Use the model to predict data points per window size\n",
        "for time in range(len(forecast_series) - window_size):\n",
        "  forecast.append(model.predict(forecast_series[time:time + window_size]))\n",
        "\n",
        "# Convert to a numpy array and drop single dimensional axes\n",
        "results = np.array(forecast).squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the results\n",
        "plot_series(time_valid, x_valid)\n",
        "plot_series(time_valid, results)"
      ],
      "metadata": {
        "id": "lZ9sP68eWXo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn1QZd7LgCcu"
      },
      "source": [
        "We can optimize this step by leveraging Tensorflow models' capability to process batches. Instead of running the for-loop above which processes a single window at a time, we can pass in an entire batch of windows and let the model process that in parallel.\n",
        "\n",
        "The function below does just that. Note that it almost mirrors the `windowed_dataset()` function but it does not shuffle the windows. That's because we want the output to be in its proper sequence so we can compare it properly to the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym73y-pDKp-X"
      },
      "outputs": [],
      "source": [
        "def model_forecast(model, series, window_size, batch_size):\n",
        "    \"\"\"Uses an input model to generate predictions on data windows\n",
        "\n",
        "    Args:\n",
        "      model (TF Keras Model) - model that accepts data windows\n",
        "      series (array of float) - contains the values of the time series\n",
        "      window_size (int) - the number of time steps to include in the window\n",
        "      batch_size (int) - the batch size\n",
        "\n",
        "    Returns:\n",
        "      forecast (numpy array) - array containing predictions\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a TF Dataset from the series values\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "\n",
        "    # Window the data but only take those with the specified size\n",
        "    dataset = dataset.window(window_size, shift=1, drop_remainder=True)\n",
        "\n",
        "    # Flatten the windows by putting its elements in a single batch\n",
        "    dataset = dataset.flat_map(lambda w: w.batch(window_size))\n",
        "\n",
        "    # Create batches of windows\n",
        "    dataset = dataset.batch(batch_size).prefetch(1)\n",
        "\n",
        "    # Get predictions on the entire dataset\n",
        "    forecast = model.predict(dataset)\n",
        "\n",
        "    return forecast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrRFuUcfieQN"
      },
      "source": [
        "With this  function implemented below, notice that the predictions are generated almost instantly.\n",
        "\n",
        "*Note: You might notice that the first line slices the `series` at `split_time - window_size:-1` which is a bit different from the slower for-loop code. That is because we want the model to have its last prediction to align with the last point of the validation set (i.e. `t=1460`). You were able to do that with the slower for-loop code by specifying the for-loop's `range()`. With the more efficient function above, you don't have that mechanism so you instead just remove the last point when slicing the `series`. If you don't, then the function will generate a prediction at `t=1461` which is outside the validation set range.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m23ny5uh8KTt"
      },
      "outputs": [],
      "source": [
        "# Reduce the original series\n",
        "forecast_series = series[split_time - window_size:-1]\n",
        "\n",
        "# Use helper function to generate predictions\n",
        "forecast = model_forecast(model, forecast_series, window_size, batch_size)\n",
        "\n",
        "# Drop single dimensional axis\n",
        "results = forecast.squeeze()\n",
        "\n",
        "# Plot the results\n",
        "plot_series(time_valid, x_valid)\n",
        "plot_series(time_valid, results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cVUoLH7k5NG"
      },
      "source": [
        "Compute the MSE and MAE and  compare the results LSTM architectures."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the MSE and MAE\n",
        "mse_metric = tf.keras.metrics.MeanSquaredError()\n",
        "mse_metric.update_state(x_valid, results)\n",
        "mse = mse_metric.result().numpy()\n",
        "print(mse)"
      ],
      "metadata": {
        "id": "PrLcQ16WCOhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae_metric = tf.keras.metrics.MeanAbsoluteError()\n",
        "mae_metric.update_state(x_valid, results)\n",
        "mae = mae_metric.result().numpy()\n",
        "print(mae)"
      ],
      "metadata": {
        "id": "CDLQJvJ8C2sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_Iah9iHF3gW"
      },
      "source": [
        "## Implementing LSTM on the same dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPqd5gC1JGXB"
      },
      "source": [
        "### Preparing Features and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAqednc2F8DV"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "window_size = 20\n",
        "batch_size = 32\n",
        "shuffle_buffer_size = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDoyuZd0GKhf"
      },
      "outputs": [],
      "source": [
        "# Generate the dataset windows\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVXpUVITJOIm"
      },
      "source": [
        "### Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Yxsr0atGLWG"
      },
      "outputs": [],
      "source": [
        "# Build the Model\n",
        "model_tune = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),input_shape=[window_size]),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "model_tune.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jUGkTy4JVEV"
      },
      "source": [
        "### Tune the Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgJ7qihQGZOP"
      },
      "outputs": [],
      "source": [
        "# Set the learning rate scheduler\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(momentum=0.9)\n",
        "\n",
        "# Set the training parameters\n",
        "model_tune.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer)\n",
        "\n",
        "# Train the model\n",
        "history = model_tune.fit(dataset, epochs=100, callbacks=[lr_schedule])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmbUc3S-Gjqx"
      },
      "outputs": [],
      "source": [
        "# Define the learning rate array\n",
        "lrs = 1e-8 * (10 ** (np.arange(100) / 20))\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Set the grid\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot the loss in log scale\n",
        "plt.semilogx(lrs, history.history[\"loss\"])\n",
        "\n",
        "# Increase the tickmarks size\n",
        "plt.tick_params('both', length=10, width=1, which='both')\n",
        "\n",
        "# Set the plot boundaries\n",
        "plt.axis([1e-8, 1e-3, 0, 30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrUoRgHhJnWw"
      },
      "source": [
        "### Train the Model\n",
        "\n",
        "Proceed to train the model with the chosen learning rate.\n",
        "\n",
        "Tip: When experimenting and  running different iterations of a model, we may want to use the clear_session() method to declutter memory used by Keras. This is added in the first line below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU4P8Tr2Gvlg"
      },
      "outputs": [],
      "source": [
        "# Reset states generated by Keras\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),input_shape=[None]),\n",
        "   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "# Set the learning rate\n",
        "learning_rate = 1e-5\n",
        "\n",
        "# Set the optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "\n",
        "# Set the training parameters\n",
        "model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(dataset,epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypMw4ZTBJ6Km"
      },
      "source": [
        "### Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLmioGAkG6gX"
      },
      "outputs": [],
      "source": [
        "# Reduce the original series\n",
        "forecast_series = series[split_time-window_size:-1]\n",
        "\n",
        "# Use helper function to generate predictions\n",
        "forecast = model_forecast(model, forecast_series, window_size, batch_size)\n",
        "\n",
        "# Drop single dimensional axis\n",
        "results = forecast.squeeze()\n",
        "\n",
        "# Plot the results\n",
        "plot_series(time_valid, x_valid)\n",
        "plot_series(time_valid, results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the MSE and MAE\n",
        "mse_metric = tf.keras.metrics.MeanSquaredError()\n",
        "mse_metric.update_state(x_valid, results)\n",
        "mse = mse_metric.result().numpy()\n",
        "print(mse)"
      ],
      "metadata": {
        "id": "LojFZCKlDnEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae_metric = tf.keras.metrics.MeanAbsoluteError()\n",
        "mae_metric.update_state(x_valid, results)\n",
        "mae = mae_metric.result().numpy()\n",
        "print(mae)"
      ],
      "metadata": {
        "id": "9PMH3A4SDvE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4uVcne4zeny"
      },
      "source": [
        "\n",
        "### [**Looking inside the math of LSTM & GRU**](https://colab.research.google.com/drive/1xpcGr1jYY9pCByHkxa3j826SaX39frib#scrollTo=B2ysJyAnTnGE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEIGT6XpS5_f"
      },
      "source": [
        "### [**Example_Using_LSTM : Consumer Complaint Classification**](https://colab.research.google.com/drive/1zzvpqGdwMIVXcdkaB0ngJ_P1PcL-6lMe?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r35isHfTVGKc"
      },
      "source": [
        "#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Walkthrough = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for Ungrading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}