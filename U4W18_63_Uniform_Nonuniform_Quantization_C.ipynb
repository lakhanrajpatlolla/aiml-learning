{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakhanrajpatlolla/aiml-learning/blob/master/U4W18_63_Uniform_Nonuniform_Quantization_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SI_AEBcqs_S"
      },
      "source": [
        "\n",
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR1wvEVM6fTG"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vESOoKZe7DBx"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "\n",
        "* Classify the MNIST dataset using MLP and then quantize the weights of the network\n",
        "* Understand how quantization reduces the storage needs of the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfk8109wnqAl",
        "cellView": "form"
      },
      "source": [
        "#@title Experiment Explanation Video\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"<video width=\"850\" height=\"480\" controls>\n",
        "  <source src=\"https://cdn.talentsprint.com/aiml/AIML_BATCH_HYD_7/March31/uniform_nonuniform_quantization.mp4\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3b5-r1S7Fkh"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIcN-gMa7KOW"
      },
      "source": [
        "\n",
        "###Description\n",
        "\n",
        "\n",
        "1. The dataset contains 60,000 Handwritten digits as training samples and 10,000 Test samples, which means each digit occurs 6000 times in the training set and 1000 times in the testing set (approximately).\n",
        "2. Each image is Size Normalized and Centered.\n",
        "3. Each image is 28 X 28 Pixel with 0-255 Gray Scale Value.\n",
        "4. That means each image is represented as 784 (28 X28) dimension vector where each value is in the range 0- 255.\n",
        "\n",
        "###History\n",
        "\n",
        "Yann LeCun (Director of AI Research, Facebook, Courant Institute, NYU) was given the task of identifying the cheque numbers (in the 90â€™s) and the amount associated with that cheque without manual intervention. That is when this dataset was created which raised the bars and became a benchmark.\n",
        "\n",
        "Yann LeCun and Corinna Cortes (Google Labs, New York) hold the copyright of MNIST dataset, which is a subset of the original NIST datasets. This dataset is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license.\n",
        "\n",
        "It is the handwritten digits dataset in which half of them are written by the Census Bureau employees and remaining by the high school students. The digits collected among the Census Bureau employees are easier and cleaner to recognize than the digits collected among the students.\n",
        "\n",
        "\n",
        "###Challenges\n",
        "\n",
        "Now, if you notice the images below, you will find that between 2 characters there are always certain similarities and differences. To teach a machine to recognize these patterns and identify the correct output is intriguing.\n",
        "\n",
        "![altxt](https://www.researchgate.net/profile/Radu_Tudor_Ionescu/publication/282924675/figure/fig3/AS:319968869666820@1453297931093/A-random-sample-of-6-handwritten-digits-from-the-MNIST-data-set-before-and-after.png)\n",
        "\n",
        "Hence, all these challenges make this a good problem to solve in Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrRu_oSI7MLm"
      },
      "source": [
        "## Domain Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZJqmAkO7Qm2"
      },
      "source": [
        "Handwriting changes person to person. Some of us have neat handwriting and some have illegible handwriting such as doctors. However, if you think about it even a child who recognizes alphabets and numerics can identify the characters of a text even written by a stranger. But even a technically knowledgeable adult cannot describe the process by which he or she recognizes the text/letters. As you know this is an excellent challenge for Machine Learning.\n",
        "\n",
        "![altxt](https://i.pinimg.com/originals/f2/7a/ac/f27aac4542c0090872110836d65f4c99.jpg)\n",
        "\n",
        "The experiment handles a subset of text recognition, namely recognizing the 10 numerals (0 to 9) from scanned images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_onQyDA7Uoc"
      },
      "source": [
        "## AI / ML Technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb2Jfqj67XEr"
      },
      "source": [
        "### Quantization for Image Classification:\n",
        "\n",
        "In this experiment, you train a neural network on the dataset to classify the images and then reduce the storage requirements of the network by quantizing the weights of the network using compression.\n",
        "\n",
        "Neural network models can take up a lot of space on disk where almost all of that space is taken up by the weights of the neural connections, which are often millions in number in a single model. As the weights are all slightly different floating point numbers, simple compression formats like zip don't compress them well. Quantization is a network compression technique that is used to save the storage for the many parameters of the network by compressing the weights. The weights intially are represented as 8-bit values, so we are using 2 * 8 = 16 in storage. If we are compressing the weights to 1-bit values, we are storing only 1 * 8 = 8 in storage thus reducing our storage needs by half.  Depending on how the weight space is distributed into clusters, there are two types of quantization techniques:\n",
        "\n",
        "1. **Uniform Quantization**: The cluster heads are uniformly spaced.\n",
        "2. **Non-uniform Quantization**:  The cluster heads are non - uniformly spaced using K - Means clustering.\n",
        "\n",
        "You will understand these in detail while working on the code in the experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEzlYL4CDrmE"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import re\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"U4W18_63_Uniform_Nonuniform_Quantization_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    from IPython.display import HTML, display\n",
        "    ipython.magic(\"sx pip3 install torch\")\n",
        "    ipython.magic(\"sx pip3 install torchvision\")\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getWalkthrough() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook, \"feedback_walkthrough\":Walkthrough ,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_inclass_mentor\": Mentor_support}\n",
        "\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://learn-iiith.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getWalkthrough():\n",
        "  try:\n",
        "    if not Walkthrough:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Walkthrough\n",
        "  except NameError:\n",
        "    print (\"Please answer Walkthrough Question\")\n",
        "    return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELePreM3QBkb"
      },
      "source": [
        "### Importing required packages\n",
        "\n",
        "* First, we import pytorch, the deep learning library which weâ€™ll be using, and torchvision, which provides our dataset and data transformations.\n",
        "\n",
        "\n",
        "* We also import torch.nn (pytorchâ€™s neural network library)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk4-H3VnNSBi"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Importing python packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dE4fUykNSBm"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKpyM4zzNSBm"
      },
      "source": [
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "use_reg = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbm-8HxIqvxR"
      },
      "source": [
        "### Initializing CUDA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9E-00Slqs_k"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJH3yYJKNSBp"
      },
      "source": [
        "### Downloading the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3inDH-alNSBq"
      },
      "source": [
        "train_dataset = datasets.MNIST(root='../data/',\n",
        "                            train=True,\n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='../data/',\n",
        "                           train=False,\n",
        "                           transform=transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cIqGefQNSBw"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3Nxm0bXNSBw"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrX-dz4fNSB1"
      },
      "source": [
        "### Define the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYXjWlJsNSB2"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.fc1 = nn.Linear(7*7*32, 300)\n",
        "        self.fc2 = nn.Linear(300, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        out = F.log_softmax(out, dim=1)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne2DrrXiNSB5"
      },
      "source": [
        "<b>The below function is called to reinitialize the weights of the network and define the required loss criterion and the optimizer.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ui3ChcWNSB6"
      },
      "source": [
        "def reset_model():\n",
        "    net = Net()\n",
        "    net = net.to(device)\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    return net, criterion, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfBSn8rJNSB8"
      },
      "source": [
        "### Initializing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vLzLbbnNSB9"
      },
      "source": [
        "net, criterion, optimizer = reset_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyXY7Oq6NSCB"
      },
      "source": [
        "### Defining a L1 Regularizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMLxnmDkNSCC"
      },
      "source": [
        "def l1_regularizer(net, loss, beta):\n",
        "    l1_crit = nn.L1Loss(size_average=False)\n",
        "    reg_loss = 0\n",
        "    for param in net.parameters():\n",
        "        target = (torch.FloatTensor(param.size()).zero_()).to(device)\n",
        "        reg_loss += l1_crit(param, target)\n",
        "\n",
        "    loss += beta * reg_loss\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NogS9pZNSCF"
      },
      "source": [
        "### Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hUX_GntNSCG"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "def training(net, reset = True):\n",
        "    if reset == True:\n",
        "        net, criterion, optimizer = reset_model()\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "    net.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        accuracy = []\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            temp_labels = labels\n",
        "\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            if use_reg == True :\n",
        "                loss = l1_regularizer(net, loss, beta=0.001)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct = (predicted == temp_labels).sum().item()\n",
        "            accuracy.append(correct/float(batch_size))\n",
        "\n",
        "        print('Epoch: %d, Loss: %.4f, Accuracy: %.4f' %(epoch+1,total_loss, (sum(accuracy)/float(len(accuracy)))))\n",
        "\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5fha93LNSCI"
      },
      "source": [
        "### Testing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj1AAWuBNSCL"
      },
      "source": [
        "# Test the Model\n",
        "def testing(net):\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the network on the 10000 test images: %.2f %%' % (100.0 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4WRXM1YNSCP"
      },
      "source": [
        "### Training and testing the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSfX2-vFNSCP"
      },
      "source": [
        "reset = False\n",
        "net = training(net, reset)\n",
        "testing(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8kc_CbbNSCR"
      },
      "source": [
        "### Uniform Quantization\n",
        "\n",
        "The simplest motivation for quantization is to shrink file sizes by storing the min and max for each layer, and then compressing each float value to an eight-bit integer representing the closest real number in a linear set of 256 within the range.\n",
        "\n",
        "In the function below we send 8 bits as input which ressembles that the weights of the network should be represented with only 8 bits while storing to disk. In other words we use only 2^8 or 256 clusters. Hence each weight is represented as a 8-bit integer between 0-255.\n",
        "\n",
        "Thus before using the weights during test time they need to be projected into the original weight space by using the following equation:\n",
        "\n",
        "$$\n",
        "W_{i} = min + \\dfrac{max-min}{255}*W_{index}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkD7PGOCNSCT"
      },
      "source": [
        "def uniform_quantize(weight, bits):\n",
        "    print('-------------------------LAYER---------------------------')\n",
        "    print(\"Number of unique parameters before quantization: \" + str(len(np.unique(weight))))\n",
        "    n_clusters = 2**bits\n",
        "\n",
        "    maxim = np.amax(weight)\n",
        "    minim = np.amin(weight)\n",
        "    step = (maxim-minim)/(n_clusters - 1)\n",
        "\n",
        "    clusters=[]\n",
        "\n",
        "    for i in range(0,n_clusters):\n",
        "        clusters.append(minim)\n",
        "        minim += step\n",
        "\n",
        "    for i in range(0,len(weight) ):\n",
        "        dist = (clusters-weight[i])**2\n",
        "        weight[i] = clusters[np.argmin(dist)]\n",
        "\n",
        "    print(\"Number of unique parameters after quantization: \" + str(len(np.unique(weight))))\n",
        "\n",
        "    return weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWEIuI9wNSCU"
      },
      "source": [
        "### Uniform Quantization on the weights and biases\n",
        "\n",
        "Different number of bits can be used for representing the weights and biases. The exact number of bits to use is a design choice and may depend on the complexity of the task at hand since using too less number of bits can result in poor performance. Here, we use 8 bits for quantizing the weights and 1 bit for the biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpJ1C75qNSCU"
      },
      "source": [
        "for m in net.modules():\n",
        "    if isinstance(m,nn.Conv2d) or isinstance(m,nn.BatchNorm2d) or isinstance(m,nn.Linear):\n",
        "        temp_weight = m.weight.data.cpu().numpy()\n",
        "        dims = temp_weight.shape\n",
        "        temp_weight = temp_weight.flatten()\n",
        "        temp_weight = uniform_quantize(temp_weight, 8)\n",
        "        temp_weight=np.reshape(temp_weight,dims)\n",
        "        m.weight.data = (torch.FloatTensor(temp_weight).cuda())\n",
        "\n",
        "        temp_bias = m.bias.data.cpu().numpy()\n",
        "        dims = temp_bias.shape\n",
        "        temp_bias = temp_bias.flatten()\n",
        "        temp_bias = uniform_quantize(temp_bias, 1)\n",
        "        temp_bias = np.reshape(temp_bias,dims)\n",
        "        m.bias.data = (torch.FloatTensor(temp_bias).cuda())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQsNHoNyNSCW"
      },
      "source": [
        "Now that we have replaced the weight matrix with the approximated weight of the nearest cluster, we can test the network with the modified weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qYOc_42NSCX"
      },
      "source": [
        "testing(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sCB4SNmNSCa"
      },
      "source": [
        "### Non-uniform quantization\n",
        "\n",
        "We have seen in the previous method that we divide the weight space into equally partitioned cluster heads. However, instead of forcing the cluster heads to be equally spaced it would make more sense to learn them. A common and obvious practice is to learn the weight space as a distribution of cluster centers using k-means clustering. Here, we define a function to perform k-means to the weight values.\n",
        "\n",
        "$$\n",
        "min\\sum_{i}^{mn}\\sum_{j}^{k}||w_{i}-c_{j}||_{2}^{2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUDAcctdNSCb"
      },
      "source": [
        "num_clusters = 8\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0,  max_iter=500, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2wWh5lbNSCe"
      },
      "source": [
        "def non_uniform_quantize(weights):\n",
        "    print(\"---------------------------Layer--------------------------------\")\n",
        "    print(\"Number of unique parameters before quantization: \" + str(len(np.unique(weights))))\n",
        "    weights = np.reshape(weights,[weights.shape[0],1])\n",
        "    print(weights.shape)\n",
        "    kmeans_fit = kmeans.fit(weights)\n",
        "    clusters = kmeans_fit.cluster_centers_\n",
        "\n",
        "    for i in range(0,len(weights)):\n",
        "        dist = (clusters-weights[i])**2\n",
        "        weights[i] = clusters[np.argmin(dist)]\n",
        "\n",
        "    print(\"Number of unique parameters after quantization: \" + str(len(np.unique(weights))))\n",
        "\n",
        "    return weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luzXZINVNSCg"
      },
      "source": [
        "We reset the model and train the network since we had earlier done uniform quantization on the weight already."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYdxVUsvNSCj"
      },
      "source": [
        "reset = True\n",
        "net = training(net, reset)\n",
        "testing(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXhYLriUNSCm"
      },
      "source": [
        "Non-Uniform quantization on the weights and biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygTmNfWsNSCm"
      },
      "source": [
        "for m in net.modules():\n",
        "    if isinstance(m,nn.Conv2d) or isinstance(m,nn.BatchNorm2d) or isinstance(m,nn.Linear):\n",
        "        temp_weight = m.weight.data.cpu().numpy()\n",
        "        dims = temp_weight.shape\n",
        "        temp_weight = temp_weight.flatten()\n",
        "        temp_weight = non_uniform_quantize(temp_weight)\n",
        "        temp_weight=np.reshape(temp_weight,dims)\n",
        "        m.weight.data = (torch.FloatTensor(temp_weight).cuda())\n",
        "\n",
        "        temp_bias = m.bias.data.cpu().numpy()\n",
        "        dims = temp_bias.shape\n",
        "        temp_bias = temp_bias.flatten()\n",
        "        temp_bias = non_uniform_quantize(temp_bias)\n",
        "        temp_bias = np.reshape(temp_bias,dims)\n",
        "        m.bias.data = (torch.FloatTensor(temp_bias).cuda())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAZkRYg9NSCp"
      },
      "source": [
        "testing(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZME62IFNSCr"
      },
      "source": [
        "### Retraining the network\n",
        "\n",
        "Here we see that 8 clusters are too less in order to maintain the network at the same accuracy. One of the solutions is to retrain the network. This helps the other weights to compensate for those weights which on being rounded off to the nearest cluster center have resulted in a drop in performance. Accuracy can be recovered significantly on retraining the network and then non-uniformly quantizing the weights again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDGjonRNNSCt"
      },
      "source": [
        "reset = False\n",
        "net = training(net, reset)\n",
        "# Perform non-uniform quantization\n",
        "testing(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNL2AuOeNSCx"
      },
      "source": [
        "### References\n",
        "\n",
        "1. https://arxiv.org/pdf/1412.6115.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-b-CEcpkOCi"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12SwfIaukOCj"
      },
      "source": [
        "#@title State True or False: The k-means quantization used above, clusters all the input data points (features) before training ? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\",\"TRUE\", \"FALSE\"]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0DzTLAMkOCj"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlMms_IykOCj"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHnMPzf_kOCk"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRKla4emkOCk"
      },
      "source": [
        "#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Walkthrough = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtkER1AwkOCk"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-UhjrabkOCk"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MUHD7LC0kOCk"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}