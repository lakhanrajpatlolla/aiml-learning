{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakhanrajpatlolla/aiml-learning/blob/master/U4W21_73_Part_B_PEFT_for_Dialogue_Summary_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaHNxUntdN6a"
      },
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint\n",
        "### Assignment 2 Part-B: PEFT for Dialogue Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **NOTE that** this Assignment is in 2 parts:\n",
        "> - Part-A: Finetuning a Seq2Seq (T5) Model for Summarization\n",
        "> - Part-B: PEFT for Dialogue Summary\n",
        ">\n",
        ">Only Part-B needs to be submitted for grading."
      ],
      "metadata": {
        "id": "Q2Tp7-NpJB2n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand the working of a parameter efficinet finetuning method - LoRA\n",
        "* fine tune a T5 model, `facebook/bart-large-cnn`, on the SAMSum dataset for summerization using LoRA\n",
        "* push the finetuned LoRA adapter to HuggingFace model hub\n",
        "* load the finetuned adapter from hub for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Description\n",
        "\n",
        "The **[SAMSum](https://huggingface.co/datasets/samsum) dataset** contains about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English. Linguists were asked to create conversations similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger convesations. The style and register are diversified - conversations could be informal, semi-formal or formal, they may contain slang words, emoticons and typos. Then, **the conversations were annotated with summaries**. It was assumed that summaries should be a concise brief of what people talked about in the conversation in third person. The SAMSum dataset was prepared by Samsung R&D Institute Poland and is distributed for research purposes.\n",
        "\n",
        "Data Splits:\n",
        "- train: 14732\n",
        "- val: 818\n",
        "- test: 819\n",
        "\n",
        "Data Fields:\n",
        "\n",
        "- ***dialogue***: text of dialogue\n",
        "- ***summary***: human written summary of the dialogue\n",
        "- ***id***: unique id of an example\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example:**\n",
        "\n",
        "\\{\n",
        "> '**id**': '13818513',\n",
        "\n",
        ">'**summary**': 'Amanda baked cookies and will bring Jerry some tomorrow.',\n",
        "\n",
        ">'**dialogue**': \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n",
        "\n",
        "\\}"
      ],
      "metadata": {
        "id": "RFJT-5ZuIFwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information"
      ],
      "metadata": {
        "id": "_cgllrAuIPKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Parameter-Efficient Fine-Tuning (PEFT) methods**\n",
        "\n",
        "Fine-tuning large pretrained models is often prohibitively costly due to their scale. Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of large pretrained models to various downstream applications by only fine-tuning a small number of (extra) model parameters instead of all the model's parameters. This significantly decreases the computational and storage costs. Recent state-of-the-art PEFT techniques achieve performance comparable to fully fine-tuned models.\n",
        "\n",
        "PEFT is integrated with Transformers for easy model training and inference, and Accelerate for distributed training and inference for really big models."
      ],
      "metadata": {
        "id": "TYeIx2tiKzXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[PEFT](https://github.com/huggingface/peft) is also a new open-source library from Hugging Face to enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications ***without*** fine-tuning all the model's parameters.\n",
        "\n",
        "PEFT currently includes techniques for:\n",
        "\n",
        "- **LoRA:** Low-Rank Adaptation of Large Language Models\n",
        "- **Prefix Tuning:** P-Tuning v2\n",
        "- **P-Tuning**\n",
        "- **Prompt Tuning**\n"
      ],
      "metadata": {
        "id": "yxTLFMYK69n3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LoRA**\n",
        "\n",
        "It is a technique that accelerates the fine-tuning of large models while consuming less memory.\n",
        "\n",
        "To make fine-tuning more efficient, LoRA's approach is to represent the weight updates with two smaller matrices (called update matrices) through low-rank decomposition.\n",
        "\n",
        "A and B are update matrices in below figure.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png\" width=900px>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "- These new matrices can be trained to **adapt to the new data** while keeping the overall number of changes low.\n",
        "- The original weight matrix **remains frozen** and doesn't receive any further adjustments.\n",
        "- To produce the final results, both the original and the adapted weights are **combined**."
      ],
      "metadata": {
        "id": "XSiWexRtSec4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2418775\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEzlYL4CDrmE"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"9959000490\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form",
        "outputId": "831e00e8-ff19-4443-a207-3a6ebd54d17f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import re\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"U4W21_73_Part-B_PEFT_for_Dialogue_Summary_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    from IPython.display import HTML, display\n",
        "    ipython.magic(\"sx pip3 install torch\")\n",
        "    ipython.magic(\"sx pip3 install torchvision\")\n",
        "    ipython.magic(\"sx mkdir results\")\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getWalkthrough() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook, \"feedback_walkthrough\":Walkthrough ,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_inclass_mentor\": Mentor_support}\n",
        "\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://learn-iiith.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getWalkthrough():\n",
        "  try:\n",
        "    if not Walkthrough:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Walkthrough\n",
        "  except NameError:\n",
        "    print (\"Please answer Walkthrough Question\")\n",
        "    return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2418775&recordId=2542\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install required dependencies"
      ],
      "metadata": {
        "id": "AUSwPHjuId3t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gonLcSrAlDd9"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers datasets evaluate transformers[torch]\n",
        "\n",
        "# A dependecy required for loading SAMSum dataset\n",
        "!pip -q install py7zr\n",
        "\n",
        "!pip -q install peft"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import required packages"
      ],
      "metadata": {
        "id": "1_dZbe3263lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "pzoJRJTLIptL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load Model & Tokenizer**"
      ],
      "metadata": {
        "id": "RFsEFc9SSgqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model from HF Model Hub\n",
        "\n",
        "\"\"\"\n",
        "BART HAS 400M PARAMS: https://github.com/facebookresearch/fairseq/tree/main/examples/bart\n",
        "Look into Model card - 400 Million parameters\n",
        "\"\"\"\n",
        "\n",
        "checkpoint = \"facebook/bart-large-cnn\"                # username/repo-name\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "t_W1cU6wVt0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load Dataset**"
      ],
      "metadata": {
        "id": "QEVKX9v6I81m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SAMSum dataset\n",
        "dataset = load_dataset(\"samsum\", trust_remote_code=True)\n",
        "dataset"
      ],
      "metadata": {
        "id": "bn4W7mkMlRNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prepare the Dataset**"
      ],
      "metadata": {
        "id": "tqgPdnieSjbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to prepare dataset\n",
        "\n",
        "def tokenize_inputs(example):\n",
        "\n",
        "    start_prompt = \"Summarize the following conversation.\\n\\n\"\n",
        "    end_prompt = \"\\n\\nSummary: \"\n",
        "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example['dialogue']]\n",
        "    example['input_ids'] = tokenizer(prompt, padding='max_length', truncation=True, max_length=512, return_tensors='pt').input_ids             # 'pt' for pytorch tensor\n",
        "    example['labels'] = tokenizer(example['summary'], padding='max_length', truncation=True, max_length=512, return_tensors='pt').input_ids\n",
        "\n",
        "    return example"
      ],
      "metadata": {
        "id": "i0TYG5rslTU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenized_datasets = dataset.map(tokenize_inputs, batched=True)       # using batched=True for Fast tokenizer implementation\n",
        "\n",
        "# Remove columns/keys that are not needed further\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'dialogue', 'summary'])"
      ],
      "metadata": {
        "id": "mzD32yb_cUG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shortening the data: Just picking row index divisible by 100\n",
        "# For learning purpose! It will reduce the compute resource requirement and training time\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
      ],
      "metadata": {
        "id": "ZsYGQ9cCWQQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_datasets['train'].shape)\n",
        "print(tokenized_datasets['validation'].shape)\n",
        "print(tokenized_datasets['test'].shape)"
      ],
      "metadata": {
        "id": "CwnHmfn2VAs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create PEFT Model using LoRA**\n",
        "\n",
        "To fine-tune a model using LoRA, you need to:\n",
        "\n",
        "- Instantiate a base model, here it is `facebook/bart-large-cnn`\n",
        "- Create a configuration (`LoraConfig`) where you define LoRA-specific parameters\n",
        "- Wrap the base model with `get_peft_model()` to get a trainable `PeftModel`\n",
        "- Train the `PeftModel` as you normally would train the base model"
      ],
      "metadata": {
        "id": "fGkmutqoSlzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# LoRA-specific parameters\n",
        "lora_config = LoraConfig(\n",
        "    r=32,                       # 8, 16, 32    # the rank of the update matrices\n",
        "    lora_alpha=32,                             # LoRA scaling factor\n",
        "    lora_dropout=0.05,\n",
        "    bias='none',                               # specifies if the bias parameters should be trained\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,           # telling lora that this is a sq2seq modeling task\n",
        ")"
      ],
      "metadata": {
        "id": "HmSd_CdglXeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainable PEFTModel\n",
        "peft_model = get_peft_model(model, peft_config=lora_config)"
      ],
      "metadata": {
        "id": "WF2lf6HOlYmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train PEFT Model**"
      ],
      "metadata": {
        "id": "s4pj1N2uSp4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=\"./mode_tuned_peft\",           # local directory\n",
        "    hub_model_id=\"sumanthk/PEFT_Partb\",      # identifier on the Hub for directly pushing to HFhub model\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=5,      ## for 5 epochs took around 10 minutes\n",
        "    weight_decay=0.01,\n",
        "    auto_find_batch_size=True,\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,                    # model to be fine-tuned\n",
        "    args=peft_training_args,                       # training arguments\n",
        "    train_dataset=tokenized_datasets['train'],          # train data to use\n",
        "    eval_dataset=tokenized_datasets['validation']       # validation data to use\n",
        ")"
      ],
      "metadata": {
        "id": "1w889uXJlZub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of trainable parameters\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "QHx8PKx4lawj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above we can see, here we are only training 1.14% of the parameters of the model."
      ],
      "metadata": {
        "id": "DTwTPl0DZ0nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Disabling Weights and Biases logging\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "d6fNhXF9AH4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "peft_trainer.train()"
      ],
      "metadata": {
        "id": "jYOaLTUKb9ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Save PEFT Adapter**\n",
        "\n",
        "**Push your Peft adapter to Hugging Face Model Hub**"
      ],
      "metadata": {
        "id": "ZXnckg4jSbY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to HuggingFace\n",
        "# Run, and paste your HF Access token when prompted\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "OXYzNIlucQCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access your pushed adapter at `https://huggingface.co/[YOUR-USER-NAME]/[YOUR-MODEL-REPO-NAME]`"
      ],
      "metadata": {
        "id": "NE3LxuRilmPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Push peft adapter to Hub\n",
        "my_peft_repo = \"PEFT_Partb\"\n",
        "\n",
        "peft_model.push_to_hub(repo_id= my_peft_repo, commit_message= \"Upload peft adapter\", )"
      ],
      "metadata": {
        "id": "VsFSoe-UdsbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reload & Test**\n",
        "\n",
        "**Test your LoRA finetuned model downloaded from HF Model Hub**"
      ],
      "metadata": {
        "id": "8pH1PsfNSvgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "username = \"sumanthk\"      # change it to your HuggingFace username\n",
        "\n",
        "checkpoint = username + '/PEFT_Partb'  # change it to your Repo name\n",
        "\n",
        "loaded_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "u3ntqyP3uOCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(input, llm):\n",
        "    \"\"\"Prepare prompt  -->  tokenize -->  generate output using LLM  -->  detokenize output\"\"\"\n",
        "\n",
        "    input_prompt = f\"\"\"\n",
        "                    Summarize the following conversation.\n",
        "\n",
        "                    {input}\n",
        "\n",
        "                    Summary:\n",
        "                    \"\"\"\n",
        "\n",
        "    input_ids = tokenizer(input_prompt, return_tensors='pt')\n",
        "    tokenized_output = llm.generate(input_ids=input_ids['input_ids'], min_length=30, max_length=200, )\n",
        "    output = tokenizer.decode(tokenized_output[0], skip_special_tokens=True)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "ohWPBucMcvvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset['test'][0]['dialogue']\n",
        "label = dataset['test'][0]['summary']\n",
        "\n",
        "output = generate_summary(sample, llm=loaded_model)\n",
        "\n",
        "print(\"Sample\")\n",
        "print(sample)\n",
        "print(\"-------------------\")\n",
        "print(\"Summary:\")\n",
        "print(output)\n",
        "print(\"Ground Truth Summary:\")\n",
        "print(label)"
      ],
      "metadata": {
        "id": "Bdzylm9qhtXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References:\n",
        "\n",
        "1. [LoRA](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)\n",
        "2. [Quicktour](https://huggingface.co/docs/peft/en/quicktour)\n",
        "3. [Efficient Large Language Model training with LoRA and Hugging Face](https://www.philschmid.de/fine-tune-flan-t5-peft)"
      ],
      "metadata": {
        "id": "L_lPNi-S-N7d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-b-CEcpkOCi"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12SwfIaukOCj"
      },
      "source": [
        "#@title Select the False statement w.r.t LoRA: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"During inference, only the original weight matrix is used to produce final results\" #@param [\"\", \"It is a technique that accelerates the fine-tuning of large models while consuming less memory\", \"During training, the original weight matrix remains frozen and doesn't receive any further adjustments\", \"During inference, only the original weight matrix is used to produce final results\", \"None of the above\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0DzTLAMkOCj"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlMms_IykOCj"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"good\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHnMPzf_kOCk"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRKla4emkOCk"
      },
      "source": [
        "#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Walkthrough = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtkER1AwkOCk"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-UhjrabkOCk"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MUHD7LC0kOCk",
        "outputId": "fcde91b6-04d6-4172-c63c-1e2c9a68b5f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 2542\n",
            "Date of submission:  19 Apr 2025\n",
            "Time of submission:  15:27:02\n",
            "View your submissions: https://learn-iiith.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ]
}