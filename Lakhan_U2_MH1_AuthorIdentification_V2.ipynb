{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ioUZ8pJa9--n",
        "gXGRHmvSJWRd"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakhanrajpatlolla/aiml-learning/blob/master/Lakhan_U2_MH1_AuthorIdentification_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74lzxaiQ9-9s"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4mJKZzgrMLp"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiWKLfqkrOB6"
      },
      "source": [
        "The problem is to identify the author of a  book from a given list of possible authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ElufS5-9-99"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCHP0foL9--C"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* Use NLTK package\n",
        "* Extract handcrafted features\n",
        "* Preprocess the text\n",
        "* Write an algorithm to identify the author of a given book\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYG9AXM_oe--",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "a5ad7ee6-b604-49d7-da36-3c579a56b99d"
      },
      "source": [
        "#@title  Mini Hackathon Walkthrough\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"<video width=\"854\" height=\"480\" controls>\n",
        "  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Walkthrough/authoridentification.mp4\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=\"854\" height=\"480\" controls>\n",
              "  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Walkthrough/authoridentification.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCz5oobSruGF"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZWNTcd4oo-D"
      },
      "source": [
        "Author identification is the task of identifying the author of a given text. It can be considered as a typical classification problem, where a set of books with known authors are used for training. The aim is to automatically determine the corresponding author of an anonymous text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BQEA97zTlTa"
      },
      "source": [
        "## Grading = 10 Marks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fYwJv9T9--K"
      },
      "source": [
        "## Setup Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f23c480-0592-4484-87fd-2581d43e47e7",
        "cellView": "form",
        "id": "CZBuwQH_fqzW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "\n",
        "from IPython import get_ipython\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook=\"U2_MH1_AuthorIdentification\" #name of the notebook\n",
        "Answer = \"This notebook is graded by mentors on the day of hackathon\"\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget https://cdn.talentsprint.com/talentsprint1/archives/sc/aiml/experiment_related_data/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.rar\")\n",
        "    ipython.magic(\"sx unrar e /content/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.rar\")\n",
        "    print (\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "setup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioUZ8pJa9--n"
      },
      "source": [
        "### NOTE: You are allowed to use ML libraries such as Sklearn, NLTK, etc wherever applicable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR6_IRo7vTrs"
      },
      "source": [
        "### Downloading the required nltk Packages before moving ahead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BOKJN039--v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f291356-194e-4220-9cb5-96d49c374bd7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp3h5QWhNsUv"
      },
      "source": [
        "## **Stage 1:** Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bLZIvs3Ae70"
      },
      "source": [
        "### 1 Marks -> Ensure you appropriately split the multiple short stories for the below-mentioned authors, Which will be your training data.\n",
        "\n",
        "**1.** Before moving ahead choose two authors based on your team-number allocation: <br/>\n",
        "\n",
        "\n",
        "Team=1,5,9,13,17,21  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    Author-A Vs Author-B <br />\n",
        "Team=2,6,10,14,18,22 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         Author-B Vs Author-C <br />\n",
        "Team=3,7,11,15,19,23 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         Author-C Vs Author-D <br />\n",
        "Team=4,8,12,16,20,24 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;           Author-D Vs Author-E <br />\n",
        "\n",
        "\n",
        "\n",
        "**2.** Link to the short stories collection of each author for your problem: <br />\n",
        "\n",
        "*   Author-A -> Rudyard Kipling   [Short Stories Collection](http://www.gutenberg.org/files/2781/2781-0.txt) &nbsp;&nbsp;\n",
        "*   Author-B -> Anton Chekhov [Short Stories Collection](http://www.gutenberg.org/files/1732/1732-0.txt) &nbsp;&nbsp;\n",
        "*   Author-C -> Guy De Maupassant [Short Stories Collection](http://www.gutenberg.org/cache/epub/21327/pg21327.txt)&nbsp;&nbsp;\n",
        "*   Author-D -> Mark Twain [Short Stories Collection](http://www.gutenberg.org/files/245/245-0.txt)&nbsp;&nbsp;\n",
        "*   Author-E -> Saki [Short Stories Collection](http://www.gutenberg.org/files/1477/1477-0.txt)&nbsp;&nbsp;\n",
        "\n",
        "**Hint for downloading raw text from Gutenberg :**  Refer to the section \"Electronic Books\" in the following  [link](https://www.nltk.org/book/ch03.html) for the instructions.\n",
        "\n",
        "\n",
        "\n",
        "**Hint for finding the index of a text:**   You may use `raw.find()` and `raw.rfind()` in the same [link](https://www.nltk.org/book/ch03.html) to find the appropriate index of the start and end location\n",
        "\n",
        "**Hint for splitting the multiple stories:** Split the stories using long space (white space character)\n",
        "\n",
        "**Note:** Ignore the table of contents section from the given stories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ11fjLkoj8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa411d9f-21c3-4a00-f89a-d57100bd69c0"
      },
      "source": [
        "from urllib import request\n",
        "import requests\n",
        "import re\n",
        "\n",
        "# YOUR CODE HERE for downloading and splitting the multiple stories of respective authors which are allocated to you\n",
        "author_d_url = 'https://www.gutenberg.org/files/245/245-0.txt'\n",
        "author_e_url = 'https://www.gutenberg.org/files/1477/1477-0.txt'\n",
        "\n",
        "\n",
        "def extract_main_text(url,\n",
        "                    start_marker, end_marker,\n",
        "                    toc_start_marker,toc_end_marker):\n",
        "\n",
        "  # response = request.urlopen(url)\n",
        "  # raw = response.read().decode('utf8')\n",
        "\n",
        "  # Fetch the book content\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    raw = response.text\n",
        "  else:\n",
        "    print(f\"Failed to fetch the book. HTTP Status: {response.status_code}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "  print(f\"type: {type(raw)}\")\n",
        "  print(f\"length: {len(raw)}\")\n",
        "  print(raw[:50])\n",
        "\n",
        "  # Locate the start and end of the main text\n",
        "  start_index = raw.find(start_marker) + len(start_marker)\n",
        "  print(f\"start index: {start_index}\")\n",
        "  end_index = raw.rfind(end_marker)\n",
        "  print(f\"end index: {end_index}\")\n",
        "\n",
        "  # Extract the main content\n",
        "  if start_index != -1 and end_index != -1:\n",
        "      main_text = raw[start_index:end_index].strip()\n",
        "      print(f\"Main text starts at index {start_index}, ends at index {end_index}.\")\n",
        "      print(f\"Extracted text sample: {main_text[:500]}\")  # Display the first 500 characters\n",
        "\n",
        "      # Remove the table of contents section\n",
        "      toc_start_index = main_text.find(toc_start_marker)\n",
        "      toc_end_index = main_text.find(toc_end_marker)\n",
        "      print(f\"\\n Table of content index starts at {toc_start_index}, ends at index {toc_end_index}.\")\n",
        "\n",
        "      if toc_start_index != -1 and toc_end_index != -1:\n",
        "        print(f\"Dropping Table of contents \\n \")\n",
        "        main_text = main_text[:toc_start_index] + main_text[toc_end_index:]  # Exclude table of contents\n",
        "\n",
        "      # Split the text based on long whitespace characters\n",
        "      stories = re.split(r'\\s{3,}', main_text)  # Match sequences of 3 or more whitespace characters\n",
        "      print(f\"Number of stories/sections found: {len(stories)}\")\n",
        "\n",
        "    # Print a sample of the first few stories\n",
        "      for i, story in enumerate(stories[:8]):\n",
        "        print(f\"\\n--- Story {i + 1} ---\\n{story[:500]}...\\n\")  # Show the first 500 characters\n",
        "\n",
        "      return stories\n",
        "\n",
        "  else:\n",
        "      print(\"Start or end markers not found.\")\n",
        "\n",
        "\n",
        "# Define the markers for the start and end of the main text LIFE ON THE MISSISSIPPI\n",
        "start_marker_author_D = \"*** START OF THIS PROJECT GUTENBERG EBOOK LIFE ON THE MISSISSIPPI\"\n",
        "end_marker_author_D = \"*** END OF THIS PROJECT GUTENBERG EBOOK LIFE ON THE MISSISSIPPI\"\n",
        "toc_start_marker_author_D = \"TABLE OF CONTENTS\"\n",
        "toc_end_marker_author_D = \"CHAPTER 1\"  # Assuming Chapter 1 is the end of the table of contents\n",
        "author_d_main_text = extract_main_text(author_d_url,start_marker_author_D, end_marker_author_D, toc_start_marker_author_D, toc_end_marker_author_D)\n",
        "print(f\"Sample main text extracted from Author D \\n: {author_d_main_text[:10]}\")\n",
        "\n",
        "\n",
        "# Define the markers for the start and end of the main text TOYS OF PEACE\n",
        "start_marker_author_E = \"***START OF THE PROJECT GUTENBERG EBOOK THE TOYS OF PEACE***\"\n",
        "end_marker_author_E = \"***END OF THE PROJECT GUTENBERG EBOOK THE TOYS OF PEACE***\"\n",
        "toc_start_marker_author_E = \"Contents\"\n",
        "toc_end_marker_author_E = \"HECTOR HUGH MUNRO\"\n",
        "author_e_main_text = extract_main_text(author_e_url,start_marker_author_E, end_marker_author_E, toc_start_marker_author_E, toc_end_marker_author_E)\n",
        "print(f\"\\n Sample main text extracted from Author E: {author_e_main_text[:10]}\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type: <class 'str'>\n",
            "length: 842811\n",
            "﻿\r\n",
            "The Project Gutenberg EBook of Life On The Miss\n",
            "start index: 638\n",
            "end index: 823919\n",
            "Main text starts at index 638, ends at index 823919.\n",
            "Extracted text sample: ,\r\n",
            "COMPLETE ***\r\n",
            "\r\n",
            "Produced by David Widger. Earliest PG text edition produced by Graham\r\n",
            "Allan\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "LIFE ON THE MISSISSIPPI\r\n",
            "\r\n",
            "By Mark Twain\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "TABLE OF CONTENTS\r\n",
            "\r\n",
            "CHAPTER I. The Mississippi is Well worth Reading about.--It is\r\n",
            "Remarkable.--Instead of Widening towards its Mouth, it grows\r\n",
            "Narrower.--It Empties four hundred and six million Tons of Mud.--It\r\n",
            "was First Seen in 1542.--It is Older than some Pages in European\r\n",
            "History.--De Soto has the Pull.--Older than the Atlantic Coast.\n",
            "\n",
            " Table of content index starts at 155, ends at index 13440.\n",
            "Dropping Table of contents \n",
            " \n",
            "Number of stories/sections found: 2036\n",
            "\n",
            "--- Story 1 ---\n",
            ",\r\n",
            "COMPLETE ***...\n",
            "\n",
            "\n",
            "--- Story 2 ---\n",
            "Produced by David Widger. Earliest PG text edition produced by Graham\r\n",
            "Allan...\n",
            "\n",
            "\n",
            "--- Story 3 ---\n",
            "LIFE ON THE MISSISSIPPI...\n",
            "\n",
            "\n",
            "--- Story 4 ---\n",
            "By Mark Twain...\n",
            "\n",
            "\n",
            "--- Story 5 ---\n",
            "CHAPTER 1...\n",
            "\n",
            "\n",
            "--- Story 6 ---\n",
            "The River and Its History...\n",
            "\n",
            "\n",
            "--- Story 7 ---\n",
            "THE Mississippi is well worth reading about. It is not a commonplace\r\n",
            "river, but on the contrary is in all ways remarkable. Considering the\r\n",
            "Missouri its main branch, it is the longest river in the world--four\r\n",
            "thousand three hundred miles. It seems safe to say that it is also the\r\n",
            "crookedest river in the world, since in one part of its journey it uses\r\n",
            "up one thousand three hundred miles to cover the same ground that the\r\n",
            "crow would fly over in six hundred and seventy-five. It discharges three\r...\n",
            "\n",
            "\n",
            "--- Story 8 ---\n",
            "It is a remarkable river in this: that instead of widening toward its\r\n",
            "mouth, it grows narrower; grows narrower and deeper. From the junction\r\n",
            "of the Ohio to a point half way down to the sea, the width averages a\r\n",
            "mile in high water: thence to the sea the width steadily diminishes,\r\n",
            "until, at the 'Passes,' above the mouth, it is but little over half\r\n",
            "a mile. At the junction of the Ohio the Mississippi's depth is\r\n",
            "eighty-seven feet; the depth increases gradually, reaching one hundred\r\n",
            "and twenty-...\n",
            "\n",
            "Sample main text extracted from Author D \n",
            ": [',\\r\\nCOMPLETE ***', 'Produced by David Widger. Earliest PG text edition produced by Graham\\r\\nAllan', 'LIFE ON THE MISSISSIPPI', 'By Mark Twain', 'CHAPTER 1', 'The River and Its History', 'THE Mississippi is well worth reading about. It is not a commonplace\\r\\nriver, but on the contrary is in all ways remarkable. Considering the\\r\\nMissouri its main branch, it is the longest river in the world--four\\r\\nthousand three hundred miles. It seems safe to say that it is also the\\r\\ncrookedest river in the world, since in one part of its journey it uses\\r\\nup one thousand three hundred miles to cover the same ground that the\\r\\ncrow would fly over in six hundred and seventy-five. It discharges three\\r\\ntimes as much water as the St. Lawrence, twenty-five times as much\\r\\nas the Rhine, and three hundred and thirty-eight times as much as the\\r\\nThames. No other river has so vast a drainage-basin: it draws its water\\r\\nsupply from twenty-eight States and Territories; from Delaware, on the\\r\\nAtlantic seaboard, and from all the country between that and Idaho on\\r\\nthe Pacific slope--a spread of forty-five degrees of longitude. The\\r\\nMississippi receives and carries to the Gulf water from fifty-four\\r\\nsubordinate rivers that are navigable by steamboats, and from some\\r\\nhundreds that are navigable by flats and keels. The area of its\\r\\ndrainage-basin is as great as the combined areas of England, Wales,\\r\\nScotland, Ireland, France, Spain, Portugal, Germany, Austria, Italy,\\r\\nand Turkey; and almost all this wide region is fertile; the Mississippi\\r\\nvalley, proper, is exceptionally so.', \"It is a remarkable river in this: that instead of widening toward its\\r\\nmouth, it grows narrower; grows narrower and deeper. From the junction\\r\\nof the Ohio to a point half way down to the sea, the width averages a\\r\\nmile in high water: thence to the sea the width steadily diminishes,\\r\\nuntil, at the 'Passes,' above the mouth, it is but little over half\\r\\na mile. At the junction of the Ohio the Mississippi's depth is\\r\\neighty-seven feet; the depth increases gradually, reaching one hundred\\r\\nand twenty-nine just above the mouth.\", 'The difference in rise and fall is also remarkable--not in the upper,\\r\\nbut in the lower river. The rise is tolerably uniform down to Natchez\\r\\n(three hundred and sixty miles above the mouth)--about fifty feet.\\r\\nBut at Bayou La Fourche the river rises only twenty-four feet; at New\\r\\nOrleans only fifteen, and just above the mouth only two and one half.', \"An article in the New Orleans 'Times-Democrat,' based upon reports of\\r\\nable engineers, states that the river annually empties four hundred and\\r\\nsix million tons of mud into the Gulf of Mexico--which brings to mind\\r\\nCaptain Marryat's rude name for the Mississippi--'the Great Sewer.' This\\r\\nmud, solidified, would make a mass a mile square and two hundred and\\r\\nforty-one feet high.\"]\n",
            "type: <class 'str'>\n",
            "length: 379342\n",
            "﻿The Project Gutenberg eBook, The Toys of Peace, b\n",
            "start index: 611\n",
            "end index: 360283\n",
            "Main text starts at index 611, ends at index 360283.\n",
            "Extracted text sample: Transcribed from the 1919 John Lane edition by Jane Duff and David Price,\n",
            "email ccx074@pglaf.org\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                            THE TOYS OF PEACE\n",
            "                             AND OTHER PAPERS\n",
            "\n",
            "\n",
            "                                * * * * *\n",
            "\n",
            "                                    TO\n",
            "                         THE 22ND ROYAL FUSILIERS\n",
            "\n",
            "                                * * * * *\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Note\n",
            "\n",
            "\n",
            "Thanks are due to the Editors of the _Morning Post_, the _Westminster\n",
            "Gazette_, and the _Byst\n",
            "\n",
            " Table of content index starts at 706, ends at index 2686.\n",
            "Dropping Table of contents \n",
            " \n",
            "Number of stories/sections found: 1270\n",
            "\n",
            "--- Story 1 ---\n",
            "Transcribed from the 1919 John Lane edition by Jane Duff and David Price,\n",
            "email ccx074@pglaf.org...\n",
            "\n",
            "\n",
            "--- Story 2 ---\n",
            "THE TOYS OF PEACE...\n",
            "\n",
            "\n",
            "--- Story 3 ---\n",
            "AND OTHER PAPERS...\n",
            "\n",
            "\n",
            "--- Story 4 ---\n",
            "* * * * *...\n",
            "\n",
            "\n",
            "--- Story 5 ---\n",
            "TO...\n",
            "\n",
            "\n",
            "--- Story 6 ---\n",
            "THE 22ND ROYAL FUSILIERS...\n",
            "\n",
            "\n",
            "--- Story 7 ---\n",
            "* * * * *...\n",
            "\n",
            "\n",
            "--- Story 8 ---\n",
            "Note...\n",
            "\n",
            "\n",
            " Sample main text extracted from Author E: ['Transcribed from the 1919 John Lane edition by Jane Duff and David Price,\\r\\nemail ccx074@pglaf.org', 'THE TOYS OF PEACE', 'AND OTHER PAPERS', '* * * * *', 'TO', 'THE 22ND ROYAL FUSILIERS', '* * * * *', 'Note', 'Thanks are due to the Editors of the _Morning Post_, the _Westminster\\r\\nGazette_, and the _Bystander_ for their amiability in allowing tales that\\r\\nappeared in these journals to be reproduced in the present volume.', 'R. R.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIC7IE8lmFDA"
      },
      "source": [
        "## **Stage 2**: Experiment with Handcrafted features representation\n",
        "Extract Handcrafted features for the obtained short stories from **Stage-1**\n",
        "\n",
        "**Stylometry:**\n",
        "\n",
        "Each person has a unique vocabulary, sometimes rich, sometimes limited. Although a larger vocabulary is usually associated with literary quality, this is not always the case. Ernest Hemingway is famous for using a surprisingly small number of different words in his writing, which did not prevent him from winning the Nobel Prize for Literature in 1954.\n",
        "\n",
        "Some people write in short sentences, while others prefer long blocks of text consisting of many clauses. No two people use semicolons, em-dashes, and other forms of punctuation in the same way.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**You may explore the following ways to analyze the text and generate handcrafted features by searching text in a probing way:**\n",
        "\n",
        "a)  Could the style of punctuation usage help as a handcrafted feature? Both by those who follow punctuations and by those who don't? Interesting [link](https://qwiklit.com/2014/03/05/top-10-authors-who-ignored-the-basic-rules-of-punctuation/)\n",
        "\n",
        "b) The same word can sometimes be used in different contexts repeatedly by different authors. Could this fact be converted as a handcrafted feature? [link](https://www.nltk.org/book/ch01.html)\n",
        "\n",
        "c) The above two are merely examples; As you might have noticed already the NLTK book [link](https://www.nltk.org/book/) offers several methods of analyzing and understanding the text. Each of these analyses is in itself capable of being a handcrafted feature. **However for your evaluation a minimal set of useful handcrafted features which is helping you prove a classification of an is sufficient**\n",
        "\n",
        "d) Could most common words be used to distinguish authors?  Refer \"Counting Vocabulary\" section of the [link](https://www.nltk.org/book/ch01.html)\n",
        "\n",
        "e) How about using a count of most frequently used bi-gram, tri-grams, and using it to classify an author?\n",
        "\n",
        "f) How about using the frequency histogram of the most frequently used words across the stories by a given author a useful feature?\n",
        "\n",
        "The limit here is endlessly limited only by your imagination, and of course your accuracy! :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmJX5Jl1CRKg"
      },
      "source": [
        "### 1 Marks ->  a) List 6 handcrafted features to distinguish author stories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUJlJdy_EGAP"
      },
      "source": [
        "# For eg:\n",
        "# 1. UniqueWords\n",
        "# 2. AvgSentLength\n",
        "# List the other handcrafted features here\n",
        "# 3. average_word_length\n",
        "# 4. vocabulary_richness\n",
        "# 5. stopword_ratio\n",
        "# 6. punctuation_usage\n",
        "# 7. parts of speech distribution\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4Bynbq3C4o1"
      },
      "source": [
        "###  2 Marks -> b) Write functions for any 4 of the above 6 handcrafted features and label your authors accordingly.\n",
        "\n",
        "- Get any 4 hand crafted features from the above listed 6 hand-crafted features for every story obtained from **stage-1**.\n",
        "- Identify your target variable as an author and label them accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwA5nx2IEAhN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "310cd66c-2044-4903-eb47-1eff1615f146"
      },
      "source": [
        "# Stories_list    UniqueWords    AvgSentLength     Label\n",
        "#     1               x1               x2            y\n",
        "\n",
        "# YOUR CODE HERE\n",
        "import re\n",
        "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "def extract_features(text, author_name):\n",
        "    # Tokenize words and sentences\n",
        "    words = word_tokenize(text)\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Basic counts\n",
        "    num_words = len(words)\n",
        "    num_sentences = len(sentences)\n",
        "    num_unique_words = len(set(words))\n",
        "    num_chars = sum(len(word) for word in words)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    num_stopwords = sum(1 for word in words if word.lower() in stop_words)\n",
        "    punctuation_count = Counter(char for char in text if char in \",.!?;:'\\\"-\")\n",
        "\n",
        "    # Features\n",
        "    features = {\n",
        "        \"unique_words\": num_unique_words,\n",
        "        \"average_word_length\": num_chars / num_words,\n",
        "        \"vocabulary_richness\": num_unique_words / num_words,\n",
        "        \"stopword_ratio\": num_stopwords / num_words,\n",
        "        \"average_sentence_length\": num_words / num_sentences,\n",
        "        #\"punctuation_usage\": dict(punctuation_count),\n",
        "        #\"pos_distribution\": dict(Counter(tag for word, tag in pos_tag(words))),\n",
        "        \"label\": author_name,\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# Extract features for each story\n",
        "def extract_features_per_story(stories, author_name):\n",
        "  features_per_story = []\n",
        "  for i, story in enumerate(stories):\n",
        "    #print(f\"Processing story {i + 1}/{len(stories)}...\")\n",
        "    features = extract_features(story, author_name)\n",
        "    features_per_story.append(features)\n",
        "  return features_per_story\n",
        "\n",
        "\n",
        "features_author_D = extract_features_per_story(author_d_main_text, \"Mark Twain\")\n",
        "print(features_author_D[0:10])\n",
        "\n",
        "features_author_E = extract_features_per_story(author_e_main_text, \"Saki\")\n",
        "print(features_author_E[0:10])\n",
        "\n",
        "authors_df_D = pd.DataFrame(features_author_D)\n",
        "authors_df_E = pd.DataFrame(features_author_E)\n",
        "\n",
        "authors_df = pd.concat([authors_df_D, authors_df_E], ignore_index=True)\n",
        "print(authors_df['label'].unique())\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'unique_words': 3, 'average_word_length': 2.4, 'vocabulary_richness': 0.6, 'stopword_ratio': 0.0, 'average_sentence_length': 5.0, 'label': 'Mark Twain'}, {'unique_words': 12, 'average_word_length': 4.923076923076923, 'vocabulary_richness': 0.9230769230769231, 'stopword_ratio': 0.15384615384615385, 'average_sentence_length': 6.5, 'label': 'Mark Twain'}, {'unique_words': 4, 'average_word_length': 5.0, 'vocabulary_richness': 1.0, 'stopword_ratio': 0.5, 'average_sentence_length': 4.0, 'label': 'Mark Twain'}, {'unique_words': 3, 'average_word_length': 3.6666666666666665, 'vocabulary_richness': 1.0, 'stopword_ratio': 0.3333333333333333, 'average_sentence_length': 3.0, 'label': 'Mark Twain'}, {'unique_words': 2, 'average_word_length': 4.0, 'vocabulary_richness': 1.0, 'stopword_ratio': 0.0, 'average_sentence_length': 2.0, 'label': 'Mark Twain'}, {'unique_words': 5, 'average_word_length': 4.2, 'vocabulary_richness': 1.0, 'stopword_ratio': 0.6, 'average_sentence_length': 5.0, 'label': 'Mark Twain'}, {'unique_words': 135, 'average_word_length': 4.2927756653992395, 'vocabulary_richness': 0.5133079847908745, 'stopword_ratio': 0.41064638783269963, 'average_sentence_length': 32.875, 'label': 'Mark Twain'}, {'unique_words': 62, 'average_word_length': 3.962962962962963, 'vocabulary_richness': 0.5740740740740741, 'stopword_ratio': 0.42592592592592593, 'average_sentence_length': 36.0, 'label': 'Mark Twain'}, {'unique_words': 50, 'average_word_length': 3.9178082191780823, 'vocabulary_richness': 0.684931506849315, 'stopword_ratio': 0.410958904109589, 'average_sentence_length': 24.333333333333332, 'label': 'Mark Twain'}, {'unique_words': 58, 'average_word_length': 4.243243243243243, 'vocabulary_richness': 0.7837837837837838, 'stopword_ratio': 0.2702702702702703, 'average_sentence_length': 37.0, 'label': 'Mark Twain'}]\n",
            "[{'unique_words': 18, 'average_word_length': 4.555555555555555, 'vocabulary_richness': 1.0, 'stopword_ratio': 0.2222222222222222, 'average_sentence_length': 18.0, 'label': 'Saki'}, {'unique_words': 4, 'average_word_length': 3.5, 'vocabulary_richness': 1.0, 'stopword_ratio': 0.5, 'average_sentence_length': 4.0, 'label': 'Saki'}, {'unique_words': 3, 'average_word_length': 4.666666666666667, 'vocabulary_richness': 1.0, 'stopword_ratio': 0.6666666666666666, 'average_sentence_length': 3.0, 'label': 'Saki'}, {'unique_words': 1, 'average_word_length': 1.0, 'vocabulary_richness': 0.2, 'stopword_ratio': 0.0, 'average_sentence_length': 5.0, 'label': 'Saki'}, {'unique_words': 1, 'average_word_length': 2.0, 'vocabulary_richness': 1.0, 'stopword_ratio': 1.0, 'average_sentence_length': 1.0, 'label': 'Saki'}, {'unique_words': 4, 'average_word_length': 5.25, 'vocabulary_richness': 1.0, 'stopword_ratio': 0.25, 'average_sentence_length': 4.0, 'label': 'Saki'}, {'unique_words': 1, 'average_word_length': 1.0, 'vocabulary_richness': 0.2, 'stopword_ratio': 0.0, 'average_sentence_length': 5.0, 'label': 'Saki'}, {'unique_words': 1, 'average_word_length': 4.0, 'vocabulary_richness': 1.0, 'stopword_ratio': 0.0, 'average_sentence_length': 1.0, 'label': 'Saki'}, {'unique_words': 29, 'average_word_length': 4.783783783783784, 'vocabulary_richness': 0.7837837837837838, 'stopword_ratio': 0.4864864864864865, 'average_sentence_length': 37.0, 'label': 'Saki'}, {'unique_words': 3, 'average_word_length': 1.3333333333333333, 'vocabulary_richness': 1.0, 'stopword_ratio': 0.0, 'average_sentence_length': 3.0, 'label': 'Saki'}]\n",
            "['Mark Twain' 'Saki']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "authors_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "EHDBZ8GWBj-4",
        "outputId": "311bcb74-c614-4624-8143-c63a39f05a37"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   unique_words  average_word_length  vocabulary_richness  stopword_ratio  \\\n",
              "0             3             2.400000             0.600000        0.000000   \n",
              "1            12             4.923077             0.923077        0.153846   \n",
              "2             4             5.000000             1.000000        0.500000   \n",
              "3             3             3.666667             1.000000        0.333333   \n",
              "4             2             4.000000             1.000000        0.000000   \n",
              "\n",
              "   average_sentence_length       label  \n",
              "0                      5.0  Mark Twain  \n",
              "1                      6.5  Mark Twain  \n",
              "2                      4.0  Mark Twain  \n",
              "3                      3.0  Mark Twain  \n",
              "4                      2.0  Mark Twain  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f85c8ad7-d936-42a3-a166-9253455119d7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>unique_words</th>\n",
              "      <th>average_word_length</th>\n",
              "      <th>vocabulary_richness</th>\n",
              "      <th>stopword_ratio</th>\n",
              "      <th>average_sentence_length</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>2.400000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Mark Twain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12</td>\n",
              "      <td>4.923077</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>6.5</td>\n",
              "      <td>Mark Twain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Mark Twain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3.666667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Mark Twain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Mark Twain</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f85c8ad7-d936-42a3-a166-9253455119d7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f85c8ad7-d936-42a3-a166-9253455119d7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f85c8ad7-d936-42a3-a166-9253455119d7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-69cf4a0c-1b52-4610-9484-bb87b9efc4f5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-69cf4a0c-1b52-4610-9484-bb87b9efc4f5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-69cf4a0c-1b52-4610-9484-bb87b9efc4f5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "authors_df",
              "summary": "{\n  \"name\": \"authors_df\",\n  \"rows\": 3306,\n  \"fields\": [\n    {\n      \"column\": \"unique_words\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 49,\n        \"min\": 1,\n        \"max\": 719,\n        \"num_unique_values\": 229,\n        \"samples\": [\n          185,\n          23,\n          78\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"average_word_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7776542861781829,\n        \"min\": 1.0,\n        \"max\": 13.0,\n        \"num_unique_values\": 1797,\n        \"samples\": [\n          4.737704918032787,\n          3.888888888888889,\n          4.160493827160494\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vocabulary_richness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15366511667802782,\n        \"min\": 0.14285714285714285,\n        \"max\": 1.0,\n        \"num_unique_values\": 1072,\n        \"samples\": [\n          0.7529411764705882,\n          0.6781609195402298,\n          0.5106382978723404\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stopword_ratio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13247938485643715,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1020,\n        \"samples\": [\n          0.5230769230769231,\n          0.4056603773584906,\n          0.4174757281553398\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"average_sentence_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.593594707973125,\n        \"min\": 1.0,\n        \"max\": 212.0,\n        \"num_unique_values\": 663,\n        \"samples\": [\n          31.363636363636363,\n          27.90909090909091,\n          72.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Saki\",\n          \"Mark Twain\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sample Funtion To Process Books\n",
        "# import requests\n",
        "# import re\n",
        "# from nltk import word_tokenize, sent_tokenize, pos_tag\n",
        "# from nltk.corpus import stopwords\n",
        "# from collections import Counter\n",
        "\n",
        "# # Feature extraction function\n",
        "# def extract_features(text):\n",
        "#     # Tokenize words and sentences\n",
        "#     words = word_tokenize(text)\n",
        "#     sentences = sent_tokenize(text)\n",
        "\n",
        "#     # Basic counts\n",
        "#     num_words = len(words)\n",
        "#     num_sentences = len(sentences)\n",
        "#     num_unique_words = len(set(words))\n",
        "#     num_chars = sum(len(word) for word in words)\n",
        "#     stop_words = set(stopwords.words('english'))\n",
        "#     num_stopwords = sum(1 for word in words if word.lower() in stop_words)\n",
        "#     punctuation_count = Counter(char for char in text if char in \",.!?;:'\\\"-\")\n",
        "\n",
        "#     # Features\n",
        "#     features = {\n",
        "#         \"average_word_length\": num_chars / num_words if num_words else 0,\n",
        "#         \"vocabulary_richness\": num_unique_words / num_words if num_words else 0,\n",
        "#         \"stopword_ratio\": num_stopwords / num_words if num_words else 0,\n",
        "#         \"average_sentence_length\": num_words / num_sentences if num_sentences else 0,\n",
        "#         \"punctuation_usage\": dict(punctuation_count),\n",
        "#         \"pos_distribution\": dict(Counter(tag for word, tag in pos_tag(words))),\n",
        "#     }\n",
        "\n",
        "#     return features\n",
        "\n",
        "# # Fetch Gutenberg book content\n",
        "# def fetch_gutenberg_book(url):\n",
        "#     response = requests.get(url)\n",
        "#     if response.status_code == 200:\n",
        "#         return response.text\n",
        "#     else:\n",
        "#         print(f\"Failed to fetch the book. HTTP Status: {response.status_code}\")\n",
        "#         return None\n",
        "\n",
        "# # Process the book and extract features for each story\n",
        "# def process_book(url):\n",
        "#     raw = fetch_gutenberg_book(url)\n",
        "#     if raw is None:\n",
        "#         return None\n",
        "\n",
        "#     # Define the markers for the main text\n",
        "#     start_marker = \"*** START OF THIS PROJECT GUTENBERG EBOOK\"\n",
        "#     end_marker = \"*** END OF THIS PROJECT GUTENBERG EBOOK\"\n",
        "\n",
        "#     # Locate the start and end of the main text\n",
        "#     start_index = raw.find(start_marker) + len(start_marker)\n",
        "#     end_index = raw.rfind(end_marker)\n",
        "#     if start_index == -1 or end_index == -1:\n",
        "#         print(\"Start or end markers not found.\")\n",
        "#         return None\n",
        "\n",
        "#     # Extract the main text\n",
        "#     main_text = raw[start_index:end_index].strip()\n",
        "\n",
        "#     # Remove the table of contents\n",
        "#     toc_start_marker = \"Contents\"\n",
        "#     toc_end_marker = \"Chapter 1\"\n",
        "#     toc_start_index = main_text.find(toc_start_marker)\n",
        "#     toc_end_index = main_text.find(toc_end_marker)\n",
        "#     if toc_start_index != -1 and toc_end_index != -1:\n",
        "#         main_text = main_text[:toc_start_index] + main_text[toc_end_index:]\n",
        "\n",
        "#     # Split the text into sections using long spaces\n",
        "#     stories = re.split(r'\\s{3,}', main_text)\n",
        "\n",
        "#     # Extract features for each story\n",
        "#     features_per_story = []\n",
        "#     for i, story in enumerate(stories):\n",
        "#         print(f\"Processing story {i + 1}/{len(stories)}...\")\n",
        "#         features = extract_features(story)\n",
        "#         features_per_story.append(features)\n",
        "\n",
        "#     return features_per_story\n",
        "\n",
        "# # Example usage\n",
        "# url = \"https://www.gutenberg.org/files/1342/1342-0.txt\"  # Pride and Prejudice\n",
        "# features = process_book(url)\n",
        "\n",
        "# # Display features for the first story as an example\n",
        "# if features:\n",
        "#     print(f\"Features for the first story:\\n{features[0]}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wi8t99cJ3xl8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiJOpSJ0yJod"
      },
      "source": [
        "##**Stage 3:** Experiment with Text processing and representation:\n",
        "Extract features using TFIDF or CountVectorizer or Word2vec for the obtained short stories from **Stage-1**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecMmcZfm_Eek"
      },
      "source": [
        "### 1 Mark -> a) Performing basic cleanup operations such as removing the newline characters and removing trailing spaces\n",
        "\n",
        "**For example,** Your sentence looks as follows \\[' This is a sentence\\n\\r. Another sentence \\n'].\n",
        "\n",
        "After newline removal from the above example, your sentence will look like \\['This is a sentence. Another sentence'].\n",
        "\n",
        " In order to do this, you can try using a combination of split() and join()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw2DhQGK_Eel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c0e5082-5935-4c1f-9d18-4945a0c64d3c"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "# Basic cleanup function\n",
        "def clean_text(text):\n",
        "    # Remove newline characters\n",
        "    cleaned_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "    # Remove multiple spaces and trailing spaces\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "\n",
        "    # Remove newline characters and collapse multiple spaces\n",
        "    cleaned_text = ' '.join(text.split())\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Clean each story\n",
        "author_d_cleaned_stories = [clean_text(story) for story in author_d_main_text]\n",
        "author_d_cleaned_stories[0:10]\n",
        "\n",
        "author_e_cleaned_stories = [clean_text(story) for story in author_e_main_text]\n",
        "author_e_cleaned_stories[0:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Transcribed from the 1919 John Lane edition by Jane Duff and David Price, email ccx074@pglaf.org',\n",
              " 'THE TOYS OF PEACE',\n",
              " 'AND OTHER PAPERS',\n",
              " '* * * * *',\n",
              " 'TO',\n",
              " 'THE 22ND ROYAL FUSILIERS',\n",
              " '* * * * *',\n",
              " 'Note',\n",
              " 'Thanks are due to the Editors of the _Morning Post_, the _Westminster Gazette_, and the _Bystander_ for their amiability in allowing tales that appeared in these journals to be reproduced in the present volume.',\n",
              " 'R. R.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine two books and prepare the dataset\n",
        "def prepare_dataset(book1_stories, book2_stories, author1, author2):\n",
        "\n",
        "    # Combine books into a dataset\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    data = book1_stories + book2_stories\n",
        "    labels = [author1] * len(book1_stories) + [author2] * len(book2_stories)\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "data, labels = prepare_dataset(author_d_cleaned_stories, author_e_cleaned_stories, \"Mark Twain\", \"Saki\")\n",
        "\n",
        "print(len(data), len(labels))\n",
        "print(data[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z66ZpHYE3G0",
        "outputId": "fce020f3-8f28-4fd3-fe88-dfe9d33b8c36"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3306 3306\n",
            "[', COMPLETE ***', 'Produced by David Widger. Earliest PG text edition produced by Graham Allan', 'LIFE ON THE MISSISSIPPI', 'By Mark Twain', 'CHAPTER 1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z64gLpY2_Ee1"
      },
      "source": [
        "###  2 Marks-> b) Generate vectors for the given stories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbkfSFT2xXJY"
      },
      "source": [
        "Create a representation of text, convert it into vectors (numbers)\n",
        "\n",
        "\n",
        "**Use any one** of the following algorithms for this task :\n",
        "\n",
        "* Countvectorizer or\n",
        "* TFIDFVectorizer or\n",
        "* Word2Vec (The word2vec bin file (AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD) can be downloaded as a part of setup  )\n",
        "  * perform sentence level tokenization and word level tokenization for the given stories\n",
        "\n",
        "    **Example of sentences as list of words:**<br/>\n",
        "    **Before:** ['This is a sentence .' , ' Another sentence']<br/>\n",
        "    **After:** ['This', 'is' ,'a', 'sentence' , ' . ' , ' Another ', ' sentence ' ]\n",
        " * Assign the respective label associated for each vector representation of the extracted word\n",
        "\n",
        "References Documents:\n",
        "\n",
        "1.   [Countvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
        "2.  [TFIDFVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kyk9-3BRhCf-"
      },
      "source": [
        "# YOUR CODE HERE (HINT: Convert to numpy array if needed)\n",
        "#Countvectorizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgEtwoYw_Eet"
      },
      "source": [
        "###  1 Mark -> c) Is stop word removal necessary in the context of author identification? Your thoughts below?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLrl-HCA_Eeu"
      },
      "source": [
        "# YOUR ANSWER IN TEXT\n",
        "# Yes its important to remove them because it helps in focussing distinctive charateristics of author's writing by removing\n",
        "# common words that do not contribute to author's style of writing."
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWZ-5hiZN-jR"
      },
      "source": [
        "##**Stage 4:** Classification :\n",
        "\n",
        "### Expected accuracy is above 85%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9eqZubqiRKi"
      },
      "source": [
        "### 2 Marks -> Perform a classification using either features obtained from Stage2 or Stage3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Create an object for all the algorithms\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model1 = DecisionTreeClassifier(max_depth=5)\n",
        "model2 = KNeighborsClassifier(n_neighbors=8)\n",
        "model3 = SGDClassifier()\n",
        "model4 = SVC(kernel='linear')\n",
        "model5 = RandomForestClassifier(max_depth= 100, random_state=42)\n",
        "model6 = LogisticRegression(max_iter=200)\n",
        "\n",
        "models = [model1, model2, model3, model4, model5, model6]"
      ],
      "metadata": {
        "id": "sH_zcaVNgnlK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "# Stage 2\n",
        "# authors_df = handcrafted features with labels\n",
        "#authors_df\n",
        "\n",
        "# Apply encoding to overall_lit column which is the label\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelEncoder = LabelEncoder()\n",
        "authors_df['label'] = labelEncoder.fit_transform(authors_df['label'])\n",
        "print(authors_df['label'].value_counts())\n",
        "\n",
        "features = authors_df.drop('label', axis = 1)\n",
        "labels = authors_df['label']\n",
        "print(features.shape, labels.shape)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(features, labels, test_size = 0.2,random_state = 42)\n",
        "\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "for model in models:\n",
        "      model.fit(X_train_scaled, y_train)         # fit the model\n",
        "      y_pred= model.predict(X_test_scaled)       # then predict on the test set\n",
        "      accuracy= accuracy_score(y_test, y_pred)\n",
        "      print(\"Accuracy (in %):\", model, \"is\", accuracy)\n",
        "      # print(\"\\nClassification Report:\")\n",
        "      # print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3RJ9yeEeowv",
        "outputId": "3ae6ff05-2b51-4875-c4d2-2f278bb33b53"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "0    2036\n",
            "1    1270\n",
            "Name: count, dtype: int64\n",
            "(3306, 5) (3306,)\n",
            "Accuracy (in %): DecisionTreeClassifier(max_depth=5) is 0.6691842900302115\n",
            "Accuracy (in %): KNeighborsClassifier(n_neighbors=8) is 0.7054380664652568\n",
            "Accuracy (in %): SGDClassifier() is 0.6540785498489426\n",
            "Accuracy (in %): SVC(kernel='linear') is 0.6555891238670695\n",
            "Accuracy (in %): RandomForestClassifier(max_depth=100, random_state=42) is 0.7084592145015106\n",
            "Accuracy (in %): LogisticRegression(max_iter=200) is 0.6631419939577039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKmfqV25a05p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44d45552-f198-49bd-b1d2-b0025d122b50"
      },
      "source": [
        "\n",
        "\n",
        "# Stage 3\n",
        "# Train and classify using TF-IDF and Random Forest\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# TF-IDF\n",
        "def classify_authors_tfidf(data, labels):\n",
        "    # Split into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Vectorize text using TF-IDF\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Limit to 1000 features\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "    for model in models:\n",
        "      model.fit(X_train_tfidf, y_train)         # fit the model\n",
        "      y_pred= model.predict(X_test_tfidf)       # then predict on the test set\n",
        "      accuracy= accuracy_score(y_test, y_pred)\n",
        "      print(\"Accuracy (in %):\", model, \"is\", accuracy)\n",
        "      # print(\"\\nClassification Report:\")\n",
        "      # print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "# Train and classify using CountVectorizer and Random Forest\n",
        "def classify_authors_count_vectorizer(data, labels):\n",
        "    # Split into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Vectorize text using CountVectorizer\n",
        "    count_vectorizer = CountVectorizer(max_features=1000)  # Limit to 1000 features\n",
        "    X_train_counts = count_vectorizer.fit_transform(X_train)\n",
        "    X_test_counts = count_vectorizer.transform(X_test)\n",
        "\n",
        "    for model in models:\n",
        "      model.fit(X_train_counts, y_train)         # fit the model\n",
        "      y_pred= model.predict(X_test_counts)       # then predict on the test set\n",
        "      accuracy= accuracy_score(y_test, y_pred)\n",
        "      print(\"Accuracy (in %):\", model, \"is\", accuracy)\n",
        "      # print(\"\\nClassification Report:\")\n",
        "      # print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Classify authors\n",
        "print('Classifying using TF-IDF')\n",
        "classify_authors_tfidf(data, labels)\n",
        "print('\\nClassifying using Count Vectoriser')\n",
        "classify_authors_count_vectorizer(data, labels)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifying using TF-IDF\n",
            "Accuracy (in %): DecisionTreeClassifier(max_depth=5) is 0.7235649546827795\n",
            "Accuracy (in %): KNeighborsClassifier(n_neighbors=8) is 0.6178247734138973\n",
            "Accuracy (in %): SGDClassifier() is 0.8595166163141994\n",
            "Accuracy (in %): SVC(kernel='linear') is 0.8670694864048338\n",
            "Accuracy (in %): RandomForestClassifier(max_depth=100, random_state=42) is 0.8338368580060423\n",
            "Accuracy (in %): LogisticRegression(max_iter=200) is 0.850453172205438\n",
            "\n",
            "Classifying using Count Vectoriser\n",
            "Accuracy (in %): DecisionTreeClassifier(max_depth=5) is 0.7084592145015106\n",
            "Accuracy (in %): KNeighborsClassifier(n_neighbors=8) is 0.6540785498489426\n",
            "Accuracy (in %): SGDClassifier() is 0.8308157099697885\n",
            "Accuracy (in %): SVC(kernel='linear') is 0.8353474320241692\n",
            "Accuracy (in %): RandomForestClassifier(max_depth=100, random_state=42) is 0.8277945619335347\n",
            "Accuracy (in %): LogisticRegression(max_iter=200) is 0.8429003021148036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title w2v code sample from lab\n",
        "# # Creating empty final dataframe\n",
        "# docs_vectors = pd.DataFrame()\n",
        "# W2Vmodel = gensim.models.KeyedVectors.load_word2vec_format('AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin', binary=True, limit=500000)\n",
        "\n",
        "# # Removing stop words\n",
        "# stopwords = nltk.corpus.stopwords.words('english')\n",
        "# text = df['text'].astype(str)\n",
        "# # Looping through each document and cleaning it\n",
        "# for doc in text.str.lower().str.replace('[^a-z ]', ''):\n",
        "#     temp = pd.DataFrame()\n",
        "#     for word in doc.split(' '):\n",
        "#         # If word is not present in stopwords then (try)\n",
        "#         if word not in stopwords and word.isalpha():\n",
        "#             try:\n",
        "#                 # If word is present in embeddings then get the vector representation and append it to temporary dataframe\n",
        "#                 word_vec = W2Vmodel[word]\n",
        "#                 temp = temp._append(pd.Series(word_vec), ignore_index = True)\n",
        "\n",
        "#             except:\n",
        "#                 pass\n",
        "#     # Take the average of vectors for each word\n",
        "#     doc_vector = temp.mean()\n",
        "#     # Append each document value to the final dataframe\n",
        "#     docs_vectors = docs_vectors._append(doc_vector, ignore_index = True)\n",
        "# docs_vectors.shape"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kgT4-DEaTe7V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classify using word to Vec\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "# # Tokenize stories for Word2Vec\n",
        "# def tokenize_stories(stories):\n",
        "#     return [story.split() for story in stories]\n",
        "\n",
        "# Tokenize the sentence and get vocab words\n",
        "def tokenize_stories(stories):\n",
        "  pre_processed_words = []\n",
        "  for story in stories:\n",
        "    words = word_tokenize(story)\n",
        "    words = [lemmatizer.lemmatize(w) for w in words]\n",
        "    pre_processed_words.extend(words)\n",
        "\n",
        "  pre_processed_words = set(pre_processed_words)\n",
        "\n",
        "  pre_processed_words = [word for word in pre_processed_words if word not in stopwords]\n",
        "  return pre_processed_words\n",
        "\n",
        "\n",
        "\n",
        "# Compute average Word2Vec vector for a story\n",
        "def compute_story_vector(model, story, vector_size):\n",
        "    vectors = [model.wv[word] for word in story if word in model.wv and word not in stopwords]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(vector_size)\n",
        "\n",
        "# Prepare dataset with labels\n",
        "def prepare_dataset(story_vectors, labels):\n",
        "    return np.array(story_vectors), labels\n",
        "\n",
        "\n",
        "# Train and classify using Word2Vec vectors and Random Forest\n",
        "def classify_authors(story_vectors, labels):\n",
        "    # Split into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(story_vectors, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    for model in models:\n",
        "      model.fit(X_train, y_train)         # fit the model\n",
        "      y_pred= model.predict(X_test)       # then predict on the test set\n",
        "      accuracy= accuracy_score(y_test, y_pred)\n",
        "      print(\"Accuracy (in %):\", model, \"is\", accuracy)\n",
        "      # print(\"\\nClassification Report:\")\n",
        "      # print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Tokenize stories\n",
        "book1_tokens = tokenize_stories(author_d_main_text)\n",
        "book2_tokens = tokenize_stories(author_e_main_text)\n",
        "\n",
        "# Train Word2Vec model on combined tokens\n",
        "all_tokens = book1_tokens + book2_tokens\n",
        "vector_size = 100  # Dimensionality of Word2Vec vectors\n",
        "word2vec_model = Word2Vec(sentences=all_tokens, vector_size=vector_size, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Compute story vectors\n",
        "book1_vectors = [compute_story_vector(word2vec_model, story, vector_size) for story in book1_tokens]\n",
        "book2_vectors = [compute_story_vector(word2vec_model, story, vector_size) for story in book2_tokens]\n",
        "\n",
        "# Prepare dataset\n",
        "story_vectors, labels = prepare_dataset(book1_vectors + book2_vectors, [\"Mark Twain\"] * len(book1_vectors) + [\"Saki\"] * len(book2_vectors))\n",
        "# Classify authors\n",
        "classify_authors(story_vectors, labels)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_JMA9M5P2_b",
        "outputId": "8e3fb24a-0054-4db0-ec87-7051658fe996"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (in %): DecisionTreeClassifier() is 0.48131056760498386\n",
            "Accuracy (in %): KNeighborsClassifier(n_neighbors=8) is 0.5535302261190586\n",
            "Accuracy (in %): SGDClassifier() is 0.6031379787724965\n",
            "Accuracy (in %): SVC(kernel='linear') is 0.6031379787724965\n",
            "Accuracy (in %): RandomForestClassifier(max_depth=100, random_state=42) is 0.48777111213659435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXGRHmvSJWRd"
      },
      "source": [
        "# Further Ideas for exploration after the hackathon:\n",
        "\n",
        "**Statistical analysis** of text using NLP, by analysis meaning of sentences, feature based grammars and analyzing structure of sentences!\n",
        "\n",
        "reference: www.nltk.org/book"
      ]
    }
  ]
}