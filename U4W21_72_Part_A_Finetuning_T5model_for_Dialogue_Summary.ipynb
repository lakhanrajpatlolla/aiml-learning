{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakhanrajpatlolla/aiml-learning/blob/master/U4W21_72_Part_A_Finetuning_T5model_for_Dialogue_Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaHNxUntdN6a"
      },
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint\n",
        "### Part-A: Finetuning a Seq2Seq (T5) Model for Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference notebook not for submission"
      ],
      "metadata": {
        "id": "b6FLla4u4TOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **NOTE that** this Assignment is in 2 parts:\n",
        "> - Part-A: Finetuning a Seq2Seq (T5) Model for Summarization\n",
        "> - Part-B: PEFT for Dialogue Summary\n",
        ">\n",
        ">Only Part-B needs to be submitted for grading."
      ],
      "metadata": {
        "id": "NdmcP2rmIiTj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* fine tune a T5 model, `facebook/bart-large-cnn`, on the SAMSum dataset for summerization\n",
        "* push the finetune model to HuggingFace model hub\n",
        "* load the finetuned model from hub for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Description\n",
        "\n",
        "The **[SAMSum](https://huggingface.co/datasets/samsum) dataset** contains about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English. Linguists were asked to create conversations similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger convesations. The style and register are diversified - conversations could be informal, semi-formal or formal, they may contain slang words, emoticons and typos. Then, **the conversations were annotated with summaries**. It was assumed that summaries should be a concise brief of what people talked about in the conversation in third person. The SAMSum dataset was prepared by Samsung R&D Institute Poland and is distributed for research purposes.\n",
        "\n",
        "Data Splits:\n",
        "- train: 14732\n",
        "- val: 818\n",
        "- test: 819\n",
        "\n",
        "Data Fields:\n",
        "\n",
        "- ***dialogue***: text of dialogue\n",
        "- ***summary***: human written summary of the dialogue\n",
        "- ***id***: unique id of an example\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example:**\n",
        "\n",
        "\\{\n",
        "> '**id**': '13818513',\n",
        "\n",
        ">'**summary**': 'Amanda baked cookies and will bring Jerry some tomorrow.',\n",
        "\n",
        ">'**dialogue**': \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n",
        "\n",
        "\\}"
      ],
      "metadata": {
        "id": "cD1T_p2o7uCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information"
      ],
      "metadata": {
        "id": "kC9XiC6N_u95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarization** creates a shorter version of a document or an article that captures all the important information. Along with translation, it is another example of a task that can be formulated as a sequence-to-sequence task."
      ],
      "metadata": {
        "id": "Dv5eqO4F-eDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BART** is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.\n",
        "\n",
        "BART is pre-trained by\n",
        "1. corrupting text with an arbitrary noising function, and\n",
        "2. learning a model to reconstruct the original text.\n",
        "\n",
        "BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint, `facebook/bart-large-cnn`, has been fine-tuned on CNN Daily Mail dataset, a large collection of text-summary pairs.\n",
        "\n",
        "To know more about BART `facebook/bart-large-cnn`, refer to its Model card [here](https://huggingface.co/facebook/bart-large-cnn)."
      ],
      "metadata": {
        "id": "wE19XaH0_NHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install required dependencies"
      ],
      "metadata": {
        "id": "v3I0DFzPlWde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install \"transformers==4.27.2\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.40.2\" loralib --upgrade --quiet\n",
        "# install additional dependencies needed for training"
      ],
      "metadata": {
        "id": "Jqz2L6ahMwxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4xsOx2Ku-Lg"
      },
      "outputs": [],
      "source": [
        "# HuggingFace transformers and datasets\n",
        "!pip -q install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'Accelerate' is the backend for the PyTorch side\n",
        "#  It enables the PyTorch code to be run across any distributed configuration\n",
        "!pip -q install accelerate -U\n",
        "\n",
        "\n",
        "# To install both 'transformer' and 'accelerate' in one go\n",
        "# !pip install transformers[torch]"
      ],
      "metadata": {
        "id": "N_145MFnd6g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A dependecy required for loading SAMSum dataset\n",
        "!pip -q install py7zr"
      ],
      "metadata": {
        "id": "AJS6CIYyKz00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers"
      ],
      "metadata": {
        "id": "TmwvYmIE341j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import required packages"
      ],
      "metadata": {
        "id": "YyOOHKn8Dyid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "6ZsYQy_zD2UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load Model & Tokenizer**"
      ],
      "metadata": {
        "id": "LuWSBF_hLBlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Load the model and tokenizer from HF Model Hub for finetuning**\n",
        "\n",
        "    - In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the `from_pretrained()` method. **AutoClasses** can be used to automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.\n",
        "\n",
        "    - Instantiating one of `AutoConfig`, `AutoModel`, and `AutoTokenizer` will directly create a class of the relevant architecture.\n",
        "\n",
        "    - `AutoModelForSeq2SeqLM` instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration.\n",
        "\n",
        "    - Full path of the model repo needs to be specified i.e. ***''USER-NAME/REPO-NAME''*** while calling `from_pretrained()` method."
      ],
      "metadata": {
        "id": "eP1Ss8v3oynD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model from HF Model Hub\n",
        "\n",
        "\"\"\"\n",
        "BART HAS 400M PARAMS: https://github.com/facebookresearch/fairseq/tree/main/examples/bart\n",
        "Look into Model card - 400 Million parameters\n",
        "\"\"\"\n",
        "\n",
        "checkpoint = \"facebook/bart-large-cnn\"                # username/repo-name\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "L6CJaWfDDNid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load Dataset**"
      ],
      "metadata": {
        "id": "QRgFEAQ2LFcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SAMSum dataset\n",
        "dataset = load_dataset(\"samsum\", trust_remote_code=True)\n",
        "dataset"
      ],
      "metadata": {
        "id": "RdqUVIY4MhHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Testing the pre-trained model**\n",
        "\n",
        "#### Observing the data"
      ],
      "metadata": {
        "id": "4m3Oda8Qluxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset['test'][0]['dialogue']\n",
        "label = dataset['test'][0]['summary']\n",
        "print(sample,'\\n','--------------')\n",
        "print(label)"
      ],
      "metadata": {
        "id": "hG1UGA7XQopn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompt Preparation"
      ],
      "metadata": {
        "id": "amGYwYh_RMeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(input, llm):\n",
        "    \"\"\"Prepare prompt  -->  tokenize -->  generate output using LLM  -->  detokenize output\"\"\"\n",
        "\n",
        "    input_prompt = f\"\"\"\n",
        "                    Summarize the following conversation.\n",
        "\n",
        "                    {input}\n",
        "\n",
        "                    Summary:\n",
        "                    \"\"\"\n",
        "\n",
        "    input_ids = tokenizer(input_prompt, return_tensors='pt')\n",
        "    tokenized_output = llm.generate(input_ids=input_ids['input_ids'], min_length=30, max_length=200)\n",
        "    output = tokenizer.decode(tokenized_output[0], skip_special_tokens=True)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "yNnm7q3XKAGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Getting the output"
      ],
      "metadata": {
        "id": "VCD3_iFXReUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = generate_summary(sample, llm=model)\n",
        "print(\"Sample\")\n",
        "print(sample)\n",
        "print(\"-------------------\")\n",
        "print(\"Model Generated Summary:\")\n",
        "print(output)\n",
        "print(\"Correct Summary:\")\n",
        "print(label)"
      ],
      "metadata": {
        "id": "ZUhONXZz7gi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prepare Dataset**"
      ],
      "metadata": {
        "id": "FjsuPUR0Or44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to prepare dataset\n",
        "\n",
        "def tokenize_inputs(example):\n",
        "\n",
        "    start_prompt = \"Summarize the following conversation.\\n\\n\"\n",
        "    end_prompt = \"\\n\\nSummary: \"\n",
        "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example['dialogue']]\n",
        "    example['input_ids'] = tokenizer(prompt, padding='max_length', truncation=True, max_length=512, return_tensors='pt').input_ids             # 'pt' for pytorch tensor\n",
        "    example['labels'] = tokenizer(example['summary'], padding='max_length', truncation=True, max_length=512, return_tensors='pt').input_ids\n",
        "\n",
        "    return example"
      ],
      "metadata": {
        "id": "8vKjUuiNGTwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below code, we are using `batched=True` to use Fast tokenizer implementation.\n",
        "\n",
        "**Slow** tokenizers are those written in Python inside the HF Transformers library, while the **fast** versions are the ones provided by HF Tokenizers, which are written in Rust.\n",
        "\n",
        "To know more about 'slow' and 'fast' tokenizers, refer [here](https://huggingface.co/learn/nlp-course/chapter6/3?fw=pt)"
      ],
      "metadata": {
        "id": "TKdfSLpAfuGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenized_datasets = dataset.map(tokenize_inputs, batched=True)       # using batched=True for Fast tokenizer implementation\n",
        "\n",
        "# Remove columns/keys that are not needed further\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'dialogue', 'summary'])"
      ],
      "metadata": {
        "id": "5sZvueW-yWS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shortening the data: Just picking row index divisible by 100\n",
        "# For learning purpose! It will reduce the compute resource requirement and training time\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
      ],
      "metadata": {
        "id": "TSdFD683hRoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_datasets['train'].shape)\n",
        "print(tokenized_datasets['validation'].shape)\n",
        "print(tokenized_datasets['test'].shape)"
      ],
      "metadata": {
        "id": "R-yNTg6ZMA4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets['train'][0].keys()"
      ],
      "metadata": {
        "id": "liKwC2ahNBzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Define Training Arguments and Trainer object**"
      ],
      "metadata": {
        "id": "XmRbEVkPl8RG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To upload the finetuned model on HF Model Hub, first you need to create a HuggingFace Account and Create a new model repository and Access Tokens with read/write permission**\n",
        "* [Sign up](https://huggingface.co/join) for a Hugging Face account\n",
        "    \n",
        "        * Follow the below steps to create reposotory\n",
        "    \n",
        "            - By going through your icon on huggingface you will find new model.\n",
        "            - Create your Model name, with License as ( MIT/mit ), keep it public and create model.\n",
        "            - You can access your folder location from the browser URL : https://huggingface.co/[YOUR-USER-NAME]/[YOUR-MODEL-REPO-NAME]\n",
        "            - With your user name and model repo name in training arguments uncomment and rename them `\"sumanthk/PEFT_expo\"`\n",
        "\n"
      ],
      "metadata": {
        "id": "lEYGFyJPeO7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bart-cnn-samsum-finetuned\",        # local directory\n",
        "    hub_model_id=\"sumanthk/PEFT_expo\",      # identifier on the Hub for directly pushing to HFhub model\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    auto_find_batch_size=True,\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_steps=10,\n",
        ")"
      ],
      "metadata": {
        "id": "7g8clKDvGFOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GenerationConfig"
      ],
      "metadata": {
        "id": "y3NUqFSb1isO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure generation settings\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    max_length=142,  # Maximum length of generated sequences\n",
        "    min_length=56,  # Minimum length of generated sequences\n",
        "    early_stopping=True,  # Stop generation early if all beams reach an EOS token\n",
        "    num_beams=4,  # Number of beams for beam search\n",
        "    length_penalty=2.0,  # Penalty for longer sequences\n",
        "    no_repeat_ngram_size=3,  # Prevent repeating n-grams\n",
        "    forced_bos_token_id=0,  # Force the beginning of sequence token\n",
        "    forced_eos_token_id=2,  # Force the end of sequence token\n",
        ")\n",
        "\n",
        "model.generation_config.decoder_start_token_id = tokenizer.cls_token_id"
      ],
      "metadata": {
        "id": "-siDjU7oz4OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,           # model to be finetuned\n",
        "    tokenizer=tokenizer,       # tokenizer to use\n",
        "    args=training_args,        # training arguments such as epochs, learning_rate, etc\n",
        "    train_dataset=tokenized_datasets['train'],         # training data to use\n",
        "    eval_dataset=tokenized_datasets['validation'],     # validation data to use\n",
        ")"
      ],
      "metadata": {
        "id": "bHRhbixLiJzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disabling Weights and Biases logging\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "MQ5ITvJ4xxe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "x6W1tjL3-cuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Save the model on Local system**"
      ],
      "metadata": {
        "id": "ufScp1JfDMmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ver = 1\n",
        "output_directory=\"./bart-cnn-samsum-finetuned\"\n",
        "model_path = os.path.join(output_directory, f\"tuned_model_{ver}\" )\n",
        "\n",
        "# Save finetuned model\n",
        "trainer.save_model(model_path)\n",
        "\n",
        "# Save associated tokenizer\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n",
        "print(f\"\\nSaved at path: {model_path}\")"
      ],
      "metadata": {
        "id": "Ax_KTG8QCA1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load the model from Local system and test**"
      ],
      "metadata": {
        "id": "15EydLoj5azx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model4local = AutoModelForSeq2SeqLM.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "ydq4Uvs55-gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = generate_summary(sample, llm = model4local)\n",
        "\n",
        "print(\"Sample\")\n",
        "print(sample)\n",
        "print(\"-------------------\")\n",
        "print(\"Summary:\")\n",
        "print(output)\n",
        "print(\"Ground Truth Summary:\")\n",
        "print(label)"
      ],
      "metadata": {
        "id": "q7UesjT_ABeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Push your model to Hugging Face Model Hub**\n",
        "\n",
        "To upload the finetuned model on HF Model Hub, please follow below steps:"
      ],
      "metadata": {
        "id": "YCujYAd41jit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps to push your fine-tuned model to HuggingFace Model Hub:**\n",
        "\n",
        "1. Go to already signed up Hugging Face account\n",
        "\n",
        "2. Create an access token for your account and save it\n",
        "\n",
        "    To create an access token:\n",
        "    \n",
        "        - Go to your `Settings`, then click on the `Access Tokens` tab. Click on the `New token` button to create a new User Access Token.\n",
        "        - Select a role(`write`) and a name for your token\n",
        "        - Click Generate a token\n",
        "\n",
        "    To know more about Access Tokens, refer [here](https://huggingface.co/docs/hub/security-tokens).\n"
      ],
      "metadata": {
        "id": "Io_OtwO6VZp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Once you have your User Access Token, run the following command to authenticate your identity to the Hub.\n",
        "        - `notebook_login()`\n",
        "        - Paste your Access token when prompted\n",
        "    \n",
        "    For more details on login, refer [here](https://huggingface.co/docs/huggingface_hub/quick-start#login)."
      ],
      "metadata": {
        "id": "AWfvdeX4WSaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "KFMzjRgTJlGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Push your fine-tuned model and tokenizer to Model Hub\n",
        "        - Use `push_to_hub()` method of your model and tokenizer both, to push them on hub"
      ],
      "metadata": {
        "id": "_vnzwf7tVySB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "4F-C1cNYJyM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test your finetuned model downloaded from HF Model Hub**"
      ],
      "metadata": {
        "id": "Tur7rNhTPVwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Specify user name and your repository where the model and tokenizer will be loaded from.\n",
        "    "
      ],
      "metadata": {
        "id": "dAoQ9DFrb8y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "username = \"sumanthk\"      # change it to your HuggingFace username\n",
        "\n",
        "checkpoint = username + '/PEFT_expo'  # change it to your Repo name\n",
        "\n",
        "loaded_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "lPAObv_8Eb-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = generate_summary(sample, llm=loaded_model)\n",
        "\n",
        "print(\"Sample\")\n",
        "print(sample)\n",
        "print(\"-------------------\")\n",
        "print(\"Summary:\")\n",
        "print(output)\n",
        "print(\"Ground Truth Summary:\")\n",
        "print(label)"
      ],
      "metadata": {
        "id": "qDy-EkbrEaZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "\n",
        "- [Summarization](https://huggingface.co/docs/transformers/tasks/summarization)"
      ],
      "metadata": {
        "id": "dw7C6gODdgk0"
      }
    }
  ]
}