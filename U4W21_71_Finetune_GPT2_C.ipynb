{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakhanrajpatlolla/aiml-learning/blob/master/U4W21_71_Finetune_GPT2_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaHNxUntdN6a"
      },
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* load and pre-process data from text file\n",
        "* load and use a pre-trained tokenizer\n",
        "* finetune a GPT-2 language model from Hugging Face's `transformers` library\n",
        "* push the finetuned model to HuggingFace model hub\n",
        "* load the finetuned model from hub for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Description\n",
        "\n",
        "The text data file is taken from The International Gita Society's eBook named \"***BHAGAVAD-GITA*** Author: Sage Veda Vyasa\", Translated in English by Ramananda Prasad, refer [here](https://www.gita-society.com/Read-bhagavad-gita.html).\n",
        "\n",
        "It contains:\n",
        "\n",
        "* the concept of duty and the moral implications of one's actions\n",
        "\n",
        "* the importance of performing one's duty without attachment to the results\n",
        "\n",
        "* various teachings, including the importance of performing one's duty according to one's station in life, the nature of the self, and the ultimate purpose of life\n",
        "\n",
        "* guidance on how to live a righteous life, manage one's emotions, and make ethical decisions\n",
        "\n",
        "* insights into achieving spiritual enlightenment and understanding one's true nature beyond the physical body\n",
        "\n",
        "The text data is inside **`document.pdf`** and will be downloaded once the below setup cells are executed."
      ],
      "metadata": {
        "id": "2nmP8OaXuO--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GPT-2**\n",
        "\n",
        "In recent years, the OpenAI GPT-2 exhibited an impressive ability to write coherent and passionate essays that exceeded what current language models can produce. The GPT-2 wasn't a particularly novel architecture - its architecture is very similar to the **decoder-only transformer**. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset.\n",
        "\n",
        "Here, we are going to fine-tune the GPT2 model with the text of International Gita Society's eBook - BHAGAVAD-GITA. We can expect that the model will be able to reply to the prompt related to the subject matter of this book after fine-tuning.\n",
        "\n",
        "To know more about GPT-2, refer [here](http://jalammar.github.io/illustrated-gpt2/)."
      ],
      "metadata": {
        "id": "hK3ZixaWfhHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Dependencies"
      ],
      "metadata": {
        "id": "dIdI8V0_edy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# For loading models, tokenizers, and datasets from HuggingFace\n",
        "!pip -q uninstall pyarrow -y\n",
        "!pip -q install pyarrow==15.0.2\n",
        "!pip -q install datasets\n",
        "!pip -q install accelerate\n",
        "!pip -q install transformers\n",
        "\n",
        "# For reading text from PDF files\n",
        "!pip -q install PyPDF2"
      ],
      "metadata": {
        "id": "C2ZtQKF5edGU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy pandas datasets torch transformers PyPDF2 -y\n",
        "!pip install numpy==1.26.4 pandas==2.2.2 datasets==2.20.0 torch==2.4.1 transformers==4.44.2 PyPDF2==3.0.1\n",
        "import os\n",
        "os.kill(os.getpid(), 9)  # Forces a full restart of the runtime"
      ],
      "metadata": {
        "id": "6RswfwhICLqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"#990000\">Restart Session/Runtime</font>"
      ],
      "metadata": {
        "id": "E29JLGDPe_xa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2418775\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEzlYL4CDrmE"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"9959000490\" #@param {type:\"string\"}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form",
        "outputId": "1f671fd6-ea71-48ee-8638-1d082f85fe8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import re\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"U4W21_71_Finetune_GPT2_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    from IPython.display import HTML, display\n",
        "    ipython.magic(\"sx wget https://drive.google.com/uc?id=12jYBY0yqwNEErkqol06BtEQF3C-wSUBS -O document.pdf\")\n",
        "\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getWalkthrough() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook, \"feedback_walkthrough\":Walkthrough ,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_inclass_mentor\": Mentor_support}\n",
        "\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://learn-iiith.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getWalkthrough():\n",
        "  try:\n",
        "    if not Walkthrough:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Walkthrough\n",
        "  except NameError:\n",
        "    print (\"Please answer Walkthrough Question\")\n",
        "    return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2418775&recordId=2540\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RH8Ecq9sbYU"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import PyPDF2\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "JfqnHAnMYfWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the data"
      ],
      "metadata": {
        "id": "ChIKT30jKuo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is in a PDF file (.pdf)\n",
        "\n",
        "Create function to read pdf file:"
      ],
      "metadata": {
        "id": "a_Bn7925nhYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read document pdf files\n",
        "\n",
        "def read_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "\n",
        "    # Open the PDF file\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Iterate over each page\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            if page_num > 3:                         # extract text starting from page 5\n",
        "                page = reader.pages[page_num]\n",
        "                text += page.extract_text()\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "ryR44tMnhRMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read files/documents\n",
        "\n",
        "pdf_path = 'document.pdf'\n",
        "text_file = read_pdf(pdf_path)"
      ],
      "metadata": {
        "id": "DuiQD165hSp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_file[:8000])"
      ],
      "metadata": {
        "id": "ycLbDPgDg5_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing\n",
        "\n",
        "- Remove any excess newline characters from the text\n",
        "- Remove any excess spaces\n",
        "- Remove unnecessary words (Header & Page number)\n",
        "- Keep 100 words per line inside text"
      ],
      "metadata": {
        "id": "9ESwtGMaL5vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove excess newline characters\n",
        "text_file = re.sub(r'\\n+', '\\n', text_file).strip()"
      ],
      "metadata": {
        "id": "4tYqGszOL-9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove excess spaces\n",
        "text_file = re.sub(r' +', ' ', text_file).strip()"
      ],
      "metadata": {
        "id": "E4h2SRDCw17D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unnecessary words (Header & Page number)\n",
        "text_file = re.sub(r' \\d+ International Gita Society', '', text_file)\n",
        "text_file = re.sub(r' Bhagavad -Gita \\d+', '', text_file)"
      ],
      "metadata": {
        "id": "IfcxGAKc0S13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_file[:8000])"
      ],
      "metadata": {
        "id": "ZleqLz-jdYU2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Keep 100 words per line inside text"
      ],
      "metadata": {
        "id": "K2jVl0QMJ6VW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_list = []\n",
        "new_text_file = ''\n",
        "\n",
        "for line in text_file.split('\\n'):\n",
        "    words = line.split()\n",
        "    for word in words:\n",
        "        word_list.append(word)\n",
        "        if len(word_list) == 100:\n",
        "            new_text_file += ' '.join(word_list) + '\\n'\n",
        "            word_list = []\n",
        "\n",
        "if word_list:\n",
        "    new_text_file += ' '.join(word_list) + '\\n'\n"
      ],
      "metadata": {
        "id": "FvWru40_F0Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_text_file[:8000])"
      ],
      "metadata": {
        "id": "FURAL8FxF0Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(new_text_file.split('\\n')[0].split())"
      ],
      "metadata": {
        "id": "Egfuqi6-Hf2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the text into training and validation sets"
      ],
      "metadata": {
        "id": "DUO4loorLsA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text into training and validation sets\n",
        "\n",
        "train_fraction = 0.8\n",
        "split_index = int(train_fraction * len(new_text_file))\n",
        "\n",
        "train_text = new_text_file[:split_index]\n",
        "val_text = new_text_file[split_index:]"
      ],
      "metadata": {
        "id": "Au9dHz2IcOLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_text)"
      ],
      "metadata": {
        "id": "_7YIqYzIawcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the training and validation data as text files\n",
        "\n",
        "with open(\"train.txt\", \"w\") as f:\n",
        "    f.write(train_text)\n",
        "\n",
        "with open(\"val.txt\", \"w\") as f:\n",
        "    f.write(val_text)"
      ],
      "metadata": {
        "id": "ABHKmK7ZcUlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load pre-trained tokenizer - GP2Tokenizer"
      ],
      "metadata": {
        "id": "Y1hlYtjjML1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GPT2Tokenizer is based on ***Byte-Pair-Encoding***.\n",
        "\n",
        "Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model.\n",
        "\n",
        "In BPE, new tokens are added until the desired vocabulary size is reached by learning ***merges***, which are rules to merge two elements of the existing vocabulary together into a new one.\n",
        "\n",
        "Below figure shows how the vocabulary updates as the BPE algorithm progresses.\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/Byte-pair-encoding.png\" width=450px>\n",
        "</center>\n",
        "\n",
        "To know more about Byte-Pair Encoding, refer [here](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt#byte-pair-encoding-tokenization).\n",
        "\n",
        "<br>\n",
        "\n",
        "Some of the parameters required to create a GP2Tokenizer includes:\n",
        "\n",
        "- ***vocab_file (str):*** path to the vocabulary json file; maps token to integer ids\n",
        "\n",
        "- ***merges_file (str):*** path to the ***merges*** file; contains the merge rule; The merge rule file should have one merge rule per line. Every merge rule contains merge entities separated by a space.\n",
        "\n"
      ],
      "metadata": {
        "id": "6LIwiw1qnGIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will instantiate a GPT-2 tokenizer from a predefined tokenizer using `from_pretrained()` method.\n",
        "\n",
        "It includes a parameter:\n",
        "\n",
        "- ***pretrained_model_name_or_path:*** It can be a string of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
        "\n",
        "    For example: *gpt2, gpt2-medium, gpt2-large, or gpt2-xl*\n",
        "\n",
        "    This will download the corresponding vocab, merges, and config files."
      ],
      "metadata": {
        "id": "HXxBS2ShDIYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the tokenizer\n",
        "checkpoint = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)    # also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl\n",
        "\n",
        "# set pad_token_id to unk_token_id\n",
        "tokenizer.pad_token = tokenizer.unk_token"
      ],
      "metadata": {
        "id": "-qiMe9TAplyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize sample text using GP2Tokenizer\n",
        "sample_ids = tokenizer(\"Hello world\")\n",
        "sample_ids"
      ],
      "metadata": {
        "id": "_l2MihgfwEPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate tokens for sample text\n",
        "sample_tokens = tokenizer.convert_ids_to_tokens(sample_ids['input_ids'])\n",
        "sample_tokens"
      ],
      "metadata": {
        "id": "sXYqWSzLwaBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate original text back\n",
        "tokenizer.convert_tokens_to_string(sample_tokens)"
      ],
      "metadata": {
        "id": "sHuZKceCwNKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize text data"
      ],
      "metadata": {
        "id": "InX4FOvgP0mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "XwIjTYLjjWod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_path = 'train.txt'\n",
        "val_file_path = 'val.txt'\n",
        "\n",
        "dataset = load_dataset(\"text\", data_files={\"train\": train_file_path,\n",
        "                                           \"validation\": val_file_path})"
      ],
      "metadata": {
        "id": "T1tQt0spjWhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "pcyNf7_Wjan9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']['text'][0]"
      ],
      "metadata": {
        "id": "t5Eolw86jdW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 256     # max tokens in an input sample\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True, max_length=block_size, return_tensors='pt')\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "fe593JNEj8xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "u7hUV8OIkGbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_datasets['train']['input_ids'][0])"
      ],
      "metadata": {
        "id": "sotAhBQIks_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenized_datasets['train']['input_ids'][0])"
      ],
      "metadata": {
        "id": "hCUdQMcejalM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Collator\n",
        "\n",
        "Data collators are objects that:\n",
        "\n",
        "- will form a batch by using a list of dataset elements as input\n",
        "- may apply some processing (like padding)\n",
        "\n",
        "One of the data collators, `DataCollatorForLanguageModeling`, can also apply some random data augmentation (like random masking) on the formed batch.\n",
        "\n",
        "<br>\n",
        "\n",
        "`DataCollatorForLanguageModeling` is a data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they are not all of the same length.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "- ***tokenizer:*** The tokenizer used for encoding the data.\n",
        "- ***mlm*** (bool, optional, default=True): Whether or not to use masked language modeling.\n",
        "    - If set to False, the labels are the same as the inputs with the padding tokens ignored (by setting them to -100).\n",
        "    - Otherwise, the labels are -100 for non-masked tokens and the value to predict for the masked token.\n",
        "- ***return_tensors*** (str): The type of Tensor to return. Allowable values are “np”, “pt” and “tf” for numpy array, pytorch tensor, and tensorflow tensor respectively.\n",
        "\n",
        "To know more about `DataCollatorForLanguageModeling` parameters, refer [here](https://huggingface.co/docs/transformers/v4.32.0/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling)."
      ],
      "metadata": {
        "id": "itcSXbZMib_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Data collator object\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "Z_XWmIF3cmhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load pre-trained Model"
      ],
      "metadata": {
        "id": "uophCXjYq9MO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***GPT2LMHeadModel*** is the GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n",
        "\n",
        "This model is a PyTorch `torch.nn.Module` subclass which can be used as a regular PyTorch Module.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "- ***config (GPT2Config):*** Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration.\n",
        "\n",
        "Here, we will instantiate a pretrained pytorch model from a pre-trained model configuration, using `from_pretrained()` method, that will load the weights associated with the model."
      ],
      "metadata": {
        "id": "nuE_NdXuqvw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the model\n",
        "model = GPT2LMHeadModel.from_pretrained(checkpoint)    # also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl"
      ],
      "metadata": {
        "id": "HxQWgssCqy7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: The training time for different GPT models with GPU for this dataset are as follows:**\n",
        "\n",
        "* **GPT-2 : ~25 minutes for 100 epochs**\n",
        "\n",
        "* **GPT-2 Medium:  ~1 hour for 100 epochs**\n",
        "\n",
        "* **GPT-2 Large : Run out of memory**"
      ],
      "metadata": {
        "id": "pM13pdhzJY8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tune Model *(Switch to GPU runtime if needed)*"
      ],
      "metadata": {
        "id": "LdcpMx9QOPnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a GPT-2 model using the provided training arguments. Save the resulting trained model and tokenizer to a specified output directory."
      ],
      "metadata": {
        "id": "D1LqRO_Unfmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Trainer` class provides an API for feature-complete training in PyTorch for most standard use cases.\n",
        "\n",
        "Before instantiating your Trainer, create a `TrainingArguments` to access all the points of customization during training.\n",
        "\n",
        "`TrainingArguments` parameters:\n",
        "\n",
        "- ***output_dir*** (str): The output directory where the model predictions and checkpoints will be written.\n",
        "- ***overwrite_output_dir*** (bool, optional, default=False): If True, overwrite the content of the output directory. Use this to continue training if output_dir points to a checkpoint directory.\n",
        "- ***per_device_train_batch_size*** (int, optional, default=8): The batch size per GPU/TPU/MPS/NPU core/CPU for training.\n",
        "- ***per_device_eval_batch_size*** (int, optional, default=8): The batch size per GPU/TPU/MPS/NPU core/CPU for evaluation.\n",
        "- ***save_total_limit*** (int, optional): If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir.\n",
        "\n",
        "To know more about `TrainingArguments` parameters, refer [here](https://huggingface.co/docs/transformers/v4.32.0/en/main_classes/trainer#transformers.TrainingArguments).\n",
        "\n",
        "To know more about `Trainer` parameters, refer [here](https://huggingface.co/docs/transformers/v4.32.0/en/main_classes/trainer#transformers.Trainer)."
      ],
      "metadata": {
        "id": "J7pZfFvsopWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the training arguments\n",
        "\n",
        "model_output_path = \"/content/gpt2_model\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = model_output_path,\n",
        "    overwrite_output_dir = True,\n",
        "    per_device_train_batch_size = 4, # try with 2\n",
        "    per_device_eval_batch_size = 4,  #  try with 2\n",
        "    num_train_epochs = 100,\n",
        "    save_steps = 1_000,\n",
        "    save_total_limit = 2,\n",
        "    logging_dir = './logs',\n",
        "    )"
      ],
      "metadata": {
        "id": "UPZiEvn2cuPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = tokenized_datasets[\"train\"],\n",
        "    eval_dataset = tokenized_datasets[\"validation\"],\n",
        ")"
      ],
      "metadata": {
        "id": "UQe5YHVKlfsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disabling Weights and Biases logging\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "ZnMxHb-f9o3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "BUlxoIvFN4ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "saved_model_path = \"/content/finetuned_gpt2_model\"\n",
        "trainer.save_model(saved_model_path)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(saved_model_path)"
      ],
      "metadata": {
        "id": "UX-EmL_dc_H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Model with user input prompts"
      ],
      "metadata": {
        "id": "4izo_-go8cDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Now, let us test the model with some prompt\n"
      ],
      "metadata": {
        "id": "m0JzfOFRcUDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `generate_response()` function takes a trained *model*, *tokenizer*, and a *prompt* string as input and generates a response using the GPT-2 model."
      ],
      "metadata": {
        "id": "W8WnPwpHnz57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(model, tokenizer, prompt, max_length=200):\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")      # 'pt' for returning pytorch tensor\n",
        "\n",
        "    # Check the device of the model\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Move input_ids to the same device as the model\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    # Create the attention mask and pad token id\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=pad_token_id\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "qeTKPArfgDJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "\n",
        "my_model = GPT2LMHeadModel.from_pretrained(saved_model_path)\n",
        "my_tokenizer = GPT2Tokenizer.from_pretrained(saved_model_path)"
      ],
      "metadata": {
        "id": "pJJ3bzD9fsJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "prompt = \"How can one live a righteous life?\"           # Replace with your desired prompt\n",
        "response = generate_response(model, tokenizer, prompt)\n",
        "print(\"Generated response:\")\n",
        "response"
      ],
      "metadata": {
        "id": "hHRnPkIuWeCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "prompt = \"What is the purpose of life?\"           # Replace with your desired prompt\n",
        "response = generate_response(model, tokenizer, prompt)\n",
        "print(\"Generated response:\")\n",
        "response"
      ],
      "metadata": {
        "id": "ULUD4riu8i3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "prompt = \"What is Karma?\"          # Replace with your desired prompt\n",
        "response = generate_response(model, tokenizer, prompt, max_length=150)\n",
        "print(\"Generated response:\")\n",
        "response"
      ],
      "metadata": {
        "id": "DBlBeSY7gDyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "prompt = \"How to overcome dilemma or fear?\"          # Replace with your desired prompt\n",
        "response = generate_response(model, tokenizer, prompt, max_length=150)\n",
        "print(\"Generated response:\")\n",
        "response"
      ],
      "metadata": {
        "id": "8kqiUQz3Xi02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "prompt = \"How to control emotions during tough times?\"          # Replace with your desired prompt\n",
        "response = generate_response(model, tokenizer, prompt, max_length=150)\n",
        "print(\"Generated response:\")\n",
        "response"
      ],
      "metadata": {
        "id": "F_Li_5L9YPG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "prompt = \"Is there a way to achieve enlightenment?\"          # Replace with your desired prompt\n",
        "response = generate_response(model, tokenizer, prompt, max_length=150)\n",
        "print(\"Generated response:\")\n",
        "response"
      ],
      "metadata": {
        "id": "xMgWq9jdYiQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of the GPT-2 tokenizer, the model uses a byte-pair encoding (BPE) algorithm, which tokenizes text into subword units. As a result, one word might be represented by multiple tokens.\n",
        "\n",
        "For example, if you set max_length to 50, the generated response will be limited to 50 tokens, which could be fewer than 50 words, depending on the text."
      ],
      "metadata": {
        "id": "XHcQBzdJeB27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push your fine-tuned model to HuggingFace Model Hub"
      ],
      "metadata": {
        "id": "Pzz2iQ3BcLXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps to push your fine-tuned model to HuggingFace Model Hub**\n",
        "\n",
        "1. [Sign up](https://huggingface.co/join) for a Hugging Face account\n",
        "2. Create an access token for your account and save it\n",
        "3. Store your access token in the Hugging Face cache folder within colab\n",
        "4. Push your fine-tuned model and tokenizer to Model Hub\n",
        "5. Load the model back from Hub and test it with user input prompts"
      ],
      "metadata": {
        "id": "4vbWDawDPDby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Create an access token for your account**\n",
        "\n",
        "    Once you have an account, to create an access token:\n",
        "    \n",
        "    - Go to your `Settings`, then click on the `Access Tokens` tab. Click on the `New token` button to create a new User Access Token.\n",
        "    - Select a Token type as `Write` and give a name for your token\n",
        "    - Click on Create token\n",
        "    - Once a token is created save it somewhere\n",
        "    - When required later, use the old saved token or create a new token again\n",
        "\n",
        "    To know more about Access Tokens, refer [here](https://huggingface.co/docs/hub/security-tokens)."
      ],
      "metadata": {
        "id": "6R39dW0-VSsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Store your access token in the Hugging Face cache folder within colab**\n",
        "\n",
        "    Once you have your User Access Token, run the following command to authenticate your identity to the Hub.\n",
        "    - `!huggingface-cli login`\n",
        "    - Paste your Access token when prompted\n",
        "    - Type **n** when prompted to Add token as git credential? (Y/n)\n",
        "\n",
        "    For more details on login, refer [here](https://huggingface.co/docs/huggingface_hub/quick-start#login)."
      ],
      "metadata": {
        "id": "0qlXt3xcbT3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "CjgJ-czwhf0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Push your fine-tuned model and tokenizer to Model Hub**\n",
        "\n",
        "    - Use `push_to_hub()` method of your model and tokenizer both, to push them on hub\n",
        "    - Specify name for your repository where the model and tokenizer will be pushed using `repo_id` parameter\n",
        "    - Push model and tokenizer to the same repository\n",
        "\n",
        "        - Use `push_to_hub()` method of your model. For parameter details, refer [here](https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.push_to_hub).\n",
        "        - Use `push_to_hub()` method of your tokenizer. For parameter details, refer [here](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.push_to_hub).\n",
        "        - Access your pushed model at `https://huggingface.co/[YOUR-USER-NAME]/[YOUR-MODEL-REPO-NAME]/tree/main`"
      ],
      "metadata": {
        "id": "lEYGFyJPeO7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Push model\n",
        "my_repo = \"gita-text-generation-gpt2\"\n",
        "model.push_to_hub(repo_id= my_repo, commit_message= \"Upload fine-tuned model\")"
      ],
      "metadata": {
        "id": "ROyD_J42ls8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push tokenizer\n",
        "tokenizer.push_to_hub(repo_id= my_repo, commit_message= \"Upload tokenizer used\")"
      ],
      "metadata": {
        "id": "qm7WIMgymxPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access your pushed model at `https://huggingface.co/[YOUR-USER-NAME]/[YOUR-MODEL-REPO-NAME]/tree/main`\n",
        "\n",
        "For example: https://huggingface.co/sumanthk/gita-text-generation-gpt2/tree/main"
      ],
      "metadata": {
        "id": "tfh2CEI7vmM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Load the model and tokenizer back from Hub and test it with user input prompts**\n",
        "\n",
        "    - In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the `from_pretrained()` method. **AutoClasses** can be used to automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.\n",
        "\n",
        "    - Instantiating one of `AutoConfig`, `AutoModel`, and `AutoTokenizer` will directly create a class of the relevant architecture.\n",
        "\n",
        "    - When the GPT2 Model transformer has a language modeling head on top, you can use an auto class with language modeling head on top as well - `AutoModelWithLMHead`.\n",
        "\n",
        "    - Specify full path of your model repo i.e. ***''YOUR-USER-NAME/YOUR-REPO-NAME''*** while calling `from_pretrained()` method."
      ],
      "metadata": {
        "id": "eP1Ss8v3oynD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer"
      ],
      "metadata": {
        "id": "KWCT1lR9nP8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "username = \"sumanthk\"      # change it to your HuggingFace username\n",
        "\n",
        "my_checkpoint = username + '/' + my_repo       # eg. \"yrajm1997/gita-text-generation-gpt2\"\n",
        "my_checkpoint"
      ],
      "metadata": {
        "id": "dH9iW4rkeyKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your model from hub\n",
        "loaded_model = AutoModelWithLMHead.from_pretrained(my_checkpoint)"
      ],
      "metadata": {
        "id": "AtP-5fBzIF-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your tokenizer from hub\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(my_checkpoint)"
      ],
      "metadata": {
        "id": "5mtKza_LIOOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "prompt = \"How can one live a righteous life?\"           # Replace with your desired prompt\n",
        "response = generate_response(loaded_model, loaded_tokenizer, prompt)\n",
        "print(\"Generated response:\")\n",
        "response"
      ],
      "metadata": {
        "id": "UwiKrOXEfZDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-b-CEcpkOCi"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12SwfIaukOCj"
      },
      "source": [
        "#@title The architecture of GPT is very similar to: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"the decoder-only transformer\" #@param [\"\", \"the encoder-only transformer\", \"the decoder-only transformer\", \"the encoder-decoder transformer\", \"none of the above\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0DzTLAMkOCj"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlMms_IykOCj"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"good\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHnMPzf_kOCk"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRKla4emkOCk"
      },
      "source": [
        "#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Walkthrough = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtkER1AwkOCk"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-UhjrabkOCk"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MUHD7LC0kOCk",
        "outputId": "676dfa08-da13-4e3d-fc40-0b8671f324fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 2540\n",
            "Date of submission:  19 Apr 2025\n",
            "Time of submission:  15:17:40\n",
            "View your submissions: https://learn-iiith.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ]
}