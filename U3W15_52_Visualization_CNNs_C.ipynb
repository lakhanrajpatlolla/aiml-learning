{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakhanrajpatlolla/aiml-learning/blob/master/U3W15_52_Visualization_CNNs_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdCnz388ED4A"
      },
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QxI_medbLHO"
      },
      "source": [
        "### Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXJ98oYNbNXj"
      },
      "source": [
        "At the end of the experiment, you will be able to :\n",
        "\n",
        "1. Visualize the learned representations at each layer from the network using below methods:\n",
        "\n",
        " a. Apply Matthew D Zeiler, Rob Fergus approach to visualize CNNs\n",
        "\n",
        " b. Apply  Mahendran and Vedaldi approach to visualize CNNs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jll8vCv2sV5Y"
      },
      "outputs": [],
      "source": [
        "#@title Experiment Explanation Video\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"<video width=\"800\" height=\"400\" controls>\n",
        "  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/module_3_week_11_experiment_4.mp4\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYa2yoAMbYSl"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcmpUGOSbZ6Q"
      },
      "source": [
        "#### History\n",
        "\n",
        "#### ImageNet\n",
        "The ImageNet project is a large image database used for research purposes. It contains more than 14 million images belonging to the 20 thousand different categories. For example categories like 'balloon' or 'apple' contains several hundred images.\n",
        "\n",
        "This database was presented for the first time as a poster at the 2009 Conference on Computer Vision and Pattern Recognition (CVPR) in Florida by researchers from the Computer Science department at Princeton University.\n",
        "\n",
        "An interesting 2012 breakthrough in solving the ImageNet Challenge is widely considered to be the beginning of the deep learning revolution of the 2010s:\n",
        "\n",
        "#### VGG\n",
        "\n",
        "VGG  is a pre-trained model on imagenet which was developed by Visual Geometry Group, the University of Oxford for the ILSVRC-2014. They released two different CNN models, specifically a 16-layer model and a 19-layer model.  VGG16 model consists of 16 convolutional layers and is very appealing because of its very uniform architecture. This currently the most preferred choice in the community for extracting features from images. The weight configuration of the VGGNet is publicly available and has been used in many other applications and challenges as a baseline feature extractor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg9Eat8lbg2K"
      },
      "source": [
        "#### Description\n",
        "\n",
        "We have seen how to build the CNN models from previous experiments. Whereas, in this experiment we will visualize the learned weights at each layer of the VGG16 model.\n",
        "\n",
        "The input to the network is a dog image which as 728 px height, 300 px width, and 3 channels of color(RGB). The learned representations from the trained VGG16 model for this image are visualized using various approaches at each layer.\n",
        "\n",
        "![alt text](https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/dog.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo6WlZ9Fbin2"
      },
      "source": [
        "### AI/ML Technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu5ZwbF5bmHX"
      },
      "source": [
        "In this experiment we will perform CNN's visualization at each layer using these below two  approaches and VGG 16 trained model :\n",
        "\n",
        "*    Matthew D Zeiler and Rob Fergus Approach\n",
        "*    Mahendran and Vedaldi Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbCetQo7bpfd"
      },
      "source": [
        "#### Matthew D Zeiler and Rob Fergus Approach\n",
        "Recently, visualizing deep neural networks in particularly CNNs has resulted in many publications.\n",
        "\n",
        "The recent research or publications are base on the paper published by Matthew D Zeiler and Rob Fergus. In this, they proposed a multi-layered deconvolutional network (deconvnet) to project the feature activations back to the input pixel space for a   trained network. This reconstruction process makes use of extra information about locations of maxima in intermediate max-pooling layers.\n",
        "\n",
        "To read more about this refer to the paper of [Matthew D Zeiler, Rob Fergus](https://arxiv.org/abs/1311.2901)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKqG-aEwbr7N"
      },
      "source": [
        "#### Mahendran and Vedaldi Approach\n",
        "\n",
        "In this approach, we try to understand CNNs by inverting them using Gradient Descent. This kind of inversions allows us to know which information of the given input image is preserved in the features.\n",
        "\n",
        "\n",
        "To read more about this refer to the paper of   [Mahendran and Vedaldi](https://arxiv.org/abs/1412.0035)\n",
        "\n",
        "\n",
        "While working on the experiment you will able to understand these two approaches in a detailed way and differences between them.\n",
        "\n",
        "&nbsp;&nbsp;\n",
        "\n",
        "In this experiment we will perform the following steps:\n",
        "    \n",
        "    1. Load the model and see its architecture\n",
        "    2. Load and preprocess an image to pass as input to the network\n",
        "    3. Visualize the kernel weights at each layer\n",
        "    4. Visualize the image as it passes through the network\n",
        "    5. Visualize the output of each filter at a given layer\n",
        "    6. Understand deep image representations by inverting Them [Mahendran, Vedaldi]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEzlYL4CDrmE"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import re\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"U3W15_52_Visualization_CNNs_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    from IPython.display import HTML, display\n",
        "    ipython.magic(\"sx pip3 install torch\")\n",
        "    ipython.magic(\"sx pip3 install torchvision\")\n",
        "    ipython.magic(\"sx pip install Pillow\")\n",
        "    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week9/Exp4/dog.jpg\")\n",
        "    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week9/Exp4/cnn_utils.py\")\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getWalkthrough() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook, \"feedback_walkthrough\":Walkthrough ,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_inclass_mentor\": Mentor_support}\n",
        "\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://learn-iiith.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getWalkthrough():\n",
        "  try:\n",
        "    if not Walkthrough:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Walkthrough\n",
        "  except NameError:\n",
        "    print (\"Please answer Walkthrough Question\")\n",
        "    return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pbvhU58VTca"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9s-pePFED4K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmPeN1sZED4I"
      },
      "source": [
        "### 1. Load the model and see its architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pWM4XDYVP7C"
      },
      "outputs": [],
      "source": [
        "model = models.vgg16(pretrained=True)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TzExHEzED4T"
      },
      "source": [
        "#### We will load all the module details in a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1wEeMK5ED4U"
      },
      "outputs": [],
      "source": [
        "modules = list(model.features.modules())\n",
        "modules = modules[1:]\n",
        "print(modules, \"\\n\\n\")\n",
        "print(\"third module = \", modules[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaJRjWkTED4a"
      },
      "source": [
        "### 2. Load and preprocess an image to pass as input to the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8kcBiQDED4b"
      },
      "outputs": [],
      "source": [
        "def normalize(image):\n",
        "    normalize = transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406],\n",
        "    std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "    preprocess = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "    ])\n",
        "    image = Variable(preprocess(image).unsqueeze(0))\n",
        "    return image\n",
        "\n",
        "img_raw = Image.open(\"dog.jpg\")\n",
        "plt.imshow(img_raw)\n",
        "plt.title(\"Image loaded successfully\")\n",
        "\n",
        "img = normalize(img_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqIuGuwNED4h"
      },
      "source": [
        "### 3. Visualize the kernel weights at each layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8djbmV1UED4i"
      },
      "outputs": [],
      "source": [
        "def visualize_weights(image, layer):\n",
        "    weight_used = []\n",
        "\n",
        "    # Gather all Convolution layers and append their corresponding filters in a list\n",
        "    for w in model.features.children():\n",
        "        if isinstance(w, torch.nn.modules.conv.Conv2d):\n",
        "            weight_used.append(w.weight.data)\n",
        "\n",
        "    print(\"(#filters, i/p depth, size of filter) === \",weight_used[layer].shape)\n",
        "    print(\"No. of filters: \", weight_used[layer].shape[0])\n",
        "    filters = []\n",
        "    for i in range(weight_used[layer].shape[0]):\n",
        "        filters.append(weight_used[layer][i,:,:,:].sum(dim=0))    # Summing across input depth(3 in the first layer)\n",
        "        filters[i].div_(weight_used[layer].shape[1])\n",
        "\n",
        "    fig = plt.figure()\n",
        "    plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
        "    for i in range(int(np.sqrt(weight_used[layer].shape[0])) * int(np.sqrt(weight_used[layer].shape[0]))):\n",
        "        a = fig.add_subplot(int(np.sqrt(weight_used[layer].shape[0])),int(np.sqrt(weight_used[layer].shape[0])),i+1)\n",
        "        imgplot = plt.imshow(filters[i])\n",
        "        plt.axis('off')\n",
        "\n",
        "visualize_weights(img, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "838kbLzfED4n"
      },
      "source": [
        "The filters are usually small $(3X3)$ and hence the visualization of filter weights usually doesn't give us a clear understanding of the what the filters learn.\n",
        "\n",
        "Therefore, we will visualize how the the input image looks as it is passed through the various layers in the network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-gby1X_TKSE"
      },
      "source": [
        "### 4. Visualizing the image as it passes through the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H01mcx-ED4o"
      },
      "outputs": [],
      "source": [
        "def to_grayscale(image):\n",
        "    image = torch.mean(image, dim=0)\n",
        "    image = torch.div(image, image.shape[0])\n",
        "    return image\n",
        "\n",
        "def layer_outputs(image):\n",
        "    outputs = []\n",
        "    names = []\n",
        "\n",
        "    # Feed forward the image through the network and store the outputs\n",
        "    for layer in modules:\n",
        "        image = layer(image)\n",
        "        outputs.append(image)\n",
        "        names.append(str(layer))\n",
        "\n",
        "    # For visualization purposes, convert the output into a 2D image by averaging across the filters.\n",
        "    output_im = []\n",
        "    for i in outputs:\n",
        "        i = i.squeeze(0)\n",
        "        temp = to_grayscale(i)  # Convert say 64x112x112 to 112x112\n",
        "        output_im.append(temp.data.numpy())\n",
        "\n",
        "    fig = plt.figure()\n",
        "    plt.rcParams[\"figure.figsize\"] = (30, 40)\n",
        "\n",
        "    for i in range(len(output_im)):\n",
        "        a = fig.add_subplot(8,4,i+1)\n",
        "        imgplot = plt.imshow(output_im[i])\n",
        "        plt.axis('off')\n",
        "        a.set_title(str(i+1)+\". \"+names[i].partition('(')[0], fontsize=15)\n",
        "\n",
        "layer_outputs(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAoqS-B2ED4t"
      },
      "source": [
        "Through the above visualization, it is clearly visible how the CNN responds to an image at each layer and in the final layer, the pixels of the image which produce the highest activation values are visible as well. However, we still haven't looked at how the filters at each layer respond differently to features present in the input.\n",
        "\n",
        "### 5. Visualizing output of each filter at a given layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ixef3LvtED4u"
      },
      "outputs": [],
      "source": [
        "def filter_outputs(image, layer_to_visualize, num_filters):\n",
        "    if layer_to_visualize < 0:\n",
        "        layer_to_visualize += 31\n",
        "    output = None\n",
        "    name = None\n",
        "    # Image at each layer\n",
        "    # Get outputs corresponding to the mentioned layer\n",
        "    for count, layer in enumerate(modules):\n",
        "        image = layer(image)\n",
        "        if count == layer_to_visualize:\n",
        "            output = image\n",
        "            name = str(layer)\n",
        "            break\n",
        "\n",
        "    filters = []\n",
        "    output = output.data.squeeze()\n",
        "\n",
        "    # If num_filters==-1, visualize all the filters\n",
        "    num_filters = min(num_filters, output.shape[0])\n",
        "    if num_filters==-1:\n",
        "        num_filters = output.shape[0]\n",
        "\n",
        "    for i in range(num_filters):\n",
        "        filters.append(output[i,:,:])\n",
        "\n",
        "    fig = plt.figure()\n",
        "    plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
        "    print(\"No. of filters: \", len(filters))\n",
        "\n",
        "    for i in range(int(np.sqrt(len(filters))) * int(np.sqrt(len(filters)))):\n",
        "        fig.add_subplot(int(np.sqrt(len(filters))),int(np.sqrt(len(filters))),i+1)\n",
        "        imgplot = plt.imshow(filters[i])\n",
        "        plt.axis('off')\n",
        "\n",
        "# If num_filters==-1, visualize all the filters\n",
        "filter_outputs(img,0,16)    # Visualize the outputs of first 16 filters of the 1st layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45imeFMQED41"
      },
      "source": [
        "The above visualization shows that each filter responds differently to an input which implies that each filter learns and extracts different features from an input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N_hwnQNED42"
      },
      "source": [
        "### 6. Understanding Deep Image Representations by Inverting Them [Mahendran, Vedaldi]\n",
        "\n",
        "Like Zeiler and Fergus, their method starts from a specific input image. They record the network’s representation of that specific image and then reconstruct an image that produces a similar code. Thus, their method provides insight into what the activation of a whole layer represent, not what an individual neuron represents.\n",
        "\n",
        "They show what each neuron “wants to see”, and thus what each neuron has learned to look for.\n",
        "\n",
        "To visualize the function of a specific unit in a neural network, we $synthesize$ inputs that cause that unit to have high activation. To synthesize such a “preferred input example”, we start with a random image, meaning we randomly choose a color for each pixel. The image will initially look like colored TV static."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFKA0TDVED43"
      },
      "outputs": [],
      "source": [
        "random_noise_img = Variable(1e-1 * torch.randn(1, 3, 224, 224), requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a83K-4sED47"
      },
      "source": [
        "Now we take an image $X$ whose representation $X_0$ at some layer $``target\\_layer\"$ we want to learn. Our aim is to reconstruct the noise image to get this representation $X_0$. The principle behind this is that the noise image will be so reconstructed such that it will represent what the particular layer for which it is trained against wants to see."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4NFoCSBED49"
      },
      "outputs": [],
      "source": [
        "def get_output_at_nth_layer(inp, layer):\n",
        "    for i in range(layer):\n",
        "        inp = modules[i](inp)\n",
        "    return inp[0]\n",
        "\n",
        "# Dont forget that the system is 0 indexed\n",
        "target_layer = 18    # Which is this layer Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "inp_img = normalize(Image.open(\"./dog.jpg\"))\n",
        "inp_img_representation = get_output_at_nth_layer(inp_img, target_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogtvsBaXED5B"
      },
      "source": [
        "Some functions of code (euclidean_loss, alpha_norm, total_variation_norm, recreate_image) are present in utils.py. If you truly want to understand how this is implemented, it is recommended that you read the second and third page of this [paper](https://arxiv.org/abs/1412.0035), specifically, the regularization part, before asking questions on that. The aim of this code is to mainly understand the deep representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3nk6EiKED5B"
      },
      "outputs": [],
      "source": [
        "import cnn_utils\n",
        "\n",
        "# Define optimizers for learning the representation of the noise input image\n",
        "optimizer = optim.SGD([random_noise_img], lr=1e4, momentum=0.9)\n",
        "alpha_reg_alpha = 6\n",
        "alpha_reg_lambda = 1e-7\n",
        "tv_reg_beta = 2\n",
        "tv_reg_lambda = 1e-8\n",
        "\n",
        "# Put model into evaluation state\n",
        "model.eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HudW3gu0ED5J"
      },
      "outputs": [],
      "source": [
        "imgs=[]\n",
        "for i in range(161):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get output at the target layer (not the final layer)\n",
        "        output = get_output_at_nth_layer(random_noise_img,target_layer)\n",
        "\n",
        "        # Calculate euclidian loss between output image and the target image\n",
        "        euc_loss = 1e-1 * cnn_utils.euclidian_loss(inp_img_representation.detach(), output)\n",
        "\n",
        "        # Regularization\n",
        "        reg_alpha = alpha_reg_lambda * cnn_utils.alpha_norm(random_noise_img, alpha_reg_alpha)\n",
        "        reg_total_variation = tv_reg_lambda * cnn_utils.total_variation_norm(random_noise_img,tv_reg_beta)\n",
        "\n",
        "        loss = euc_loss + reg_alpha + reg_total_variation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Generate image every 10 iterations\n",
        "        if i % 10 == 0:\n",
        "            print('Iteration:', str(i), 'Loss:', loss.item())\n",
        "            x = cnn_utils.recreate_image(random_noise_img)\n",
        "            imgs.append(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsuqgehUED5Q"
      },
      "source": [
        "### Visualize the learned representations for the layer \"target_layer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd42p_g-ED5Q"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
        "for i in range(int(np.sqrt(len(imgs))) * int(np.sqrt(len(imgs)))):\n",
        "    a = fig.add_subplot(int(np.sqrt(len(imgs))), int(np.sqrt(len(imgs))),i+1)\n",
        "    imgplot = plt.imshow(imgs[i])\n",
        "    a.set_title(\"iter = \"+ str((i+1)*10))\n",
        "    plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z5yCWHhED5U"
      },
      "source": [
        "#### **Ungraded Exercise:** Above we visualized the inverted representations for the 18th layer, find similar representations for different layers in the network and thus visualize what the network learns at each of those layer. Try for different target images as well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4-oWiOAI7kZ"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvm-8H2RI7kd"
      },
      "source": [
        "#@title State True or False: According to Mahendran and Vedaldi technique, we loop through the dataset to optimize weights of the network in discussion? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\",\"TRUE\", \"FALSE\"]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94dIhQKDI7kf"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKzGvfdyI7kg"
      },
      "source": [
        "#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Walkthrough = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OjkIq7II7kh"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwLT_C0vI7ki"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gPMKJg_eI7kj"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}